CHAPTER

4

Dynamic games in
empirical industrial
organization✩

Victor Aguirregabiriaa,b,∗ , Allan Collard-Wexlerc,d , and Stephen P. Ryane,d,f
a University of Toronto, Toronto, ON, Canada
b CEPR, London, United Kingdom
c Duke University, Durham, NC, United States
d NBER, Cambridge, MA, United States
e Washington University in St. Louis, St. Louis, MO, United States
f CESIfo, Munich, Germany

∗ Corresponding author: e-mail address: victor.aguirregabiria@utoronto.ca

CONTENTS
1 Introduction........................................................................................ 227
1.1 Role of dynamic games in empirical industrial organization ................ 227
1.2 Organization of this chapter ...................................................... 229
2 Models ............................................................................................. 230
2.1 Basic framework ................................................................... 230
2.2 Markov perfect Nash equilibrium................................................ 232
2.2.1 Definition .......................................................................... 232
2.2.2 Equilibrium existence ........................................................... 232
2.2.3 Incomplete information ......................................................... 233
2.2.4 Multiple equilibria................................................................ 234
2.3 Examples ............................................................................ 235
2.4 Extensions of the basic framework .............................................. 236
2.4.1 Continuous time.................................................................. 236
2.4.2 Oblivious equilibrium............................................................ 236
2.4.3 Large state spaces ............................................................... 239
2.4.4 Persistent asymmetric information ........................................... 239
2.4.5 Firms’ biased beliefs ............................................................ 241
3 Identification and estimation ................................................................... 242
3.1 Data .................................................................................. 242

✩ We are grateful to the editors and to four anonymous referees for constructive comments and sugges-

tions. We would like to thank Seohee Kim for research assistance, and Peter Arcidiacono, John Asker,
Nick Buchholz, Yanyou Chen, Jan De Loecker, Ulrich Doraszelski, Gautam Gowrisankaran, Robin Lee,
Yao Luo, Mathieu Marcoux, Eduardo Souza-Rodrigues, Adam Rosen, and Daniel Xu for productive conversations about the manuscript.
Handbook of Industrial Organization, Volume 4, ISSN 1573-448X. https://doi.org/10.1016/bs.hesind.2021.11.004
Copyright © 2021 Elsevier B.V. All rights reserved.

225

226

CHAPTER 4 Dynamic games in empirical industrial organization

3.2

Identification ....................................................................... 244
3.2.1 Non-identification result ........................................................ 244
3.2.2 A set of sufficient conditions for identification ............................. 246
3.2.3 Relaxing restrictions (ID.1) to (ID.8) ......................................... 247
3.2.4 Identification of mixed continuous-discrete choice models ............. 252
3.3 Estimation methods ............................................................... 253
3.3.1 Full solution methods ........................................................... 254
3.3.2 Two-step CCP methods ......................................................... 258
3.3.3 Bajari-Benkard-Levin (BBL) method......................................... 262
3.3.4 Large state space and finite dependence ................................... 265
3.3.5 Unobserved market heterogeneity ........................................... 266
3.4 The promise of machine learning................................................ 268
4 Empirical applications........................................................................... 272
4.1 Earlier empirical work on dynamic games...................................... 272
4.1.1 Competition in the hospital market ........................................... 272
4.1.2 Dynamic output competition with learning by doing ...................... 274
4.1.3 Dynamics in auctions ........................................................... 275
4.1.4 Environmental regulations in concentrated industries.................... 278
4.1.5 Demand shocks and market structure ...................................... 282
4.1.6 Subsidizing entry................................................................. 284
4.2 Innovation and market structure................................................. 286
4.2.1 Microprocessor innovation: Intel vs AMD ................................... 286
4.2.2 Hard drive innovation: new products and cannibalization ............... 290
4.2.3 Car innovation and quality ladders ........................................... 293
4.2.4 Data on innovation............................................................... 294
4.3 Antitrust policy towards mergers ................................................ 295
4.3.1 Endogenous mergers ........................................................... 295
4.3.2 Evolving market structure and mergers ..................................... 297
4.3.3 Revealed merger efficiencies .................................................. 298
4.4 Dynamic pricing.................................................................... 299
4.4.1 Competition with price adjustment costs.................................... 299
4.4.2 Limit pricing....................................................................... 300
4.4.3 Dynamic pricing with network effects........................................ 301
4.5 Regulation .......................................................................... 303
4.5.1 Environmental regulation ....................................................... 303
4.5.2 Land use regulation ............................................................. 304
4.5.3 Product variety ................................................................... 306
4.5.4 Industrial policy .................................................................. 308
4.6 Retail ................................................................................ 309
4.6.1 Economies of density and cannibalization .................................. 309
4.6.2 Chains.............................................................................. 311
4.6.3 Unobserved heterogeneity and entry in retail .............................. 312
4.6.4 Effect of Walmart on rival grocers............................................. 313
4.6.5 Exit in declining industries ..................................................... 313
4.6.6 Repositioning ..................................................................... 314
4.6.7 Advertising ........................................................................ 315
4.7 Uncertainty and firms’ investment decisions .................................. 316
4.7.1 Firm investment under uncertainty .......................................... 316

1 Introduction

4.7.2 Uncertainty and oil drilling in Texas .......................................... 318
4.7.3 Uncertainty in shipping ......................................................... 318
4.8 Network competition in the airline industry ................................... 321
4.9 Dynamic matching................................................................. 323
4.10 Natural resources .................................................................. 325
5 Concluding remarks.............................................................................. 328
References............................................................................................ 331

Abstract
This chapter is organized around three main topics on dynamic games in empirical
IO: models, econometrics, and empirical applications. Section 2 presents the theoretical framework, introduces the concept of Markov Perfect Nash Equilibrium,
discusses existence and multiplicity, and describes the representation of this equilibrium in terms of conditional choice probabilities. We also discuss extensions of
the basic framework, including models in continuous time, the concepts of oblivious equilibrium and experience-based equilibrium, and dynamic games where firms
have non-equilibrium beliefs. In Section 3, we first provide an overview of the types
of data used in this literature, before turning to a discussion of identification issues
and results, and estimation methods. We review different methods to deal with multiple equilibria and large state spaces, describe recent developments for estimating
games with serially correlated unobservables, and discuss the use of machine learning methods to solving and estimating dynamic games. Section 4 discusses empirical
applications of dynamic games in IO. We start describing the first empirical applications in this literature during the early 2000s. Then, we review recent applications
dealing with innovation, antitrust and mergers, dynamic pricing, regulation, product
repositioning, advertising, uncertainty and investment, airline network competition,
dynamic matching, and natural resources. We conclude with our view of the progress
made in this literature and the remaining challenges.
Keywords
Industry dynamics, Market entry, Investment, Innovation, Capacity, Structural estimation

1 Introduction
1.1 Role of dynamic games in empirical industrial organization
A central focus in industrial organization (IO) is understanding the role of market
structure on equilibrium outcomes such as prices, product quality and variety, and
market shares, and how those outcomes influence producer profits and consumer welfare. Market structure encompasses all of the features of the supply side of a market:
the number of competitors in a market, their cost structure, where firms are located,

227

228

CHAPTER 4 Dynamic games in empirical industrial organization

the size of installed base in network industries, productivity advantages due to learning and managerial expertise, types and capacities of capital, the state of technology,
vertical and horizontal relationships between firms, and everything else that is critical to understand competition. Dynamics is key to understanding the endogenous
evolution of market structure. There are many questions in IO that revolve around the
dynamic aspects faced by firms. Investment, production, or pricing decisions can affect firms’ future profits, as well as future profits of their competitors in the industry.
Supply-side dynamics can arise from different sources, including sunk costs of entry,
partially irreversible investments, product repositioning costs, price adjustment costs,
or learning-by-doing. Demand is dynamic when consumers have switching costs, or
when products are durable or storable. Accounting for dynamics can change our view
of the impact of competition in industries and our evaluation of public policies.
An early attempt to understand these relationships, the structure-conduct-performance paradigm (Bain, 1951, 1956) looked at empirical associations between market structure and outcomes across industries. However, the endogeneity of market structure—that market structure is itself the result of many firm decisions in
equilibrium—was the core reason why the structure-conduct-performance paradigm
ended up generating more questions than answers in empirical IO. Thus, the so-called
New Empirical IO movement (see the survey by Bresnahan (1989)) emphasized the
need to build models of endogenous market structure. To that end, Bresnahan and
Reiss (1990, 1991), and Berry (1992) proposed entry models that used observed market structure to infer aspects of the underlying profit function. While it is possible
to include predetermined variables in the payoff function (e.g., firm size, capacity,
incumbent status) and to interpret the payoff function as an intertemporal value function (see Bresnahan and Reiss (1994)), these models are essentially static in nature
and have important limitations. The observed structure of an industry or market is
the result of years or decades of cumulative firm decisions, and these static models
leave no room for the history of demand or technology, or the process by which firms
enter the industry and then shake out, to fundamentally alter the predictions for market structure. In addition, empirical questions in IO that have to do with the effects
of uncertainty on firm behavior and competition, or that try to distinguish between
short-run and long-run effects of exogenous shocks, typically require the specification and estimation of dynamic structural models that explicitly take into account
firms’ forward-looking behavior. For these reasons, most of the recent work in IO
dealing with industry dynamics has relied on a more explicit modeling of dynamics.
Advances in econometric methods and modeling techniques, in conjunction with
the increased availability of data and computing power, have led to a large body
of empirical papers that study the dynamics of competition in oligopoly markets,
especially over the last decade. The history of applications in the dynamic games literature in industrial organization can be delineated by two defining innovations. The
first innovation was the introduction of Markov Perfect Nash Equilibrium (henceforth
MPNE) by Maskin and Tirole (1988a,b), and Ericson and Pakes (1995). The concept
of MPNE restricts players’ strategies to depend only on payoff-relevant state variables, which reduces substantially the set of equilibria and facilitates their computa-

1 Introduction

tion. At the same time, it provides a flexible framework that allows for rich dynamics
and can be applied to a wide variety of settings. A first generation of applications
focused on the calibration of stylized computational models that illustrated general
economic principles, such as Pakes and McGuire (1994) and Gowrisankaran (1999).1
At the same time, a handful of empirical applications directly applied those theoretical frameworks, such as Benkard (2004) and Gowrisankaran and Town (1997). These
papers were highly influential for both the substantive research questions they addressed and for highlighting the need for an econometric approach to estimation that
sidestepped the computational burden of repeatedly solving the theoretical model.
The second innovation was the subsequent development of conditional choice probability (henceforth CCP) based methods inspired by the dynamic single agent work of
Hotz and Miller (1993) and Hotz et al. (1994) adapted to dynamic games by a collection of papers (Aguirregabiria and Mira, 2007; Bajari et al., 2007; Pesendorfer and
Schmidt-Dengler, 2008; Pakes et al., 2007) that has directly led to the current era of
empirical applications. These CCP based approaches allowed very complex dynamic
games to be estimated—indeed much more complex than the types of dynamic games
that can usually be solved for.

1.2 Organization of this chapter
This survey is organized around three main topics: Models, in Section 2; Econometrics, in Section 3; and Empirical Applications, in Section 4.
In Section 2, we present the theoretical framework, introduce the concept of
Markov Perfect Nash Equilibrium, discuss existence and multiplicity, describe the
representation of this equilibrium in terms of conditional choice probabilities, and
illustrate it with some examples. We also discuss some extensions of the basic framework, including models in continuous time, the concepts of oblivious equilibrium and
experience-based equilibrium, and dynamic games where firms have non-equilibrium
beliefs.
In Section 3, we first provide a brief overview of the types of data used in this
literature, in Section 3.1, before turning to a discussion of identification issues and
results in Section 3.2. As a dynamic model, a key issue is the separate identification
of true dynamics from spurious dynamics due to serially correlated unobservables.
As a game, the presence of multiple equilibria in the data and unobservables correlated across players introduce relevant identification issues. The identification of the
discount factor and the biases from normalization restrictions of the payoff function
are issues that dynamic games share with other structural models where agents are
forward-looking. For discrete choice models, the misspecification of the distribution
of the unobservables is an important issue as it affects average marginal effects and

1 See Besanko et al. (2014), Doraszelski and Judd (2012), and Borkovsky et al. (2012) for more recent

examples of this computational theory papers in IO. These computational models have been covered in a
previous Handbook Chapter (Doraszelski and Pakes, 2007).

229

230

CHAPTER 4 Dynamic games in empirical industrial organization

counterfactuals using the estimated model. We discuss these identification issues and
present positive and negative identification results for dynamic games.
Section 3.3 deals with estimation methods. Empirical applications of dynamic
games in IO need to deal with two main computational issues. First, for a given value
of the structural parameters, the model typically has multiple equilibria. This introduces important challenges in the implementation of standard estimation methods
such as maximum likelihood or generalized method of moments. Second, the curse
of dimensionality in the solution of dynamic programming problems is particularly
important in dynamic games with heterogeneous agents. We review different methods to deal with these issues. We also describe recent developments for estimating
games with serially correlated unobservables. We finish Section 3 with a discussion
on the use of machine learning methods to solving and estimating dynamic games.
Section 4 discusses empirical applications of dynamic games in IO. We start
in Section 4.1 describing the main features—data, model, estimation, computation,
and research questions—in the first empirical applications in this literature during
the early 2000s. Then, we review more recent applied papers. We have organized
these applications around the following topics: innovation; antitrust and mergers; dynamic pricing; regulation; retail; product repositioning; advertising; uncertainty and
investment; airline network competition; dynamic matching; and natural resources.
Admittedly, this classification is based in multiple criteria (e.g., empirical question,
firms’ decisions, industry), but we prefer that each section covers clearly related papers, perhaps at the expense of missing some relevant connections between papers at
different sections.
Previous survey papers on dynamic games in IO include Doraszelski and Pakes
(2007), Arcidiacono and Ellickson (2011), Aguirregabiria and Nevo (2013), and
Berry and Compiani (2021). Doraszelski and Pakes (2007) deal with algorithms for
computing MPNE in dynamic games. Estimation methods are the main focus in Arcidiacono and Ellickson (2011) and Aguirregabiria and Nevo (2013). Our chapter
also covers these topics, though our treatment of solution methods is quite limited.
Instead, we provide a more extensive coverage of recent developments on identification of dynamic games, and on empirical applications. Other chapters in this fourth
volume of the Handbook of IO include sections on dynamics, e.g., the chapters on
demand, productivity, collusion, or innovation, among others. In this chapter, we focus on methodological issues (specification, computation, identification, inference),
which are particularly critical for dynamic games, as well as empirical applications
where the dynamics of strategic interactions plays a key role for the empirical results.

2 Models
2.1 Basic framework
We start presenting the Ericson-Pakes model (Ericson and Pakes, 1995), which is a
general framework that includes as particular cases most empirical applications of
dynamic games in IO.

2 Models

The game is played by N firms that we index by i ∈ I = {1, 2, ..., N}. Time is
discrete and indexed by t.
Firms maximize

 their expected and discounted flow of
∞
s π
profits in the market, Et
where βi ∈ (0, 1) is the firm’s discount
β
i,t+s
s=0 i
factor, and πit is its profit at period t. Every period, each firm i makes two strategic
decisions: a static decision, that affects current profits but not future profits; and an
investment (dynamic) decision, that has implications on future profits. For instance, in
a differentiated product industry, incumbent firms choose their prices (static decision)
and make investments to improve the quality of their products (dynamic decision).
See, for instance, Pakes and McGuire (1994). These decisions correspond to a static
and a dynamic game, respectively, that firms play in this market.
In the differentiated product example, given demand and marginal costs at period
t, active firms compete in prices, and a static Bertrand equilibrium determines the
current profit of each incumbent firm at period t—up to investment costs. We represent this component of the profit function as ri (xt ), where xt is a vector of state
variables affecting demand and costs at period t. We represent the investment decision using variable ait . This dynamic action involves an investment cost ci (ait , xt ).
The total profit function is πi (ait , xt ) = ri (xt ) − ci (ait , xt ). The set of feasible investment decisions may depend on the state variables, and it is represented as Ai (xt ). For
instance, investments may be restricted to be positive, or not larger than a borrowing
constraint that may evolve endogenously.
The vector of state variables xt follows a first order controlled Markov process
with transition CDF Fx (xt+1 |xt , a t ), where a t ≡ (ait : i ∈ I) is the vector with the
investment decisions of the N firms. Vector xt includes endogenous state variables—
such as capital stock, capacity, or product quality—with transition rules that depend
on firms’ investments. For instance, a firm’s stock of physical capital, kit , may evolve
according to equation ki,t+1 = δi,t+1 kit +ait , where ait in this example is investment,
and δi,t+1 is a depreciation rate that may be stochastic or not. The vector xt may also
include exogenous state variables with transition probabilities that do not depend
on firms’ investment decisions such as demand shifters (e.g., market population and
demographics) and input prices.
In our description of the model, we have implicitly assumed that a firm’s investment affects its own profit—other than the investment cost—and other firms’ profits
one period after the investment decision is taken. This is the assumption of time-tobuild that has been used in many models of firm investment such as Kydland and
Prescott (1982), but more specifically in Ericson and Pakes (1995). Though timeto-build is a feature in many applications of dynamic games that we review in this
chapter, there are also many studies that do not make this assumption. For instance, in
a model of market entry, we may consider that entry decisions are made at the beginning of period t and are effective during the same period. For this alternative timing
assumption, we need to modify the notation above and allow the variable profit and
the total profit functions to depend on all the firms’ current investment decisions. That
is, these functions become ri (a t , xt ) and πi (a t , xt ), respectively. For the rest of the
paper, unless we state otherwise, we adopt this notation without time-to-build.

231

232

CHAPTER 4 Dynamic games in empirical industrial organization

2.2 Markov perfect Nash equilibrium
2.2.1 Definition
Building on the seminal work of Maskin and Tirole (1988a,b), most of the IO literature studying industry dynamics has used the solution concept of Markov Perfect
Nash Equilibrium (MPNE). A key assumption in MPNE is that players’ strategies
at period t are functions only of payoff-relevant state variables at the same period.
In this model, it means that firms’ strategies are functions of the vector xt only. Let
α = {αi (xt ) : i ∈ I} be a set of strategy functions, one for each firm. A MPNE is a set
of strategy functions such that every firm is maximizing its value given the strategies
of the other players.
For given strategies of the other firms, the decision problem of a firm is a singleagent dynamic programming (DP) problem. Let Viα (xt ) be the value function of this
DP problem. This value function is the unique solution to the Bellman equation:



α
α
α
α
(1)
Vi (xt ) = max
πi (ait , xt ) + βi Vi (xt+1 ) dFx,i (xt+1 |xt , ait )
ait ∈Ai (xt )

α (x
where πiα (ait , xt ) and Fx,i
t+1 |xt , ait ) are the firm’s profit and the transition CDF
of the state variables given action ait for firm i and the strategy functions {αj (xt ) :
j = i} for firms other than i. That is, πiα (ait , xt ) = πi (ait , αj (xt ) : j = i, xt ) and
α (x
Fx,i
t+1 |xt , ait ) = Fx (xt+1 |xt , ait , αj (xt ) : j = i). For the description of some results, it is convenient to define the expression in brackets {} in Eq. (1) as the conditional choice value function viα (ait , xt ). That is,

α
viα (ait , xt ) ≡ πiα (ait , xt ) + βi Viα (xt+1 ) dFx,i
(xt+1 |xt , ait )
(2)

The best response decision rule for firm i is argmax in ait ∈ Ai (xt ) of viα (ait , xt ).

2.2.2 Equilibrium existence
Doraszelski and Satterthwaite (2010) show that existence of a MPNE in pure strategies is not guaranteed in this model under the conditions in Ericson and Pakes (1995).
They show that, when firms make discrete choices such as market entry and exit decisions, the existence of an equilibrium cannot be ensured without allowing firms to
randomize over these discrete actions. A possible approach to guarantee equilibrium
existence is to allow for mixed strategies. However, computing a MPNE in mixed
strategies poses important computational challenges. Instead, to establish equilibrium
existence in this class of models, Doraszelski and Satterthwaite (2010) propose incorporating private information state variables.2 This approach is in the spirit of Harsanyi
(1973) technique for purifying mixed-strategy Nash equilibria of static games. This
incomplete information version of the Ericson-Pakes model has been the one adopted
in most empirical applications of dynamic games in IO.
2 This issue of existence is also discussed in Gowrisankaran (1995) for these models.

2 Models

2.2.3 Incomplete information
Rust (1994a) was the first paper to present an incomplete information version of the
Ericson-Pakes model (see his Section 9 on discrete dynamic games). This is also
the model in the first econometric papers in this literature, such as Jofre-Bonet and
Pesendorfer (2003), Aguirregabiria and Mira (2007), and Pakes et al. (2007).
In addition to the common knowledge state variables xt , a firm’s profit depends on
a private information shock (or vector of shocks) εit , such that the profit function is
πi (a t , xt , εit ). This private information shock is independently distributed over time
and across firms with a distribution function Fε that has support over the real line.
Similarly as in the complete information version of Ericson-Pakes model, a Markov
Perfect Bayesian Nash Equilibrium (MPBNE) in this model is an N-tuple of strategy
functions α = {αi (xt , εit ) : i ∈ I} such that a firm’s strategy maximizes its value
taking as given other firms’ strategies.
For the computation of an equilibrium and for the estimation of the model, it
is very convenient to represent firm’s strategies as conditional choice probabilities
(CCP). For any strategy function αi (xt , εit ) we can define its corresponding CCP
function, Pi (ait |xt ), as the probability distribution induced by this strategy and the
distribution of private information. That is,

Pi (ait |xt ) ≡

1 {αi (xt , εit ) = ait } dFε (εit ),

(3)

where 1 {·} is the indicator function. This CCP function represents the expected behavior of a firm from the point of view of other firms (or the researcher) who do not
know this firm’s private information.
We can represent firms’ best responses and equilibrium conditions as a fixed point
mapping in the space of these CCPs. Let P ≡ {Pi (ait |xt )} be a vector of CCPs for every firm i ∈ I, every action ait ∈ A, and every state xt ∈ X . Define πiP (ait , xt , εit ) as
firm i’s expected profit given that the other firms behave according to their respective
CCPs in P. That is,

πiP (ai , xt , εit ) ≡


a −it ∈A−i (xt )

⎤

⎡

Pj (aj t |xt )⎦ πi (ait , a −it , xt , εit ).

⎣

(4)

j =i

Similarly, FiP (xt+1 |xt , ait ) is the transition probability of the state variables from firm
i’s perspective and given that the other firms behave according their CCPs in P:

FiP (xt+1 |xt , ait ) ≡


a −it ∈A−i (xt )

⎡
⎣
j =i

⎤
Pj (aj t |xt )⎦ Fx (xt+1 |xt , ait , a −it ).

(5)

233

234

CHAPTER 4 Dynamic games in empirical industrial organization

Then, for every firm i, action ait , and state xt , we have that CCPs satisfy the following
equilibrium condition:
Pi (ait |xt ) =
⎧
⎫⎤
⎡
⎪
⎪

⎨ πiP (ai , xt , εit )
⎬
⎥
⎢

1 ⎣ait = arg max
⎦ dFε (εit )
P
P
⎪
ai ∈Ai (xt ) ⎪
⎩ +βi Vi (xt+1 ) dFx,i (xt+1 |xt , ai ) ⎭

(6)

where ViP is the (integrated) value function in firm i’s DP problem given that the
other firms behave according their CCPs in P. This value function uniquely solves
the following integrated Bellman equation:
⎧
⎫
⎪
⎪

⎨ πiP (ai , xt , εit )
⎬
P

dFε (εit ). (7)
Vi (xt ) =
max
P (x
⎪
ai ∈Ai (xt ) ⎪
⎩ +βi ViP (xt+1 ) dFx,i
t+1 |xt , ai ) ⎭
Using a more compact vector notation, a MPBNE is a vector of CCPs, P ∈
[0, 1]|I ||A||X | that solves the fixed point problem P = (P), where (.) is a vectorvalued function that is defined by stacking the function in the right-hand-side of
Eq. (6) over every value (i, ait , xt ) ∈ I × A × X . Vector P lives in the compact simplex space, and under standard conditions on the distribution Fε , the best response
function  is continuous. Therefore, Brower’s fixed point theorem implies equilibrium existence (see footnote 45 in Rust (1994a), and for more details, Section 2.3 in
Aguirregabiria and Mira (2007), and Proposition 2 in Doraszelski and Satterthwaite
(2010)).

2.2.4 Multiple equilibria
The model generically has multiple equilibria, as agents best respond to the strategies
of other agents, and there are potentially many strategies that are consistent with
this definition of equilibrium. There are conditions on the primitives of the model
that guarantee equilibrium uniqueness, but they are typically strong restrictions. For
instance, a set of sufficient conditions for uniqueness is: (i) the game has a finite time
horizon; (ii) firms are (ex-ante) homogeneous in their profit functions and transition
probabilities; and (iii) every period, only one firm can make an investment decision.3
However, multiple equilibria are possible when we relax only one of the conditions
(i), (ii), and (iii). Alternatively, Abbring and Campbell (2010) work through an entry
model with identical firms, in which entry and exit decisions are assumed to follow
a last-in first-out (LIFO) structure. With the addition of assumptions on the process
for demand, they show that this model has a unique equilibrium. In general, this
uniqueness result does not extend beyond identical firms.

3 See Igami (2017) for an empirical application that imposes this set of restrictions.

2 Models

Dealing with multiple equilibria, in the estimation of the model and in counterfactual experiments using the estimated model, is a topic that has received substantial
attention in this literature. We deal with this issue in Section 3.3.

2.3 Examples
The framework presented above has been used in a wide range of empirical applications in IO, including market entry and exit, firms’ adoption of new technologies
or products, investment in physical capital or capacity, investment in R&D and innovation, learning-by-doing, competition in product quality, product positioning,
store geographic location, price competition with menu costs or/and with durable
or storable products, search and matching, dynamic auction games, market networks,
endogenous mergers, and exploitation of natural resources. We cover all these applications in Section 4. To illustrate some specific features and economic trade-offs in
these models, we briefly describe here three examples.
Market entry and exit. The investment decision is binary, with ait = 0 if the firm is
not active in the market, and ait = 1 if active. The endogenous state variable is the
lagged decision that determines if the firm is an incumbent (ai,t−1 = 1) or a potential entrant (ai,t−1 = 0). Potential entrants pay an entry cost if they decide to enter.
Incumbents do not pay an entry cost if they decide to be active, but pay exit costs
(or receive a scrap value) if they choose to be inactive. A firm’s number of years of
experience in the market may have a positive effect on the firm’s profit by increasing consumer demand or reducing costs (i.e., passive learning). A key parameter of
interest in these applications is the sunk entry cost: the difference between the entry
cost and the firm’s scrap value upon exit. The magnitude of this sunk cost—and its
distribution across firms—has important implications on market structure. Firms’ uncertainty about future demand and costs also plays an important role in firms’ entry
and exit decisions.
Price competition with durable products. In the market of a differentiated durable
product, firms face a dynamic trade-off in their pricing decisions. A price reduction implies an increase in today’s sales but also a reduction in future demand, as
consumers buying the product today exit the market and will not be part of future
demand. Goettler and Gordon (2011) study this type of dynamic pricing in the PC
microchip industry, discussed in Section 4.2.1. Esteban and Shum (2007) study the
effects of durability and secondary markets on dynamic price competition between
automobile manufacturers.
Exploitation of natural resources. In industries where firms exploit a common-pool
resource, a firm’s amount of output implies a dynamic externality on other firms
because of the depletion of the common stock. This is known popularly as the tragedy
of the commons. Huang and Smith (2014) study this problem in the context of the
shrimp fishery industry in North Carolina discussed in Section 4.10. They propose
and estimate a dynamic game of fishermen’s daily fishing decisions.

235

236

CHAPTER 4 Dynamic games in empirical industrial organization

2.4 Extensions of the basic framework
2.4.1 Continuous time
Doraszelski and Judd (2012) introduced continuous time methods to the dynamic
games literature. A key property of continuous models is that, with probability one,
only one firm may experience a change in its state – with the subsequent potential
change in its action. This reduces the number of future states we need to integrate
over to calculate expected continuation values, and consequently it can generate computational savings when solving for an equilibrium. In a discrete time game, if there
are N agents with A possible actions each, one has to integrate over AN states. By
contrast, in continuous time, one has to sum over (A − 1)N terms; this is smaller than
the previous quantity and also grows slower as either A and N increase. Furthermore,
in continuous time models, transition matrices are typically more sparse than in discrete time, such that solution algorithms that exploit sparse matrices can be more
effective when computing the solution to Bellman equations. A negative computational property of continuous time models is that the time discount factor becomes
larger than in discrete time, such that a solution algorithm that iterates in the Bellman equation requires a larger number of iterations than in discrete time. However,
when the number of players N becomes larger, the savings from the smaller cost per
iteration dominates the cost from the larger number of iterations.
The computational advantage of modeling dynamic games in continuous time
comes with a cost in terms of flexibility and realism. In these models, state variables
do not respond instantaneously after a change in a firm’s action. This implies a delay
in firms’ strategic responses. For instance, in an actual market, if a competitor cuts
its price, a firm might want to respond almost instantaneously by cutting its own
price. But in the continuous time model, the firm will not respond until state variables
change, and this takes a while. The standard continuous time dynamic game can
be modified to allow state variables to change instantaneously after a change in a
firm’s action. However, this extension eliminates the computational savings of the
continuous time model, as it requires integrating over all possible states to calculate
continuation values. Arcidiacono et al. (2016) developed methods for estimation and
counterfactual experiments in dynamic discrete choice models in continuous time.

2.4.2 Oblivious equilibrium
In competitive models of industry dynamics, such as those explored in Hopenhayn
(1992), firms are atomistic. That is, any choice that they make has no bearing on the
evolution of the aggregate state. This is very helpful for computation, since it means
that firms can take the path of the aggregate state as given. If one assumes that there
are no aggregate shocks, i.e., shocks that create uncertainty on the evolution of a
market level state such as changes in demand, then the path of the aggregate state is
also deterministic. In other words, firms have perfect foresight.4
4 Of course, if there are aggregate shocks, such as shocks to demand, then firms will need consider the

distribution of aggregate states in the future. This is the basis of many models in macroeconomics with
heterogeneous agents. See Krusell and Smith (1998) for a very influential example of this type of work.

2 Models

In contrast, the work that builds on Ericson and Pakes (1995) takes as a fundamental that firms can, by themselves, affect the profits of their rivals. This brings up
issue of strategic considerations, which requires firms to take into account the entire
distribution of their rival’s states. As a matter of computation, the size of the state
space required for even very austere models of industry dynamics is quite large. For
instance, a Pakes and McGuire (1994) model with 10 firms choosing 20 different
quality levels, has over 10 trillion states, which raises an issue of how to store the
value function in computer memory, let alone how to compute it.
Weintraub et al. (2008) work out the consequences of a model where firms behave
more closely to how they would in a competitive environment, with the goal of simplifying computation. More precisely, they propose a model where firms only keep
track of their own state when making choices, that is they restrict strategies αi (·) to
be functions of a firm-specific state xit , which is a component of the vector xt . In
addition, every firm believes that the state of the market (i.e., the average value of xit
over all the firms in the market) is the long-run average of this variable in equilibrium.
Given that the state space considered by oblivious firms is merely xit , the state space
is as large as the one considered by a single agent model, considerably alleviating
computation.
The restrictions imposed by the oblivious equilibrium concept on the variables
that enter firms’ strategies are somewhat ad hoc, much as the MPNE refinement restricts strategies to depend only on payoff relevant state variables and rules out the
type of strategies typically used to describe tacit collusion such as Green and Porter
(1984). However, Weintraub et al. (2008) also show that, for a class of oligopoly
models, the oblivious equilibrium of this game converges to the MPNE as the market
becomes large and the number of firms increases. Furthermore, under certain conditions, the quality of this approximation, measured by numerical experiments, can be
quite reasonable even with a small number of firms in the market. Thus, one can think
of oblivious equilibrium as a good approximation for MPNE in industries with many
firms, none of which is particularly large, rather than a theoretical refinement which
of interest by itself. Oblivious equilibria have been used in a number of applications,
such as Xu and Chen (2020).
The immediate issue with oblivious equilibria is how to generalize its computational simplicity to environments where firms may need to keep track of more than
their state variable xit . For instance, many industries are characterized by a small
number of leading firms and a large number of fringe firms. In this context it may
make sense for firms to track the states of the dominant firms in addition to their own
state variable. Benkard et al. (2015) study this model.
Even in an unconcentrated industry, there are some difficulties with oblivious
strategies if there are aggregate state variables—again think of a macro demand
shifter. For instance, if we consider a market where demand has been declining, say
Cleveland, there may be a large number of firms compared to a market where demand has been growing over time, like Austin, even if the two cities currently have
comparable metro level populations. This means that current demand might not capture the number of firms in the market very well, and oblivious equilibria could differ

237

238

CHAPTER 4 Dynamic games in empirical industrial organization

substantially from MPNE. One way to get around this problem is the moment based
Markov equilibrium (henceforth MME) proposed by Ifrach and Weintraub (2017).
This equilibrium concept restricts firms’ strategies to be functions of the firm’s own
state xit but also of moments of the state xt , which are denoted as 
st . For instance,
in a dynamic game of investment in capacity where firms compete each period à la
Cournot, relevant moments in
st could be the number of active firms and total capacity of all the firms. These two moments are sufficient statistics for profits in a static
Cournot game with ex-ante homogeneous firms. Note however, there is no guarantee that two states xt that generate the same moments will have the same profits in
the future in this game, so they are not usually sufficient statistics for a firm’s value
function.
An important contribution of Ifrach and Weintraub (2017) is to structure how
firms form expectations on current and future profits given their own state xit as well
as the moment based state for the rest of the market given by 
st . The issue here is
that one needs to forecast current profits and future state transitions given the MME
state (xi,t ,
st ). In the Cournot example with homogeneous firms discussed above, the
aggregate capacity and the own capacity are sufficient statistics to compute a firm’s
current profit. This is not usually the case. Thus, in more general cases, one needs to
associate the expected profit 
π (xit ,
st ) to the actual profit function π(xt ) for the states
xt that generate moments 
st , and this requires understanding what type of weighted
sum will do this properly. Furthermore, the state transitions defined on the MME state
[xi,t+1 ,
denoted as F
st+1 |xit ,
st ] are also unknown, and these are essential for computing the firm’s value function. Ifrach and Weintraub (2017) propose to sample from
the ergodic distribution. That is, given a set of MME strategies α̂(xit ,
st ), they use
forward simulation to compute both the expected profit 
π (xit ,
st ) and expected state
[xi,t+1 ,
transition probabilities F
st+1 |xit ,
st ]. This allows them to solve for the value
,
function and firm policies, and repeat this sampling procedure to compute 
π and F
until this algorithm converges. Incidentally, this sampling idea is also found in the algorithm used in Fershtman and Pakes (2012)’s model with asymmetric information.
Moreover, a natural specification of MME without any moments at all, reduces down
to an oblivious equilibrium. A number of recent papers have used MME, such as Jeon
(2020), Caoui (2019), and Vreugdenhil (2020), which all incorporate rich firm level
heterogeneity making a reduction of the state space inescapable.5
While the researcher can in principle choose any vector of moments 
st , usually
these moments are chosen so that the MME might be close to a MPNE of the game.
This immediately poses the question of which moments to choose, which we discuss
in more detail in the next section.

5 Notice that an attractive part of using MME’s is that the reduction in the state space happens in the

choice of the moments 
st . Otherwise, this choice will be made earlier in the paper when choosing the
richness of the underlying state space xt to begin with.

2 Models

2.4.3 Large state spaces
An alternative to using MME to reduce the state space
 is to kapproximate the value
k
function with a basis approximation such as V (xt ) ≈ K
k θk φ (xt ), where each φ (·)
is a basis function and θk is a coefficient. To make this more concrete, if two firms
were competing in quality and xt = (x1t , x2t ) where xit is firm i’s quality at period
t, then basis functions could be a second order polynomial such that value function
2 + θ x2 + θ x x . A
V (x1t , x2t ) is approximated using θ1 + θ2 x1t + θ3 x2t + θ4 x1t
5 2t
6 1t 2t
key feature of this approach is that to compute an approximation to the solution of
the DP problem, one does not need to solve for a fixed point for the value function
at each state xt , but for a fixed point in the space of the vector of coefficients θ ≡
{θk }K
k=1 . A good example of this approach using Chebyshev polynomials for a basis
function is Doraszelski (2003)’s work on an R&D race in duopoly.6 Farias et al.
(2012) explore the numerical implementation of these solutions in the context of
a Pakes and McGuire (1994) model. Several empirical papers have also used this
approach, such as Sweeting (2013) for a dynamic game of competition between radio
stations (that we describe in Section 4.5.3), Barwick and Pathak (2015) for the market
for real estate agents, and Arcidiacono et al. (2016) for retail entry.
In the context of dynamic games, the main difficulty in the application of this
method is finding a suitable basis function, given the large dimension of the state
xt . Clearly, polynomial basis functions do not work when there are ten firms in the
market since this means that the space of θ has at least ten dimensions. Instead,
Powell (2007) suggests including features of the states, which roughly translates to
picking relevant moments of the state and have basis functions defined over them.
This returns the question to how to properly pick moments of the state as in our
discussion of MME. One approach is to think about the components of the state
that help predict profits. In the Cournot example, total capacity of the industry is the
relevant state variable. In a model with firms competing in quality with a logit demand
system, the relevant aggregate is McFadden surplus (or inclusive value), such as in
Gowrisankaran and Rysman (2012) and Aguirregabiria and Ho (2012).
A promising approach for solving dynamic games is using tools from machine
learning and artificial intelligence. For instance, in the context of approximate DP
methods described above, the relevant moments can be identified using an iterative
algorithm that simultaneously solves for the solution of the Bellman equation. In
this spirit, Kalouptsidi (2018) uses LASSO to pick out the basis functions in her
application. More promising is the application to dynamic games of newly developed
techniques in deep learning.

2.4.4 Persistent asymmetric information
The model considered so far assumes that the only form of firms’ private information
is an idiosyncratic shock that is independently distributed across firms and over time.
This precludes interesting forms of asymmetric information. For instance, Laffont
6 These approximate DP methods are extensively covered in a book by Powell (2007).

239

240

CHAPTER 4 Dynamic games in empirical industrial organization

and Tirole (1993)’s work on the regulation of a monopolist with unknown costs relies
critically on persistent asymmetric information. There is a deep interest in IO theory
on these types of models.
In dynamic games, an important challenge of incorporating persistent private
information is that the complete history of previous states and decisions becomes
payoff relevant. For a firm trying to uncover its rival’s private information, any action
that they have taken in the past, and the context in which this action was taken, i.e.,
the state at that point, is relevant to form a posterior. Therefore, if we maintain the
assumption of MPBNE without further restrictions, the dimension of the state space
becomes intractable for solving or estimating even simple versions of these games.
Fershtman and Pakes (2012) study this type of model and propose an alternative
equilibrium concept to deal with the high dimensionality problem. In their model,
every firm i observes a public state 
xt and a private state εit , such that their information set at period t is (
xt , εit ) and its strategy function is αi (
xt , εit ). This information
set has two important differences with respect to the model considered so far. First,

xt may contain lagged values {xs : s < t}. Second, εit can be serially correlated or
even time-invariant. To avoid the dimensionality problem of the MPBNE solution
concept in this game, Fershtman and Pakes (2012) propose an equilibrium concept
that they denote Experience Based Equilibrium (EBE). An EBE imposes three types
of restrictions on equilibrium strategies: (i) if a state is visited, then this state will be
visited in the future repeatedly; (ii) strategies are optimal given the evaluations of outcomes (profits); and (iii) strategies generate expected discounted values of profits that
are consistent with these evaluations in the recurrent subset of states. Fershtman and
Pakes (2012) propose a reinforcement learning algorithm to compute EBE strategies,
taking ideas from Q-learning implemented in Pakes and McGuire (2001).
In dynamic games with persistent asymmetric information, an assumption that
can reduce substantially the dimensionality problem is that firms’ private information becomes common knowledge every T periods. Under this condition, 
xt = xt
when private information is revealed; 
xt = (xt−1 , xt ) one period after revealing private information; and so on, such that the maximum dimension of the state space is

xt = (xt−T +1 , ..., xt ).
Asker et al. (2020) apply the EBE solution concept to a dynamic auction game
under the assumption of information revelation every T periods. Take the example
of either timber or construction auctions, which have been studied extensively in
empirical IO. In both contexts, firms are competing against rivals repeatedly, and
there are important sources of persistent asymmetric information. For instance, firms
have backlogs of construction projects that affect their bidding behavior (i.e., a larger
backlog lowers the value from winning an auction). A firm’s backlog is not perfectly
known by its rivals. However, when a firm wins an auction, its bid becomes publicly
available and this yields information to rivals that is helpful for subsequent auctions.
Furthermore, firms may benefit from a commitment to share their backlog information with each other. Asker et al. (2020) compute EBE for different values of T to
evaluate the impact of information sharing among bidders. They show that information sharing, even of strategically important data, can be welfare increasing.

2 Models

Fershtman and Pakes (2012)’s approach to deal with persistent asymmetric information can be critical for making empirical work on dynamic games less tied down
to the MPNE solution concept. However, the issues with large state spaces become
substantially more difficult since some history dependence needs to be tracked. Applied work will need to address these computational problems in order to make this
approach more than a proof of concept.

2.4.5 Firms’ biased beliefs
Firms’ behavior depends on their beliefs about the actions of other firms in the
same market. Managers and their firms have different abilities to collect and process information and, as a result, they are heterogeneous in their expectations. This
heterogeneity in beliefs can have important implications on firms’ performance and
welfare. The importance of firms’ heterogeneity in their ability to form expectations
and the possibility of biased beliefs has been long recognized in economics, at least
since the work of Simon (1959). However, in most fields in economics, the status
quo has been to assume rational expectations. In particular, as we described above,
the assumption of Markov perfect equilibrium has been very common in empirical
applications of dynamic games in IO.
Recent papers in empirical IO relax the assumption of rational expectations
and present evidence of substantial heterogeneity and biases in firms’ beliefs. As
one would expect, biased beliefs are more likely in new markets and after regulatory changes. For instance, after deregulation of the US telecommunication industry
(Goldfarb and Xiao, 2011), the UK electricity market (Doraszelski et al., 2018), the
Texas electricity spot market (Hortaçsu and Puller, 2008; Hortaçsu et al., 2019), or
the Washington State liquor market (Huang et al., 2020), and in the early years of
the fast-food restaurant industry in UK (Aguirregabiria and Magesan, 2020) or China
(Xie, 2021).
Most of these applications consider static games of market competition and use
the solution concepts of level-K rationality introduced by Stahl and Wilson (1995)
and Nagel (1995), and the Cognitive Hierarchy (hereafter, CH) equilibrium introduced by Camerer et al. (2004). Let Bi (a −it |xt ) be the probability distribution
that represents firm i’s belief about other firms’ actions given common knowledge
state variables xt . Under Bayesian Nash Equilibrium (BNE), beliefs correspond
to the actual probability
distribution, as represented by players’ CCPs, such that

Bi (a −it |xt ) = j =i Pj (aj t |xt ). In contrast, level-K rationality and CH are equilibrium concepts where firms have biased beliefs. In these models, firms are heterogeneous in their beliefs and there is a finite number of belief types. That is, the
probability distribution Bi (a −it |xt ) belongs to a finite number K of belief types.
These types correspond to different levels of strategic sophistication and are determined by a hierarchical structure. A type-0 firm has an arbitrary belief function
B (0) (a −it |xt ). In the level-k model, a type-k firm believes that all the other firms are
type k − 1. This recursive structure defines the belief functions for every type k between 1 and K. The only unrestricted function is the beliefs function for type-0: the
rest of the belief functions are known functions of B (0) and the structural parame-

241

242

CHAPTER 4 Dynamic games in empirical industrial organization

ters of the model. The CH model is more flexible than the level-K model. In the CH
model, a type-k firm believes that the other firms come from a probability distribution over types 0 to k − 1. This is the model of firms’ biased beliefs that has been
most commonly used in IO applications, e.g., Goldfarb and Xiao (2011), Brown et
al. (2013), and Hortaçsu et al. (2019). These models impose important restrictions:
they do not include BNE or rational beliefs as a particular case, and there is a small
number of belief types. However, they have the attractive feature of being equilibrium
models where (biased) beliefs are determined endogenously. This feature makes them
particularly attractive for counterfactual experiments.
In dynamic games, every period t firms need to form probabilistic beliefs about
the actions of competitors not only at the current period but also at future periods. Let
t
(a −i,t+s |xt+s ) be the probability distribution that represents firm i’s beliefs at
Bi,t+s
period t about the behavior of competitors at period t + s if the state is xt+s . A firm
t+1
t
(.) − Bi,t+s
(.) represents the updating from
can update its beliefs over time, and Bi,t+s
period t to period t + 1 in the beliefs that firm i has about the behavior of competitors at period t + s. This belief structure is very general and allows for general forms
of firms’ learning or forgetting. Given its beliefs at period t, a firm’s best response
is the solution of a single-agent dynamic programming problem. Under MPBNE,
firms’ beliefs are equal 
to the actual probability distribution of other firms’ choices:
t
(a −i,t+s |xt+s ) = j =i Pj,t+s (aj,t+s |xt+s ).7 CH and Level-K rationality modBi,t+s
els have been extended to dynamic games (e.g., Ho and Su (2013)). However, these
models impose strong restrictions on the evolution of beliefs over time: a firm’s belief type does not vary over time. As far as we know, there are not IO applications of
these models in dynamic games. Aguirregabiria and Magesan (2020) consider empirical dynamic games where firms’ belief functions have the general structure describe
above. They study the nonparametric identification of belief and payoff functions in
this model, and apply this model to study competition in number of stores between
McDonalds and Burger King in UK.

3 Identification and estimation
3.1 Data
The datasets used in most applications of dynamic games in IO can be described as
panel data of M markets (geographic locations, products), over T periods of time,
with information on actions and state variables for N players (often firms). The order
of magnitude of M, T , and N , and the structure of the panel (e.g., how unbalanced
it is in the different dimensions) varies across applications. Since most applications
study oligopoly markets, the number of firms N is typically small, but there is also a
literature considered in Section 4.9 that handles cases with a large number of agents.
7 The notation here considers a non-stationary model. If the model is stationary, then CCP functions P (.)
j

are time invariant.

3 Identification and estimation

The number of periods T is often small too. In many applications, a substantial part
of the sample variation comes from the number of markets M that may include hundreds or thousands of locations. Nevertheless, there are applications where the global
nature of the industry implies very few markets, or even a single national or world
market. For instance, this is the case for PC microchips (Goettler and Gordon, 2011),
or hard drives (Igami, 2017) discussed in Section 4.2. In these cases, enough sample
variation is achieved by the joint combination of firms, time periods, and markets,
where neither of the three dimensions is large but MN T can be large enough.
More generally, we have a three dimensional panel dataset {aimt , ximt : i =
1, 2, ..., N; t = 1, 2, ..., T ; m ∈ Mit }, where i indexes firms, t time, and m markets.
We use Mit ⊆ M ≡{1, 2, ..., M} to represent the set of markets where firm i is observed making decisions at period t. The structure of the sets Mit is important for the
identification and estimation of the model. In some industries every firm is a player
in all (or most of) the M markets such that Mit = M for every (i, t). For instance,
this is the case in a retail industry characterized by competition between large retail
chains which are potential entrants in any of the geographic markets that constitute
the industry, discussed in more detail in Section 4.6. With this type of data, the researcher can allow for rich forms of firm heterogeneity that is fixed across markets
and time by estimating firm-specific structural parameters. In other industries, even if
competition is local and M is large, most of the firms specialize in operating in a few
markets such that Mit is typically a small subset of M. In these cases, allowing for
rich forms of firm unobserved heterogeneity requires more sophisticated econometric
methods, and sometimes restrictions.
The idea of looking at panel data of independent markets with information on
observed demand factors and market structure – pioneered in the static setting by
Bresnahan and Reiss (1991) or Berry (1992) – has been carried over to dynamic
settings. This starts with the incompletely dynamic models of Bresnahan and Reiss
(1994) (used in Collard-Wexler (2014) as well), but adopted by both Collard-Wexler
(2013) and Dunne et al. (2013). Indeed, the most popular methods for the estimation of dynamic games (the two-step CCP methods that we describe in Sections 3.3.2
and 3.3.3) are particularly well suited to use panel data from many independent markets. However, this is not the only type of data that has been brought to bear on these
questions. For many applications, such as those for industries with a single national
of world market, this panel data with large M approach is not feasible. In some applications, the data also includes information on prices and quantities, and input costs,
that can be used to estimate demand functions and variable costs. In fact, as we show
in Section 4, empirical applications of dynamic games in IO are characterized by a
wide variation in the types of data being used.
The original idea from Bresnahan and Reiss (1991) of backing out markups purely
from the pattern of market structure and market demand shifters, that is, without
any information on prices or costs, is an admittedly heroic use of economic theory
to structure estimation. A first alternative is to use more traditional static cost and
demand estimation to fill in much of the period profit function. The second approach
is to calibrate both the determinants of static profits, and dynamic costs, such as

243

244

CHAPTER 4 Dynamic games in empirical industrial organization

entry or scrap values, from more anecdotal accounting data or engineering estimates.
Indeed, Benkard (2004) is a particularly extreme case in that no dynamic choices are
used to estimate the model, and there is only a single global market for aircraft.
Many papers have used a more ragtag empirical approach in this literature. For instance, in Ryan (2012), demand and costs are estimated using traditional IO methods
for these problems. Only a very parsimonious number of parameters are estimated using the dynamic structure of the industry, such as entry and exit costs and investment
adjustment costs. While there are fewer flagship methodological papers to illustrate
how these methods work, most empirical applications of dynamic oligopoly use empirical support in the form of well researched calibrations, static demand and cost
estimation, as well as estimation that leverages the dynamic choices in the dynamic
oligopoly game.
This makes the literature on dynamic games quite different from, say, the empirical literature on demand estimation and production function estimation that is
organized around common data structures, such as data on prices and quantities, for
demand, or firm level data on output and inputs, for production functions.

3.2 Identification
The structural parameters (or functions) of the model consist of the profit functions
πi , the discount factors βi , the transition probability of the state variables Fx , and the
distribution of private information shocks Fε . We represent all these parameters in a
compact form using θ ≡ {πi , βi , Fx , Fε : i ∈ I}. The researcher is interested in the
identification of θ using the data described above.
In this subsection, we present identification and non-identification results for dynamic games. We start with a well known non-identification result. Then, we present
a set of sufficient conditions for identification that has been used, implicitly or explicitly, in most existing empirical applications of dynamic games in IO. Finally, we
discuss recent studies showing identification of dynamic games under weaker conditions. We focus on the following identification issues that have received attention
in the literature: (i) unobserved market heterogeneity; (ii) multiple equilibria in the
data; (iii) normalization of the payoff of a choice alternative; (iv) time discount factor; (v) non-additive unobservables; (vi) nonparametric distribution of unobservables;
and (vii) non-equilibrium beliefs. Most of these results have been developed in the
context of discrete dynamic games (i.e., firms’ actions and observed state variables
are discrete), and this class of models is the focus of Sections 3.2.1 to 3.2.3. In Section 3.2.4, we present results for mixed continuous-discrete choice models.

3.2.1 Non-identification result
In the context of single-agent dynamic discrete choice games, Rust (1994b) and
Magnac and Thesmar (2002) present a non-identification result that is well-known
in the literature of dynamic structural models. Note that a single-agent dynamic
model is a restricted version of the dynamic game in Section 2: payoff functions
πi (ait , a −it , xt ) and transition probability functions Fx,i (xt+1 |xt , ait , a −it ) do not

3 Identification and estimation

depend on other firms’ actions, a −it . Since this model is more restrictive than a dynamic game, and it is estimated using the same type of data, non-identification of
single-agent models implies non-identification of the dynamic game version. For the
same reason, a positive identification result for dynamic games implies identification
of its restricted single-agent version.
The set of assumptions (ID.1) to (ID.4) below define a class of dynamic discrete
choice models that has been used in many empirical applications. These assumptions
were first introduced by Rust (1987) and Rust (1994b) such that this model is often
referred as Rust model.
Assumption (ID.1). No common knowledge unobservables. The researcher observes all the state variables that are common knowledge to firms, xt . The only
unobservables for the researcher are the private information shocks εit .
Assumption (ID.2). Additive unobservables. The private information variables are
additively separable in the payoff function. More specifically, the profit function has
the form πi (a t , xt ) + εit (ait ), where {εit (ai ) : ai ∈ Ai } are the unobservable shocks.
Assumption (ID.3). Known distribution of the unobservables. The probability distribution Fε does not depend on any parameter that is unknown to the researcher.
Furthermore, it is strictly increasing over the whole Euclidean space RJ +1 .
Assumption (ID.4). Conditional independence. Conditional on (a t , xt ), the realization of xt+1 is independent of εt . Note that this assumption was already included
in the description of the incomplete information dynamic game in Section 2.2.3.
Under conditions (ID.1) to (ID.4), the CCP functions Pi (ait |xt ) and the transition
probability function Fx (xt+1 |xt , a t ) are nonparametrically identified. Furthermore,
Hotz and Miller (1993) show that there is a one-to-one relationship between CCPs
and the conditional choice value function (as defined in Eq. (2)) relative to a baseline
choice alternative, that we can represent as 
vi (ait , xt ) ≡ vi (ait , xt ) − vi (0, xt ) (Proposition 1 in Hotz and Miller (1993)). Therefore, under these assumptions, differences
in conditional choice value functions 
vi (ait , xt ) are uniquely identified. However,
Rust (1994b) and Magnac and Thesmar (2002) show that knowledge of differences
in conditional choice value functions is not sufficient to identify the payoff function
πi and the discount factor βi (Proposition 2 in Magnac and Thesmar (2002)).
There are two main identification issues involved in this non-identification result.
First, as in any other revealed preference approach, we can identify payoff function
πi only relative to the payoff of a baseline alternative. The typical approach is to normalize to zero all the payoffs of a baseline choice alternative, e.g., πi (ait = 0, xt ) = 0
for every value of xt . The problem is that, in contrast to static discrete choice models,
this normalization condition is not innocuous in dynamic models (see Aguirregabiria
and Suzuki (2014), and Kalouptsidi et al. (2021)). In the equation describing value
differences 
vi as functions of the structural parameters, the payoff for the baseline
alternative interacts with the discount factor and with transition probabilities of the
state variables. Therefore, the effect on value differences (and on CCPs) of a change
in the discount factor or in transition probabilities depends on the level of the baseline

245

246

CHAPTER 4 Dynamic games in empirical industrial organization

payoffs, such that misspecification of baseline payoffs implies biases in the predictions about these effects.
A second problem comes from the identification of the time discount factor βi .
The identified value difference 
vi (ait , xt ) has two additive components: the difference
(a
,
x
)
−
πi (0, xt ); and the difference in continuation values
in current
payoffs,
π
i
it
t

βi xt+1 Vi (xt+1 ) [Fx,i (xt+1 |xt , ait ) − Fx,i (xt+1 |xt , 0)] where Fx,i is identified and
the value function Vi only depends on this transition probability and on πi , and βi .
Knowledge of the value difference 
vi (ait , xt ) is not enough to separately identify
πi (ait , xt ) − πi (0, xt ) and βi . The intuition is simple. Without further restrictions,
the difference in continuation values depends on the same variables as the difference in current payoffs. Therefore, the model can explain the data—i.e., the value
differences—equally well with any value of βi between 0 and 1.8

3.2.2 A set of sufficient conditions for identification
(a) Identification of single-agent dynamic model. Suppose that the researcher has data
only on agents’ choices and states, {aimt , xmt }. Consider the following additional
assumptions.
Assumption (ID.5). Normalization of payoff of one choice alternative. For one of
the choice alternatives, say ait = 0, the profit function is equal to zero (or to any other
value known to the researcher): πi (ait = 0, xt ) = 0 for any value of xt .
Assumption (ID.6). Known time discount factors. The discount factors βi are
known to the researcher.
Under conditions (ID.1) to (ID.6) all the structural parameters in θ ≡ {πi , βi ,
Fx , Fε : i ∈ I} are identified in the single-agent model.9 This set of identification
restrictions has been, so far, the most commonly used in empirical applications of
single-agent dynamic structural models.
(b) Identification of dynamic game. The identification of the dynamic game needs
to deal with two additional issues. First, the dynamic game can have multiple equilibria and, in principle, observations in the data may come from different equilibria.
Second, under conditions (ID.1) to (ID.6), and ignoring for the moment the issue of
multiple equilibria in the data, the expected profit function πiP (ait , xt )—as defined in
Eq. (4)—is identified. However, the profit function πi (ait , a −it , xt ) depends on the
actions of all the other players. That is, the dimension of the structural payoff function πi (ait , a −it , xt ) is larger than the dimension of the identified expected payoff
πiP (ait , xt ). As it is common in other empirical games or in econometric models with
social interactions, solving this identification problem requires exclusion restrictions.
Consider the following assumptions.
8 Note that the same non-identification holds if the discount factor is restricted to be the same across firms.
9 This positive identification result is a corollary of Magnac and Thesmar’s Proposition on non-

identification (Proposition 2 in Magnac and Thesmar (2002)). See also Proposition 1 in Aguirregabiria
(2005) for an explicit statement and proof of this positive identification result.

3 Identification and estimation

Assumption (ID.7). Single MPBNE in the data. Every observation (i, m, t) in the
sample comes from the same Markov perfect equilibrium.
Assumption (ID.8). Exclusion restriction in payoff. The vector of observable state
variables xt contains firm-specific state variables that enter in the profit function of
a firm but not in the profit function of competitors. More specifically, xt = (xct , zit :
i ∈ I), and the profit function is πi (a t , xct , zit ) that does not depend on zj t for j = i.
Furthermore, the support of zit has at least as many points as the support of ait .
Under conditions (ID.1) to (ID.8) all the structural parameters θ ≡ {πi , βi , Fx ,
Fε : i ∈ I} are identified in the dynamic game (see Proposition 3 in Pesendorfer and
Schmidt-Dengler (2008)). Similarly as for the case of single-agent models, these have
been the most commonly used identification restrictions in empirical applications of
dynamic games in IO.

3.2.3 Relaxing restrictions (ID.1) to (ID.8)
Some of the identification restrictions (ID.1) to (ID.8) are strong, and they might not
hold in some applications, such that imposing these restrictions may generate important biases in parameter estimates and in our understanding of firms’ behavior and
the determinants of market structure in those industries. During the last decade, there
has been a substantial amount of research dealing with identification results relaxing
some of the conditions (ID.1) to (ID.8). This subsection reviews this literature.
(i) Incorporating common-knowledge serially correlated unobservables
Assumption (ID.1) establishes that the only unobservables for the researcher are
the private information shocks, which are i.i.d. over firms, markets, and time. In most
applications in IO, this assumption is not realistic and can be easily rejected by the
data. Markets and firms differ in terms of characteristics that are payoff-relevant.
Some of these differences can be captured by state variables that the researcher observes and puts into the model, the xt ’s, but other variables are either tricky to measure
properly, or their inclusion in the model would expand the size of the state space in infeasible ways. It is difficult to believe that the state variables that the researcher does
not measure do not exhibit the same persistence over the time as the observed state
variables that are in xt . As such, no serial correlation of unobservables is a strong
assumption.
Not accounting for this heterogeneity may generate significant biases in parameter estimates and in our understanding of competition in the industry. For instance,
in the empirical applications in Aguirregabiria and Mira (2007) and Collard-Wexler
(2013), the estimation of a model without unobserved market heterogeneity implies
estimates of competition effects (i.e., in this case, the effect on a firms’ profit of other
firms’ market entry) that are strongly biased towards zero. In both applications, accounting for time-invariant unobserved market heterogeneity results in significantly
larger estimates of competition effects.
Kasahara and Shimotsu (2009) study the identification of CCPs in dynamic
discrete choice models—either single-agent or games—when the model includes

247

248

CHAPTER 4 Dynamic games in empirical industrial organization

time-invariant unobserved heterogeneity with finite support but with a nonparametric distribution. This unobserved heterogeneity may vary over firms, over markets,
or both. They derive sufficient conditions for nonparametric identification of the
CCP functions conditional on unobserved market type, and the distribution of the
unobserved types. That is, if ωm represents unobserved market heterogeneity, and
{ω(1) , ω(2) , ..., ω(L) } is the set of market types, they prove identification of the CCP
function Pi (ait |xt , ωm ) at every value in the support set of these variables, and of
the probability distribution of market types, λ(ωm ). The identification restrictions depend on the time-dimension of the panel data (T should be large enough), and on the
number of points in the support sets of the observable state variables xt and of the unobservable ωm . Given these CCPs, and the other identification restrictions described
above, all the structural parameters of the model are identified.
Hu and Shum (2012) (in a general Markov model) and Hu and Shum (2013) (more
specifically in dynamic games) extend this result on identification of CCPs to a model
where the unobservable ω can vary over time following a first order Markov process.
They present identification results for different models depending on whether the decision variable and the serially correlated unobservable ω are discrete or continuous.
(ii) Multiple equilibria in the data
In the context of discrete choice games of incomplete information, De Paula and
Tang (2012) propose a test of the restriction of unique equilibrium in the data based on
the independence between players’ actions conditional on observable state variables:
a test of the null hypothesis ait ⊥⊥ aj t |xt . They interpret failure of independence in
terms of multiple equilibria across markets. They also show (in a similar spirit as
Sweeting (2009)) that the sample variation generated by multiple equilibria across
markets provides identification of the sign of the parameters that capture the strategic
interactions between players (e.g., competition effects), without need of the exclusion
restrictions in (ID.8). A key restriction for de Paula and Tang’s identification results is
that the model does not contain (payoff relevant) common knowledge unobservables.
Otsu et al. (2016) propose statistical tests of the null hypothesis that panel data
from a discrete dynamic game can be pooled over multiple markets. This null hypothesis can be interpreted in terms of a restriction of no unobserved market heterogeneity, either payoff relevant or multiple equilibria. The asymptotics of the test is based
on large T . The authors apply their tests to data of the US Portland cement industry
from Ryan (2012) and reject the null hypothesis of homogeneity.
Aguirregabiria and Mira (2019) study the identification of discrete games of
incomplete information when there are two forms of market heterogeneity unobservable to the researcher but common-knowledge to the players: payoff-relevant
unobservables, and nonpayoff-relevant variables that determine the selection between
multiple equilibria. The number of equilibria in this class of models is (generically)
finite (see their Lemma 1, and also Doraszelski and Escobar (2010)) such that the
unobservable that represents the selection of an equilibrium has discrete and finite
support. Following Kasahara and Shimotsu (2009) and Hu and Shum (2013), the authors assume that payoff-relevant unobserved market heterogeneity has also discrete
and finite support. The authors provide necessary and sufficient conditions for the

3 Identification and estimation

identification of all the primitives of the model. Two types of conditions play a key
role in their identification results: independence between players’ private information, and the exclusion restriction in assumption (ID.8). This exclusion restriction
identifies which part of the unobserved heterogeneity affects the payoff function and
which part is associated to multiple equilibria because it affects players’ CCPs but
not the payoff function.
(iii) Identification without normalization restrictions
In the context of a single-agent model of market entry and exit, Aguirregabiria
and Suzuki (2014) show that three components of a firm’s profit function are not separately identified: the fixed cost of an incumbent firm, the entry cost of a new entrant,
and the scrap value (or exit cost) of an exiting firm. Empirical applications assume
that one of these three components is zero. Aguirregabiria and Suzuki (2014) study
the implications of these “normalization” restrictions on the predictions of counterfactual experiments using the estimated model. They show that the normalization is
innocuous (i.e., it does not introduce biases) for counterfactual experiments that consist of an additive change in the profit function. They also show that the normalization
restriction introduces important biases in the predictions from counterfactual experiments that change transition probabilities of the state variables or the discount factor.
The bias can modify even the sign of the effect. Kalouptsidi et al. (2021) extend this
analysis to a general framework that covers virtually any counterfactual encountered
in applied work in single-agent dynamic discrete choice models.
For dynamic games, Kalouptsidi et al. (2017) show that counterfactuals are not
identified, even when analogous counterfactuals of single-agent models are identified,
i.e., additive changes in players’ payoff functions. In dynamic games, a player’s best
response function depends on other players’ CCPs in a similar way as it depends
on transition probabilities of state variables. An additive change in player 1’s payoff
function affects the CCP of this player, and in turn this change affects player 2’s best
response. This second effect depends on the value of baseline payoffs.
A possible approach to deal with this identification problem is to use partial identification. There are weak and plausible restrictions on the sign of entry cost, fixed
cost, and scrap value that provide bounds on the estimation of all the structural parameters in the profit function. For instance, if the three components are always positive,
then the model implies sharper lower bounds for the entry cost and the scrap value.
Aguirregabiria and Suzuki (2014) describe this approach in the context of the model
of market entry/exit. Kalouptsidi et al. (2020) present a general partial identification
approach for the implementation of counterfactuals in dynamic discrete choice models.
In some industries, the acquisition of an incumbent firm is a common form of
firm entry and exit: the owner of an incumbent firm sells all the firm’s assets to a
new entrant. In the shipping, hotel, and banking industries this is frequently the case.
Sometimes, the researcher has data on firm acquisition prices, or else the firm’s underlying valuation can be recovered from stock market data or other assessments of
a firm’s value such as accounting statements. Under some assumptions, these additional data can be used to deal with the identification problem that we describe in this

249

250

CHAPTER 4 Dynamic games in empirical industrial organization

subsection. This is exactly the identification strategy used by Kalouptsidi (2014) for
the bulk shipping industry. Aguirregabiria and Suzuki (2014) discuss this approach
and the economic restrictions on transaction costs that it requires.
(iv) Identification of discount factors
The discount factor measures the strength of an agent’s forward-looking behavior.
For given payoff and transition probability functions, the discount factor plays a key
role in the optimal decision rule in a DP problem. However, without restrictions on
payoff or transition probability functions, the discount factor is not identified: see
Lemma 3.3 in Rust (1994b), and Proposition 2 in Magnac and Thesmar (2002).
An exclusion restriction that has certain power in the identification of the discount factor is a state variable that affects the expectation of future payoffs but not
current payoffs. Intuitively, if the agent’s behavior does not respond to changes in
this state variable, this is evidence that the agent is myopic; the stronger the observed
response, the more forward-looking the agent is. This exclusion restriction has been
used to identify the discount factor in different applications of single-agent dynamic
models.10 So far, this identification strategy has not been applied to dynamic games
in IO, possibly because most applications are to decisions about large firms where it
would be surprising that discount factors differ too much from the interest rates that
firms pay for capital.
These empirical applications also impose restrictions other than the exclusion restriction to identify the discount factor, such as parametric assumptions in the payoff
function. Therefore, it is not obvious what is the actual identification power provided by the exclusion restriction, and how much of the identification comes from
functional form restrictions. To answer this question, Abbring and Daljord (2020)
study the identification of the discount factor in a dynamic discrete choice model
under assumptions (ID.1) to (ID.5) but where the payoff function is nonparametrically specified. They show that the discount factor is partially identified, but it is not
point identified. More specifically, there are multiple (but finite) values of the discount factor that are consistent with the moment conditions implied by the exclusion
10 Chevalier and Goolsbee (2009) test whether textbook consumers are forward-looking. They consider

that the resale price of a textbook in the second hand market is the special state variable that does not
affect current utility of buying that textbook in the first-hand market but it affects future expected utility.
They find strong evidence that students are forward-looking. Fang and Wang (2015) study women’s decisions to get a mammogram. They assume that the mother’s age at death—truncated, if still alive—affects a
woman’s expectations but not her current utility. Bayer et al. (2016) estimate a dynamic model of housing
demand. Their identification of the discount factor exploits the assumption that the utility from housing
depends on the current level of the neighborhood amenities, but the variables representing amenities follow
a stochastic process with more than one-year memory such that lagged amenities shift consumer expectations but not current utility. In a model of consumer stockpiling decisions, Ching and Osborne (2020)
identify consumers’ discount factors under the assumption that current storage costs depend on the size
of the package—regardless of the level of inventory—such that the actual inventory of the product affects
expected future utilities but not current utility. De Groote and Verboven (2019) study households’ adoption
of solar photovoltaic systems, and their response to a generous subsidy program in Belgium. Their identification of the discount factor exploits that the program mainly consisted of future production subsidies
instead of upfront investment subsidies.

3 Identification and estimation

restriction. They also show that if the dynamic model exhibits finite dependence, as
defined in Arcidiacono and Miller (2011), the identified set is smaller. In a model
with τ -periods finite dependence, the identified set contains at most τ values. Therefore, in a model of market entry-exit—that has τ = 2 periods finite dependence—the
identified set for the discount factor contains only two values.
Komarova et al. (2018) study identification of the discount factor in a scenario
that is, somehow, the complement of Abbring and Daljord (2020). They consider a
model under assumptions (ID.1) to (ID.4) but without the exclusion restriction and
with a linear in parameters specification of the payoff function. That is, the payoff
function has the form πi (a t , xt ) = h(a t , xt ) θπi where h(a t , xt ) is a vector of functions known to the researcher, and θπi is a vector of parameters. They show that the
discount factor βi and the payoff parameters θπi are (generically) point identified.
Their identification proof is constructive and provides a simple two-step estimator.
(v) Non-additive unobservables
The Hotz-Miller inversion property—i.e., the one-to-one mapping between CCPs
and value differences—is a key component in the proofs of identification of dynamic
discrete choice structural models. This inversion property relies on the additive separability (and infinite support) of the unobservable shocks εit . This restriction, though
convenient, is not always plausible or desirable. For instance, in a model of competition in a differentiated product market with a logit demand system, unobservable
demand shocks (the so called ξ ’s) do not enter additively in a firm’s profit function.
Kristensen et al. (2015) show that additive separability is not necessary to obtain a
one-to-one mapping between CCPs and value differences. The necessary and sufficient condition for the inversion property is that, for every value of (a −it , xt ), the
vector of J +1 payoffs {πi (0, a −it , xt , εit ), πi (1, a −it , xt , εit ), ..., πi (J, a −it , xt , εit )}
has full support on the Euclidean space RJ +1 . This condition is satisfied by different
models where the shocks εit are not additively separable.
(vi) Non-parametric distribution of unobservable shocks
Most empirical applications of dynamic discrete choice structural models have assumed a parametric specification for the distribution of the unobservables. However,
it is well-known that, in discrete choice models, the misspecification of the distribution of unobservables Fε can generate substantial biases in the estimation of payoff
parameters (e.g., Horowitz (1993)). Relaxing this parametric assumption is quite relevant in this class of models. As in static discrete choice models, the shape of the
distribution of the unobservables plays a key role in the effect on the choice probability of a marginal change of a state variable or a structural parameter. Furthermore, in
dynamic discrete choice models, the distribution of the unobservables captures also
agents’ uncertainty about future payoffs and plays an important role in the magnitude
and shape of the continuation values of the dynamic decision problem.
Aguirregabiria (2010), based on results by Matzkin (1992), shows the nonparametric identification of the distribution Fε in a binary choice dynamic structural
model with finite horizon. A key condition in this identification result is the existence of an observable state variable that enters additively in the payoff function, i.e.,

251

252

CHAPTER 4 Dynamic games in empirical industrial organization

a so-called special regressor, using the term coined by Arthur Lewbel (see Lewbel
(1998) and Lewbel (2000)). Blevins (2014) extends this result to a more general class
of dynamic models in which agents can make both discrete and continuous choices.
Norets and Tang (2014) study partial identification when the model does not include
exclusion restrictions or “special” additive state variables, and the decision problem
can have infinite horizon. They derive sharp bounds on the distribution function Fε
and on per-period payoff functions πi . Buchholz et al. (2021) also consider an infinite
horizon model and do not impose the restriction of a special regressor. Instead, they
assume that the vector of state variables includes at least one continuous variable and
the payoff function is linear in parameters. They establish the nonparametric point
identification of Fε . Chen (2017) obtains point identification results under the restriction that a subset of the state variables affects only the current payoff function but not
agents’ beliefs about future utilities.
(vii) Non-equilibrium beliefs
Models of belief formation that depart from Bayesian Nash Equilibria can be
difficult since both payoffs and beliefs need to be identified. Aguirregabiria and
Magesan (2020) study the identification of biased beliefs in dynamic games. Their
model allows payoff and belief functions to vary over time in an unrestricted way.
First, the authors show that the exclusion restriction in (ID.8) provides testable nonparametric restrictions of the null hypothesis of equilibrium beliefs in dynamic games
with either finite or infinite horizon. Second, they prove that this exclusion restriction, together with consistent estimates of beliefs at two points in the support of the
variable involved in the exclusion restriction, is sufficient for non-parametric pointidentification of players’ belief functions and payoff functions. They apply these
results to a dynamic game of competition in number of stores between McDonalds
and Burger King in UK. They find significant evidence of biased beliefs by Burger
King. Imposing the restriction of unbiased beliefs generates a substantial attenuation
bias in the estimate of competition effects.
An et al. (2021) study the identification of dynamic discrete choice models without assuming rational expectations. For finite horizon models, their key identification
restriction is that payoff function and transition probabilities are time invariant. Under
this restriction, all the variation over time in the CCP functions should be attributed
to the proximity to the terminal period. This implies a recursive relationship between
CCPs at two consecutive periods. The authors show that this relationship provides
identification of subjective beliefs. For the identification of subjective beliefs in infinite horizon models, they impose the stronger restriction that for one of the state
variables, agents’ have rational expectations.

3.2.4 Identification of mixed continuous-discrete choice models
Blevins (2014) studies the identification of dynamic structural models with a discrete
decision, dit ∈ {0, 1, ..., J }, and a continuous decision, cit ∈ R. For instance, in the
model of Sweeting and Bhattacharya (2015), firms choose to enter auctions based on
the valuation they have. Thus, the entry and bidding problem are linked. More gener-

3 Identification and estimation

ally, we often have some information on firms outcomes post-entry, and this creates
a selection problem of what ’s are observed in the market. In Blevins (2014), the
model includes two types of unobservable state variables: unobservables associated
with the discrete choice, (εit (d) : d = 0, 1, ..., J ) ∈ RJ +1 ; and an unobservable associated with the continuous choice, ηit ∈ R. For instance, consider a firm manager
that every period decides whether to operate a production plant (dit = 1) or to keep
it idle (dit = 0), and conditional on operating, the manager chooses the amount of
output to produce, cit . Unobservable εit (1) − εit (0) represents a shock in the fixed
cost of starting up the plant. Unobservable ηit is a shock in the marginal cost of
output.
Blevins (2014)’s model maintains all the assumptions (ID.1) to (ID.8) presented
above. More specifically, the unobservables εit and ηit satisfy the conditional independence assumption (ID.4), and the discrete unobservables are additively separable in the payoff function, as in assumption (ID.2). Firm i’s profit function is
πi (dit , cit , xt , ηit ) + εit (dit ). Blevins includes three additional assumptions related
to the continuous-choice part of the model. First, shocks εit and ηit are revealed to
the firm sequentially within each period. The firm observes first the discrete-choice
shocks εit and makes its discrete choice at period t. Then, after making its discrete
choice, the continuous-choice shock ηit is revealed and the firm makes its continuous decision. A second assumption is that conditional on xt , the unobservables εit and
ηit are independently distributed. These are arguably strong assumptions that may not
hold in some applications. However, these two assumptions facilitate substantially the
analysis of this model. In particular, they imply that the conditional discrete-choice
value functions at the beginning of period t (conditional on optimal continuous decision and integrated over the distribution of ηit ) have the standard structure in the
literature where all the unobservables are additively separable. Finally, he assumes
that the marginal profit function ∂πi /∂cit is strictly monotonic in cit and ηit . This is
a standard condition in continuous decisions models.
Under these conditions, Blevins (2014) proves the nonparametric identification of
the profit function πi (.). Note that this function is nonparametric in all its arguments,
including the unobservable ηit . He presents identification results both when the distribution functions Fε and Fη are known to the researcher and when these functions
are nonparametrically specified.

3.3 Estimation methods
The primitives of the model, {πi , βi , Fx , Fε : i ∈ I}, can be described in terms of a
vector of structural parameters θ that is unknown to the researcher. In this section, we
describe methods for the estimation of θ , as well as different econometric issues. It
is convenient to distinguish three components in the vector of structural parameters:
θ = (θ π , θ f , β), where θ π represents the parameters in payoff functions and in the
distribution of the unobservables (if any), θ f contains the parameters in the transition
probabilities of state variables, and β is the vector of discount factors.

253

254

CHAPTER 4 Dynamic games in empirical industrial organization

3.3.1 Full solution methods
(i) Nested fixed point algorithm with equilibrium uniqueness
Rust’s nested fixed point algorithm (NFXP; Rust (1987), Rust (1994b)) was a
fundamental development in the estimation of dynamic structural models. NFXP is
a gradient iterative search method to obtain the maximum likelihood estimator of
the structural parameters. It was originally proposed for single-agent models, but it
has been applied also to the estimation of games with unique equilibrium (e.g., Seim
(2006), Abbring and Campbell (2010), and Igami (2017)). The own concept of a likelihood function—and not a correspondence—seems to imply that the model has only
one equilibrium for each value of the structural parameters. The condition of equilibrium uniqueness has been common in applications of NFXP to games. However,
as we describe below, a conceptually simple modification of this algorithm can be
applied to estimate dynamic games with multiple equilibria. The problem is not the
conceptual definition of this algorithm but the computational cost of implementing it.
For the moment, suppose that the dynamic game has a unique MPBNE for every
value of the structural parameters.11 Let {Pi (ai |x, θ ) : i ∈ I} be the equilibrium CCPs
associated with a value of the structural parameters θ . Under
 assumptions (ID.1) to
(ID.4) the full log-likelihood function of the data is (θ ) = M
m=1 m (θ ), where m (θ )
is the contribution of market m and has the following form:
m (θ ) =

T
N 


log Pi (aimt |xmt , θ ) + log fx (xm,t+1 |a mt , xmt , θ f ) + log Pr(xm1 |θ )

i=1 t=1

(8)
where fx is the transition density function, and log Pr(xm1 |θ ) is the contribution of
the initial conditions to the likelihood, e.g., the observed market structure at the
first sample period. Most applications imposing the restriction of no serially correlated unobservables follow a conditional likelihood approach that ignores the term
log Pr(xm1 |θ ). Though this approach is consistent as long as there are not serially
correlated unobservables, it implies a loss of efficiency that can be important in stationary dynamic games.12 Under the conditional independence assumption (ID.4),
the subvector of structural parameters θ f can be estimated separately from the rest of
the parameters without solving for an equilibrium of the game. To reduce the computational cost in the estimation of the model, most applications use a sequential
approach wherethe parameters θ f are estimated in a first step based on the partial likelihood m,t log fx (xm,t+1 |a mt , xmt , θ f ), and in a second step the rest of
c
the
 parameters are estimated using the conditional partial likelihood  (θ π , β) =

i,m,t log Pi (aimt |xmt , θ π , β, θ f ).
The NFXP combines a Berndt et al. (1974) (BHHH hereafter) method for the
outer algorithm, that searches for a root of the likelihood equations, with a solution
11 As we have mentioned in Section 2.2.4 above, equilibrium uniqueness in this class of dynamic games

requires strong restrictions.
12 In applications with serially correlated unobservables, accounting for the endogeneity of the initial

conditions is key to generate consistent estimators. We describe this in Section 3.3.5 below.

3 Identification and estimation

algorithm (inner algorithm) that solves for the MPBNE of the game for each trial
value of the structural parameters. The algorithm is initialized with an arbitrary vector
of structural parameters, say 
θ 0 . A BHHH iteration is defined as:

θk +
θ k+1 = 

 M
−1  M

 ∂m (
 ∂m (
θ k ) ∂m (
θk)
θk)
m=1

∂θ

∂θ

m=1

∂θ

(9)

θ k )/∂θ depends on ∂ log Pi (aimt |xmt , 
θ k )/∂θ . To obtain these
The score vector ∂m (
derivatives, the inner algorithm of NFXP solves for the equilibrium CCPs given 
θk.
This solution algorithm can be based on value function iterations, or policy function
iterations, or a hybrid of the two. As any other gradient method, the NFXP algorithm
returns a solution to the likelihood equations.13
There is a long list of applications in IO which have used the NFXP algorithm
to estimate single-agent Rust models. For instance, the machine replacement model
used in Rust (1987), Das (1992), Rust and Rothwell (1995). The list of applications
for dynamic games is shorter but includes important recent contributions such as
Igami (2017) and Igami and Uetake (2020).
(ii) Maximum likelihood estimation with multiple equilibria
A modified version of NFXP can be applied to obtain the maximum likelihood estimator (MLE) in games with multiple equilibria. To define the MLE in a model with
multiple equilibria, it is convenient to define an extended or pseudo likelihood function. For arbitrary values of the vector of structural parameters θ and firms’ CCPs P,
we define the following likelihood function of observed players’ actions conditional
on observed state variables:
Q(θ , P) =

N 
T
M 


log i (aimt | xmt , θ , P)

(10)

m=1 i=1 t=1

where i is the best response probability function that we have defined in Eq. (6). We
call Q(θ , P) a pseudo likelihood function because players’ CCPs in P are arbitrary
and do not represent the equilibrium probabilities associated with θ implied by the
model. An implication of using arbitrary CCPs, instead of equilibrium CCPs, is that
likelihood Q is a function and not a correspondence.
The MLE is defined as the pair (
θ MLE , 
PMLE ) that maximizes the pseudo likelihood subject to the constraint that the CCPs in 
PMLE are equilibrium strategies
associated with 
θ MLE . This is a constrained MLE can be defined as the solution of
the following Lagrangian problem:
(
θ MLE , 
PMLE , 
λMLE ) = arg max Q(θ , P) + λ [P − (θ , P)]
(θ,P,λ)

(11)

13 In general, the likelihood function of this class of models is not globally concave. Therefore, some

global search is necessary to check whether the root of the likelihood equations that has been found is
actually the global maximum and not just a local optimum.

255

256

CHAPTER 4 Dynamic games in empirical industrial organization

where λ is the vector of Lagrange multipliers, and λ, P, and (θ, P) are vectors
where each element corresponds to a value of (i, aimt , xmt ). This constrained MLE
satisfies the standard regularity conditions for consistency, asymptotic normality, and
efficiency of maximum likelihood estimation.
In principle, this constrained MLE can be computed using Newton or QuasiNewton methods. The first order conditions of this problem imply the following
Lagrangian equations:
⎧
⎪
⎪
⎨
⎪
⎪
⎩


PMLE − (
θ MLE , 
PMLE ) = 0
θ MLE , 
PMLE ) − 
λMLE θ (
θ MLE , 
PMLE ) = 0
θ Q(

(12)

θ MLE , 
PMLE ) − 
λMLE P (
θ MLE , 
PMLE ) = 0
P Q(

A Newton method can be used to obtain a root of this system of Lagrangian equations.
However, a key computational problem is the very high dimensionality of this system
of equations. In the empirical applications of dynamic oligopoly games, the vector of
probabilities P—and the vector of Lagrange multipliers λ—includes thousands, millions, or even more elements. In particular, the computationally most intensive part
of this algorithm is in the calculation of the Jacobian matrix P (
θ ,
P). In dynamic
games, in general, this is not a sparse matrix, and can contain billions or trillions
of elements. Furthermore, the evaluation of the best response mapping (θ , P) for a
new value of P requires solving for a valuation operator and solving a system of equations with the same dimension as P. Therefore, if L = N |A||X | is the dimension of
the vector P, the evaluation of the Jacobian matrix P (
θ ,
P) requires solving of the
2
order of L systems of linear equation with dimension L. Furthermore, this Jacobian
matrix needs to be recomputed at each iteration of the Newton’s method. Given the
value of L in empirical applications, this approach is impractical in most empirical
applications. Su and Judd (2012) have proposed using an MPEC algorithm, which is
a general purpose algorithm for the numerical solution of constrained optimization
problems. However, MPEC also requires the repeated computation of the high dimensional and non-sparse Jacobian matrix P (
θ, 
P). Due to serious computational
issues, there are no empirical applications of dynamic games with multiple equilibria
that compute the MLE, with either the NFXP or MPEC algorithms.
(iii) Nested pseudo maximum likelihood estimation
Motivated by the computational challenges of implementing the MLE in dynamic
structural models (and by limitations of the two-step methods that we describe below), Aguirregabiria and Mira (2002) (for single-agent models) and Aguirregabiria
and Mira (2007) (for dynamic games) propose an alternative estimation method
that imposes the equilibrium restrictions but requires neither repeatedly solving for
equilibrium CCPs for each trial value of the structural parameters (as in the NFXP
algorithm), nor computing Jacobian matrices P (
θ, 
P) (as in the NFXP and MPEC
algorithms). They denote their method the Nested pseudo likelihood (NPL hereafter)
estimator.

3 Identification and estimation

In the NPL method, the analog to a root of the likelihood equations is a NPL
root (or NPL fixed point). A NPL root is defined as a vector of structural paramPN P L ), that satisfy two conditions: (1) given
eters and a vector of CCPs, (
θ N P L, 

PN P L , the vector of structural parameters maximizes the pseudo likelihood function,

PN P L ); and (2) given 
θ N P L , the vector of CCPs satisfies
θ N P L = arg maxθ Q(θ , 
θ N P L, 
PN P L ). Define the NPL mapping
the equilibrium restrictions, 
PN P L = (
θ (P), P) where 
θ (P) represents the
ϕ : [0, 1]N |A||X | → [0, 1]N |A||X | as ϕ(P) ≡ (
value of θ that maximizes Q given P. Using this mapping, we can define a NPL root
as a fixed point of the NPL mapping:


PN P L ) = 0
PN P L − ϕ(
(13)

θ (
PN P L ) = 0.
θ NP L − 
The NPL estimator is defined as the NPL root with the largest value of the pseudo
likelihood. The NPL estimator is consistent and asymptotically normal under the
same regularity conditions as the MLE (Proposition 2 in Aguirregabiria and Mira
(2007)). For dynamic games, the NPL estimator has larger asymptotic variance than
the MLE. In single-agent dynamic models, the two estimators are asymptotically
equivalent (Proposition 4 in Aguirregabiria and Mira (2002)).
To compute a NPL root, Aguirregabiria and Mira propose a simple algorithm that
consists of successive iterations in the NPL mapping ϕ. They denote it NPL fixed
point algorithm. Starting with an initial P0 , at iteration k ≥ 1 the vector of CCPs
is updated using Pk = ϕ(Pk−1 ). This updating or fixed point iteration involves two
θ (Pk−1 ) by solving in θ the
calculations: (1) obtaining the pseudo ML estimator 
θk = 
θ k and Pk−1 , obtain players’ best response
system θ Q(θ , Pk−1 ) = 0; and (2) given 
CCPs if the other players behave according to Pk−1 and the structural parameters are

θ k , Pk−1 ). Computation (1) is very simple in most applications, as
θ k , i.e., Pk = (
it is equivalent to obtaining the MLE in a static single-agent discrete choice model.
The main computational task in (2) comes from the calculation of present values that
is equivalent to solving once a system of linear equations with the same dimension as
P. Therefore, one iteration of this algorithm is several orders of magnitude cheaper
than one Newton or MPEC iteration for the solution of the MLE. This is because
an iteration in the NPL mapping does not involve solving for an equilibrium (as in
NFXP) or calculating the non-sparse Jacobian matrix P (θ, P) (as in MPEC).
The NPL estimator has been used in a good number of empirical applications
in IO, for single-agent dynamic models (Copeland and Monnet, 2009; De Pinto and
Nelson, 2009; Tomlin, 2014; Aguirregabiria and Alonso-Borrego, 2014; Huang et
al., 2015), dynamic games (Sweeting, 2013; Aguirregabiria and Ho, 2012; CollardWexler, 2013; Kano, 2013; Huang and Smith, 2014; Lin, 2015; Gayle and Xie, 2018),
static games (Ellickson and Misra, 2008; Han and Hong, 2011) and networks (Lin and
Xu, 2017; Liu and Zhou, 2017).
An important limitation of the NPL is that, in games, the mapping ϕ(P) is not
a contraction, so that fixed point iterations do not guarantee convergence. In fact,
mapping ϕ may have multiple fixed points, and the fixed point algorithm may con-

257

258

CHAPTER 4 Dynamic games in empirical industrial organization

verge to a solution that is not the consistent NPL root. This issue has been pointed
out and illustrated with numerical examples by Pesendorfer and Schmidt-Dengler
(2010), Egesdal et al. (2015), Kasahara and Shimotsu (2012), and Aguirregabiria
and Marcoux (2021). It has also motivated different authors to propose algorithms to
compute the NPL estimator that share the low cost per iteration of fixed point NPL
iterations but that have better convergence properties when the NPL mapping is not
a contraction.
One way to resolve issues of convergence with the NPL is to modify the update
rule for P. Kasahara and Shimotsu (2012) propose a relaxation method that modifies
the NPL mapping so that P updated more slowly, where the speed of update is controlled by a tuning parameter α. However, as shown in the numerical experiments in
Egesdal et al. (2015) and Aguirregabiria and Marcoux (2021), this approach comes
at the cost of slower convergence. Aguirregabiria and Marcoux (2021) propose instead to use a spectral algorithm. A key feature of this approach is that the stepsize
is updated at each iteration, and no derivatives need to be computed. They apply this
spectral algorithm to multiple data generating processes from dynamic games, including those considered by Pesendorfer and Schmidt-Dengler (2010) and Egesdal
et al. (2015), and find that it converges to the NPL estimator for every Monte Carlo
simulated sample.

3.3.2 Two-step CCP methods
To avoid the large computational cost of full-solution methods, simpler two-step
methods have been proposed. Hotz and Miller (1993) was a seminal contribution on
this class of methods. In a single-agent model, under assumptions (ID.1) to (ID.4),
they show that the conditional choice value function (as defined in Eq. (2) above)
can be written as known functions of CCPs, transition probabilities, and one-period
payoffs πi . If the flow payoff function is linear in parameters, πi (ait , xt ) = h(ait , xt )
θ π,i , this representation is particularly simple:
hPi (ait , xt ) θ π,i +
eiP (ait , xt )
vi (ait , xt ) = 

(14)

eiP (ait , xt ) are the expected present values of their untilded
where 
hPi (ait , xt ) and 
counterparts h and e:
⎛
⎞
∞

j

hPi (ait , xt ) = E ⎝ βi h(ait+j , xt+j ) | ait , xt ⎠
j =0

⎞
⎛
∞

j

eiP (ait , xt ) = E ⎝ βi eiP (ai,t+j , xt+j ) | ait , xt ⎠

(15)

j =0

where future actions are drawn from the CCPs in vector P. Function eiP (j, xt ) represents the conditional expectation E(εi (j )|xt , ait = j ) and it is a known function of
the CCPs at xt ; i.e., the expectation of shocks conditional on firms behaving optimally

3 Identification and estimation

with conditional choice probabilities P. For instance, when ε’s are i.i.d. extreme value
type I, we have that eiP (j, xt ) = γ − log Pi (j |xt ) where γ is Euler’s constant. The
present values in Eq. (15) can be represented as known functions of CCPs, transition
probabilities, and discount factor. More precisely,

hPi (ait , xt ) = h(ait , xt ) + βi

eiP (ait , xt ) = βi





P (x
fx (xt+1 |ait , xt ) Wh,i
t+1 )

xt+1
P (x
fx (xt+1 |ait , xt ) We,i
t+1 )

(16)

xt+1
P (x ), W P (x )] : x ∈ X } can be obtained solvand the matrix of values WPi = {[Wh,i
t
t
e,i t
ing the following systems of linear equations:

WPi =

J


Pi (ai ) ◦

!"

$
#
hi (ai ), ePi (ai ) + βi Fx,i (ai ) WPi

(17)

at =0

where ◦ is the element-by-element or Hadamard product; Pi (ai ) is the column vector
of choice probabilities (Pi (ai |x) : x ∈ X ); Fx,i (ai ) is the matrix of transition probabilities of x given choice ai ; hi (ai ) is the matrix (hi (ai , x) : x ∈ X ); and ePi (ai )
is the vector (eiP (ai , x) : x ∈ X ). One can compute WPi using efficient methods for
solving systems of linear equations, and exploit the possible sparsity of the transition matrices Fx,i (ai ). Solving this system of linear equations has a complexity of
at most (worst case) O(|X |3 ) where |X | is the dimension of the state space. This
complexity is of the same order as solving the DP problem once. From the point of
view of estimation, the main advantage of this representation is that—combined with
initial reduced form estimates of the CCPs—can be used to estimate the structural
parameters without having to solve repeatedly the DP problem.
Hotz and Miller (1993) also show that for DP problems with an absorbing state, so
called optimal stopping problems, the representation of the conditional choice value
functions in Eq. (14) becomes extremely simple. To illustrate this assumption, the
application in Hotz and Miller (1993) is the choice to have a vasectomy when families
are choosing the number of children to have. Likewise, in many of the market entry
and exit models considered in this chapter, exit is a permanent decision (e.g., CollardWexler (2013); Dunne et al. (2013)). In these models, vi (ait , xt ) can be represented
using CCPs and transitions at only periods t and t + 1. More specifically, if ait = 0
represents the stopping decision and ait = j is any other choice alternative, we have
that:
vi (j, xt ) − vi (0, xt ) = πi (j, xt ) − πi (0, xt )
"
#
+ βi Et πi (j, xt+1 ) − πi (0, xt+1 ) + eiP (j, xt+1 ) .

(18)

It is clear that the representation in Eq. (18) is computationally much simpler than
the general representation in Eq. (14): a complexity of O(|X |) instead of O(|X |3 ).

259

260

CHAPTER 4 Dynamic games in empirical industrial organization

Arcidiacono and Miller (2011) generalize this result to DP models with finite dependence structure, which is a substantially broader class than optimal stopping
problems. We describe this extension in Section 3.3.4 below.
Given either the general representation in (14) or the finite dependence representation in (18), the pseudo likelihood function Q(θ , P) has practically the same structure
as in a static or reduced form discrete choice model. That is, the best response probabilities i (aimt |xmt , θ , P) that enter in the pseudo likelihood Q(θ , P) can be seen as
the choice probabilities in a standard random utility model:
%
&
'(
i (aimt |xmt , θ , P) = Pr aimt = arg max 
eiP (j, xmt ) + εit (j ) .
hPi (j, xmt ) θ i +
j

(19)
Given 
hPi (., xmt ) and 
eiP (., xmt ) and a parametric specification for the distribution of
ε (e.g., logit, probit), the vector of parameters θ i can be estimated as in a standard
logit or probit model.
The method proceeds in two steps. Let P0 be the vector with the population values of the CCPs. Under assumptions (ID.1) to (ID.4), these CCPs can be estimated
consistently using standard nonparametric methods. Let 
P0 be a consistent nonpara0
metric estimator of P . The two-step estimator of θ is defined as:

θ 2S = arg max Q(θ , 
P0 ).
θ

(20)

Under standard regularity conditions, this two-step estimator is root-M consistent
and asymptotically normal (see Proposition 2 in Hotz and Miller (1993), and more
generally Newey (1994)). Aguirregabiria and Mira (2002) show that, in single-agent
models, this two-step estimator based on the maximization of the pseudo likelihood
function Q is asymptotically efficient due to the zero Jacobian property in this class
of models.14
The first empirical applications of CCP methods in empirical IO were Slade
(1998) and Aguirregabiria (1999) on the estimation of dynamic models of firms’ pricing and inventory decisions.15 Different versions of this two-step method have been
proposed and applied to the estimation of dynamic games by Aguirregabiria and Mira
(2007), Bajari et al. (2007), Pakes et al. (2007), and Pesendorfer and Schmidt-Dengler
(2008).
14 In single-agent dynamic discrete choice models, the Jacobian matrix  (θ, P) evaluated at a fixed
P

point P is zero (Proposition 2 in Aguirregabiria and Mira (2002)). Therefore, at the population parameters
(θ 0 , P0 ) we have that P (θ 0 , P0 ) = 0, and this implies asymptotic independence between the first step
estimator of P0 and the second step pseudo maximum likelihood estimator of θ 0 , and asymptotic efficiency
of the later (Proposition 4 in Aguirregabiria and Mira (2002)).
15 For the computation of 
hP
eiP (ait , xt ), Hotz and Miller (1993) considered only finite horii (ait , xt ) and 
zon models and optimal stopping models. For infinite horizon models, they suggest treating them similarly
as finite horizon models by truncating the future stream of payoffs. Aguirregabiria (1999) was the first
paper to consider the representation of the present values 
hP
eiP (ait , xt ) in the infinite horizon
i (ait , xt ) and 
stationary DP problems as presented above in Eqs. (16) and (17). This representation has been used later
in IO applications of CCP methods.

3 Identification and estimation

In dynamic games, the two-step pseudo likelihood estimator in Eq. (20) is not
asymptotically efficient because the zero Jacobian property does not hold in dynamic
games. Pesendorfer and Schmidt-Dengler (2008) propose a two-step estimator for
dynamic games that is asymptotically efficient. Their estimator belongs to a general
class of minimum distance (or asymptotic least squares) estimators described by the
following expression:
"
"
!
$#
!
$#

AM 
(21)
P0 −  
P0 −  
θ = arg min 
P0 , θ
P0 , θ
θ

where AM is a weighting matrix. Each estimator within this general class is associated with a particular choice of the weighting matrix. The asymptotically optimal
estimator within this class has the following weighting matrix:
A∗M =

"
"
#
#−1
0 0
0 0
I − P (θ , P ) 
P0 I − P (θ , P )

(22)

0
where 
P0 is the variance matrix of the initial nonparametric estimator P . Pesendorfer and Schmidt-Dengler (2008) show that this estimator is asymptotically equivalent
to the MLE defined in Eq. (11). Therefore, there is no loss of asymptotic efficiency
by using this two-step estimator of the structural parameters instead of the MLE.
From a computational point of view, in contrast to the computation of the MLE, this
two-step estimator requires computing the Jacobian matrix P  only once. Srisuma
and Linton (2012) generalize this method to dynamic games with continuous state
variables.
This family of two-step estimation methods—often referred as CCP estimators—are very attractive because they reduce substantially the computational cost
of estimating dynamic models. However, they also have some limitations. A first
limitation is the restrictions imposed by the assumption of no unobserved common
knowledge variables. Ignoring persistent unobservables, if present, can generate important biases in the estimation of structural parameters. In Section 3.3.5, we review
two-step methods that allow for persistent unobserved heterogeneity. Nevertheless,
the type of unobserved heterogeneity that we can identify when using two-step methods is substantially more restrictive than when using full-solution methods.16
A second limitation of two-step methods is their finite sample bias. The initial
nonparametric estimator can be very imprecise given the sample sizes and the dimension of the vector of state variables that we have in empirical applications in
IO. In dynamic games with heterogeneous players, the number of observable state
variables is proportional to the number of players and therefore the so called curse
16 This is because two-step methods require nonparametric identification of CCPs (conditional on unob-

served types) in the first step. The conditions for nonparametric identification of CCPs with unobserved
heterogeneity (i.e., nonparametric finite mixture models) are stronger than for the identification of the
structural model imposing all its restrictions, e.g., exclusion restrictions in profit functions. See the results
on this point in Section 5 in Aguirregabiria and Mira (2019).

261

262

CHAPTER 4 Dynamic games in empirical industrial organization

of dimensionality in nonparametric estimation can be particularly serious. The finite sample bias and variance of the first-step estimator can generate serious biases
in the two-step estimator of structural parameters. To reduce this finite sample bias,
Aguirregabiria and Mira (2002) have proposed a method that consists of fixed point
iterations in the NPL mapping defined in Section 3.3.1. In single-agent models, this
NPL mapping is a contraction, and this iterative procedure reduces higher order approximations to the bias (see Kasahara and Shimotsu (2009)). However, this is not the
case in dynamic games, and this procedure may increase the bias, and even converge
to an inconsistent estimator (Pesendorfer and Schmidt-Dengler, 2010). The development of an estimation procedure for dynamic games that guarantees bias reduction
of two-step estimators—but still is substantially cheaper to implement than full solution methods—is an interesting topic of methodological research in this field that
still needs further developments.

3.3.3 Bajari-Benkard-Levin (BBL) method
Bajari et al. (2007) propose a two-step method for the estimation of dynamic games—
the so called BBL method—that has received substantial attention in empirical IO
applications. This method has several distinguishing features with respect the twostep Hotz-Miller method described in Section 3.3.2 above. First, BBL uses Monte
P (x ) and W P (x ).
Carlo simulation to approximate the expected present values Wh,i
t
e,i t
Second, the estimator of the structural parameters in the second step is based on
moment inequalities instead of pseudo maximum likelihood (as in Aguirregabiria
and Mira (2007)), GMM (as in Pakes et al. (2007)), or minimum distance (as in
Pesendorfer and Schmidt-Dengler (2008)). Finally, the BBL method can be applied
to models with discrete or/and continuous decision and state variables.
(i) Monte Carlo approximation of present values. For the state spaces than we find in
many applications of dynamic games (with millions or billions of states), the exact
computation of the present values in Eq. (17) is impractical, even if this evaluation
needs to be done only once for the estimation of the model. An approach to deal with
this issue consists in approximating expected present values using Monte Carlo simulation, an approach used early on by Pakes (1986) and called forward-simulation. In
single-agent dynamic discrete choice models, Hotz et al. (1994) propose this simulation approach in combination with Hotz-Miller two-step method. Given xt and ait ,
we can use the estimated transition probability function fx (.|ait , xt , 
θ f ) to generate
a random draw for xt+1 . And given this simulated value of xt+1 , we can use the esi (.|xt+1 ) to generate a random draw for the optimal choice
timated CCP function P
ai,t+1 . We proceed sequentially to generate a simulated path of actions and states between periods t + 1 and t + T ∗ for some pre-specified time horizon T ∗ . We can gener(r)
(r)
ate many of these simulated paths. Let {ai,t+s , xt+s : s = 1, 2, ..., T ∗ ; r = 1, 2, ..., R}
be R simulated paths, all of them starting from the sample observation (ait , xt ). Then,

3 Identification and estimation

the Monte Carlo approximation to the expected present value 
hPi (ait , xt ) is:
⎞
⎛ ∗
R
T
$
!


1

⎠.
⎝ βis h a (r) , x(r)
hP,R
i,t+s t+s
i (ait , xt ) = h(ait , xt ) +
R
r=1

(23)

s=1

And we can use a similar expression to approximate the expected present value

eiP (ait , xt ). Bajari et al. (2007) adapt this approach to approximate expected present
values in dynamic games.
P,R
This Monte Carlo approximation implies an approximation error, uR
it = hit −

hPit . This approximation error goes to zero as R goes to infinity, but it can be substantial for the finite value that we use in an application. Simulation errors can increase
the bias and variance of our estimators. However, for simulation-based GMM estimators where the simulation error enters additively in the moment conditions, this error
does not generate (first order) asymptotic bias in the estimator and the estimator is
consistent as the sample size goes to infinity but the number of simulations R is fixed
(McFadden, 1989).17 This nice property of some simulation based GMM estimators
is shared by the method proposed by Hotz et al. (1994).
(ii) Moment inequalities. The value of firm i at state xt when all the players behave
according to their strategies in P can be written as:
ViP (xt ) = WiP (xt ) θ i
(24)
"
#
P (x ), W P (x ) , and θ ≡ (θ
where WiP (xt ) ≡ Wh,i
t
i
π,i , 1). For notational simplicity,
e,i t
below we use WitP to represent WiP (xt ). We can split the vector of CCPs P into two
sub-vectors: Pi with firm i’s CCPs, and P−i containing the probabilities of firms other
than i. Since P0 is an equilibrium associated to θ 0 , we have that P0i is firm i’s best
response to P0−i . Therefore, for any vector Pi = P0i , the following inequality holds:
 0 0 
P ,P−i

Wit i



Pi ,P0−i

θ 0i ≥ Wit

θ 0i .

(25)

We can define an estimator of θ 0 based on these (moment) inequalities. There are infinite alternative policies Pi , and therefore there are infinite moment inequalities. For
estimation, we should select a finite set of alternative policies. Indeed, a larger number of moments may lead to worse estimates in terms of larger variance, but tighter
identified sets.18 This is a very important choice for the researcher in the implementation of the BBL estimator (see our discussion below). Let P be a (finite) set of
17 Of course, the asymptotic variance of the simulation-based GMM estimator, and higher order approx-

imations to the bias, still depend on (decline with) the number of simulations R.
18 There is a large recent literature on moment selection and computing confidence set for models defined

by moment inequalities. For instance, Andrews and Soares (2010), Bugni (2010), Canay (2010), and Romano et al. (2014) study selection of unconditional moment inequalities with varying procedures, while
conditional moment inequalities are addressed in Andrews and Shi (2013), Chernozhukov et al. (2013).

263

264

CHAPTER 4 Dynamic games in empirical industrial organization

alternative CCPs selected by the researcher. Define the following criterion function:

)  0 0 

 * (2
!
$  %
P ,P
Pi ,P0
 θ , P0 ≡
min 0 ; Wimti −i − Wimt −i θ i
Q
.
(26)
m,i,t P∈P

This criterion function, proposed by Chernozhukov et al. (2007), penalizes departures
from the inequalities. Given an initial nonparametric estimator of P0 , and replacing
P with Monte Carlo approximations, Bajari et al. (2007)
exact present values Wimt
 ,
propose an estimator that minimizes in θ the criterion function Q(θ
P0 ).
In this model, the vector of structural parameters θ is point identified. However, in
most applications of the BBL method, the relatively small set of alternative CCPs, P,
selected by the researcher does not provide enough moment inequalities to achieve
point identification such that the BBL method provides set estimation of the structural
parameters.
This BBL estimator has been applied in a good number of important empirical
applications of dynamic games in IO and marketing, such as Ryan (2012), Ryan and
Tucker (2012), Suzuki (2013), Jeziorski (2014), Fowlie et al. (2016), Hashmi and
Biesebroeck (2016), and Lim and Yurukoglu (2018), among others.
The distinguishing features of BBL method are key to explain its relative popularity. Monte Carlo approximation of present values can make the difference between
being able to estimate a dynamic model or not. Nevertheless, this approximation
method can be used along with any of the other estimation methods described above,
either two-step or full solution methods. The applicability of the BBL method to
models with either continuous or discrete variables is also very convenient, and it
is a more substantial feature that distinguishes this method. Last but not least, the
researcher’s selection of the set of alternative CCPs to estimate the parameters (the
set P) can be quite attractive in some applications. Any model has its strengths and
weaknesses, and sometimes a model provides a poor match for some aspects of the
data that are not important to answer the main questions that motivate the paper. The
freedom provided by the selection of the set P allows the researcher to focus on those
predictions of the model that are key for the specific research questions. It can also
provide a more clear and intuitive picture on the contribution of different features in
observed firms’ behavior and the identification of some parameters.
Nevertheless, these attractive features also have limitations. In some applications,
the number of possible forward paths of length T ∗ is greater than the number of
atoms in the universe, but we use only a few million paths to approximate expected
present values. These approximations can be seriously biased, but we do not have any
practical way of knowing the order of magnitude of this bias in our specific application. Also, the selection of the set P can hide (intentionally or unintentionally) some
sources of misspecification in the model which may be important for the purpose of
the research.
For work with a large number of moment inequalities, which is typical of applications such as BBL, work
such as Belloni et al. (2019), Bai et al. (2021), and Chernozhukov et al. (2019) is also more appropriate.

3 Identification and estimation

3.3.4 Large state space and finite dependence
As we have mentioned above, in some empirical applications, the exact computation
of present values is impractical as it would require months or years of computing time
with even the most sophisticated computer equipment. We need to use approximations to these present values. We have already discussed Monte Carlo approximation
methods, which have received substantial attention in this literature. Other approach
that reduces this computational cost is exploiting the finite dependence property in
some dynamic models.
As we have mentioned above, in optimal stopping problems the difference between the conditional choice value functions of two choice alternatives are a simple
closed-form expression of CCPs and profits at two consecutive periods, as illustrated
in Eq. (18). Arcidiacono and Miller (2011) generalize this result to DP models with
finite dependence structure. For this class of models, two firms that make different
choices at period t have a positive probability of visiting the same state x after a finite number of periods. For instance, consider a multiple bandit dynamic decision
model where xt = (ai,t−1 , zt ) where zt is a vector of exogenous state variables. For
this model, the finite dependence property implies that the difference between the
conditional choice value functions of any two choice alternatives, say j and k, has
the following expression:
vi (j, xt ) − vi (k, xt ) = πi (j, xt ) − πi (k, xt )
,
+
+βi Et πi (0, j, zt+1 ) − πi (0, k, zt+1 ) + eiP (0, j, zt+1 ) − eiP (0, k, zt+1 ) .

(27)

Furthermore, by Hotz-Miller inversion property, the difference between conditional
choice value functions is also a known function of contemporaneous CCPs. For the
sake of concreteness, suppose that the unobservables are i.i.d. extreme value type 1,
and the profit function is linear in parameters. Then, Eq. (27) has the following form:
log Pi (j |xt ) − log Pi (0|xt ) = [hi (j, xt ) − hi (k, xt )] θ π,i
+βi Et ( [hi (0, j, zt+1 ) − hi (0, k, zt+1 )] θ π,i − log Pi (0, j, |zt+1 )

(28)

+ log Pi (0, k, |zt+1 ) ).
This provides an optimality condition that does not include expected present values but only CCPs and profits at periods t and t + 1. This equation includes the
conditional expectation at period t of profits and CCPs at t + 1, and therefore, it
seems that it requires numerical integration over the state space. However, it is possible to use this equation to construct moment conditions that do not require any
explicit integration over the space of state variables. The trick has a long tradition in
the estimation of continuous choice dynamic structural models using Euler equations
(e.g., Hansen and Singleton (1982)). Under rational expectations, the conditional expectation at period t of CCPs and profits at t + 1 is equal to these variables minus an
expectational error that is orthogonal to the state variables at period t. Therefore, for

265

266

CHAPTER 4 Dynamic games in empirical industrial organization

any vector of functions of xt , say g(xt ), we have the following moment conditions:
⎛
⎤⎞
⎡
log Pi (j |xt ) − log Pi (k|xt ) − [hi (j, xt ) − hi (k, xt )] θ π,i
⎜
⎥⎟
⎢
E ⎝g(xt ) ⎣ −βi [hi (0, j, zt+1 ) − hi (0, k, zt+1 )] θ π,i − log Pi (0|j, zt+1 ) ⎦⎠ = 0.
+ log Pi (0|k, zt+1 )
(29)
Constructing sample counterparts of these moment conditions does not require integration over the space of state variables but only averaging over the sample observations. The computational cost of estimating the structural parameters using GMM
based on these moment conditions does not depend on the dimension of the state
space. The finite dependence property, and this estimation approach, also applies to
dynamic games.
This estimation method has been used in IO applications of single-agent models (Bishop (2008) to locational choice; Aguirregabiria and Magesan (2013) to asset
replacement; Scott (2014) to land use; Murphy (2018) to housing supply) and of dynamic games (Ellickson et al., 2012; Igami and Yang, 2016).

3.3.5 Unobserved market heterogeneity
So far, we have maintained the assumption that the only unobservables for the researcher are the private information shocks that are i.i.d. over firms, markets, and
time. In most applications in IO, this assumption is not realistic and it can be easily
rejected by the data. Markets and firms are heterogeneous in terms of characteristics
that are payoff-relevant for firms but unobserved to the researcher. Not accounting for
this heterogeneity may generate significant biases in parameter estimates and in our
understanding of competition in the industry. For instance, in the empirical applications in Aguirregabiria and Mira (2007) and Collard-Wexler (2013), the estimation of
a model without unobserved market heterogeneity implies estimates of strategic interaction between firms (i.e., competition effects) that are close to zero or even have
the opposite sign to the one expected under competition. In both applications, including unobserved heterogeneity in the models results in estimates that show significant
and strong competition effects.
Aguirregabiria and Mira (2007), Collard-Wexler (2013), and Arcidiacono and
Miller (2011) have proposed methods for the estimation of dynamic games that allow
for persistent unobserved heterogeneity in players or markets. Here we concentrate
on the case of permanent unobserved market heterogeneity in the profit function.
Arcidiacono and Miller (2011) propose a method that combines the GMM-finite dependence method, that we have described in Section 3.3.4, with an EM algorithm that
facilitates the estimation of the distribution of unobserved heterogeneity.
Suppose that the payoff function πi depends on a time-invariant ‘random effect’
ωm that is common knowledge to the players but unobserved to the researcher. This
unobservable is i.i.d. across markets, with a distribution that has discrete and finite
support. Each value in the support of ω represents a ‘market type’, we index market
types by  ∈ {1, 2, ..., L}, and λ ≡ Pr(ωm = ω ). This unobservable does not enter into the transition probability of the observed state variables. Each market type

3 Identification and estimation

 has its own equilibrium mapping (with a different level of profits given ω ) and
its own equilibrium. Let P be a vector of strategies (CCPs) in market-type . The
introduction of unobserved market heterogeneity also implies that we can relax the
assumption of only ‘a single equilibrium in the data’ to allow for different market
types to have different equilibria.
MThe pseudo log likelihood function of this model is Q(θ , λ, P) =
m=1 log qm (θ , λ, P), where qm (θ , λ, P) is the contribution of market m to the
pseudo likelihood:
⎤
⎡
L

qm (θ , λ, P) =
λ|xm1 ⎣ i (aimt |xmt , ω, θ, P⎦ .
(30)
i,t

=1

where λ|x is the conditional probability Pr(ωm = ω |xm1 = x). The conditional
probability distribution λ|x is different from the unconditional distribution λ . In
particular, ωm is not independent of the predetermined endogenous state variables
that represent market structure. For instance, if ωm has a positive effect on profits,
we expect a positive correlation between firms’ lagged entry decisions and this unobservable. This is the so called initial conditions problem (Heckman, 1981). In short
panels (for T relatively small), not taking into account this dependence between ωm
and xm1 can generate significant biases, similar to the biases associated to ignoring
the existence of unobserved market heterogeneity. There are different ways to deal
with the initial conditions problem in dynamic models. One possible approach is to
derive the joint (ergodic) distribution of xm1 and ωm implied by the equilibrium of the
model. That is the approach proposed and applied in Aguirregabiria and Mira (2007)
and Collard-Wexler (2013). Collard-Wexler (2014) also models the initial conditions
problem for a time varying market level unobserved state.
Let p P ≡ {p P (xt ) : xt ∈ X } be the ergodic or steady-state distribution of xt induced by the equilibrium P and the transition Fx . This stationary distribution can be
simply obtained as the solution
to the following

system of linear equations: for every
value xt ∈ X , p P (xt ) = xt−1 ∈X p P (xt−1 ) [ a t P (a t |xt ) fx (xt |a t , xt−1 )]. Given
the ergodic distributions for the L market types, we can apply Bayes’ rule to obtain:
λ|xm1 =

λ p P (xm1 )
L


λ

p P (x

(31)

m1 )

 =1

Note that given the CCPs for each market type, this steady-state distribution does not
depend on the structural parameters θ.
For the estimators that we discuss here, we maximize Q(θ , λ, P) with respect
to (θ , λ) for given P. Therefore, the ergodic distributions p P are fixed during this
optimization. This implies a significant reduction in the computational cost associated
with the initial conditions problem. Nevertheless, in the literature of finite mixture
models, it is well known that optimization of the likelihood function with respect to

267

268

CHAPTER 4 Dynamic games in empirical industrial organization

the mixture probabilities λ is a complicated task because the problem is plagued with
many local maxima and minima. To deal with this problem, Arcidiacono and Miller
(2011) propose using the EM algorithm.
The estimators of finite mixture models in Aguirregabiria and Mira (2007),
Collard-Wexler (2013), and Arcidiacono and Miller (2011) consider that the researcher cannot obtain consistent nonparametric estimates of market-type CCPs {P0 }.
Kasahara and Shimotsu (2009) have derived sufficient conditions for the nonparametric identification of market-type CCPs {P0 } and the probability distribution of market
types, {λ0 }. Given the nonparametric identification of market-type CCPs, it is possible to estimate structural parameters using a two-step approach similar to the one
described above.
Berry and Compiani (2020) (see also Berry and Compiani (2021)) advance a generalized instrumental variable approach, following the more abstract approach to this
problem outlined in Chesher and Rosen (2017), to estimating dynamic models with
serially correlated unobservables allowed to change over time. Their instrumental
variables approach relies on the existence of observable covariates that are uncorrelated with the unobservable component of the payoff function, do not directly enter
the present period policy function, but are correlated with the present state variables.
Shocks to investment costs in prior periods, changes in regulatory policies that limited
or encouraged entry, and demographic changes across time are examples of external
economic forces can be correlated with the present state of the market but are uncorrelated with unobservables. A similar approach is taken by Kalouptsidi et al. (2021).
The focus in their work is on market-level unobserved heterogeneity, rather than the
agent-level unobserved heterogeneity emphasized by Berry and Compiani (2020).
The papers impose different assumptions on the nature of the unobservables, and
thus are not nested within each other, but both illustrate two ways that the literature
has moved forward regarding unobserved state variables.

3.4 The promise of machine learning
Machine learning, a term that covers a broad set of tools for statistical learning,
has recently generated excitement for its potential to transform empirical and computational analysis. Generally speaking, machine learning methods are algorithmic
approaches to solving problems where a minimum of guidance is provided by the
researcher in guiding that algorithm to its goal.
There are numerous applications of machine learning in economics. In the context of functional approximation such as the value function approximations that we
discussed in Section 2.4.3, neural networks, and their extension, deep learning, have
shown remarkable promise in their ability to model nonlinear relationships between
inputs and outputs. In econometrics, recent work has started to provide rigorous theoretical foundations to machine learning techniques and leverage them for model selection. See, for example, Chernozhukov et al. (2018) and Nekipelov et al. (2021). As
applied to DP, reinforcement learning has investigated ways of solving for value functions and/or optimal controls using computational techniques based on trial-and-error

3 Identification and estimation

while remaining agnostic about some aspects of the underlying theoretical machinery,
such as the transitions across states, or exactly how rivals arrive at their strategies. For
dynamic games, a fundamental question is: can techniques from the machine learning
literature help researchers overcome the various computational challenges associated
with solving DP problems with high-dimensional state spaces and complex action
sets consisting of continuous and discrete decisions while accommodating a large
number of potential agents? Our answer to this question, as of the time that this review is written, is, without a doubt, perhaps.
Machine learning has been used for solving dynamic games for decades. An early
application is the reinforcement learning algorithm (also known as Q-learning) used
in Pakes and McGuire (2001), and the real-time algorithm in Ifrach and Weintraub
(2017) based on the work by Bertsekas and Tsitsiklis (1996). Over time, under some
regularity conditions, the learner traces out the ergodic set of states that are visited
in equilibrium and act optimally at each state. One advantage of this approach is that
states that are never visited in equilibrium do not need to be included in the solution,
which may lead to a speed up in the computation of an equilibrium.19 Research in this
area has continued at a rapid pace since the turn of the century; recent advancements
include deep reinforcement learning (Arulkumaran et al., 2017). Deep reinforcement
learning also encompasses many different techniques, but the basic aim of all of them
is to utilize deep neural networks to approximate the optimal policy function. The
deep neural network may be augmented with convolutional neural networks that effectively reduce the dimensionality of the inputs. The canonical applications for these
techniques are in teaching computers to play video games.20 Highlighting the minimal amount of modeling in some reinforcement learning approaches, the basic inputs
are simply pictures of the video game screen, while outputs are controller actions
(such as up, down, left, or right). The underlying neural network learns the optimal
policy function through a trial and error association between states and long-run outcomes.
Early applications of these algorithms focused on simple single-player video
games (i.e., the opponent is a computer that does not learn), such as Space Invaders
or Snake. It is perhaps not surprising that it is possible to learn optimal policies in
such environments, where the best response of the game is relatively straightforward. What is surprising is how well these systems have been adapted to play much
more open-ended multi-agent games where the state space is extremely complex,
there are a huge number of actions available to players, and your opponent optimizes
back against your strategy. A very high-profile example of this was the headlinegenerating defeat of the (human) world-champion team playing the game Defense of
19 Note that this specific advantage of reinforcement learning algorithms does not apply to dynamic games

that include private information shocks with unbounded support for every action, as suggested by Doraszelski and Satterthwaite (2010) to guarantee existence of an equilibrium in pure strategies. With this type of
shocks, the probability of every action in every state is nonzero, and the ergodic set is the entire state space.
The algorithm may still provide other advantages in the computation of an equilibrium. Collard-Wexler
(2013) is an example of reinforcement learning used along with full support shocks.
20 Shao et al. (2019) surveys the literature.

269

270

CHAPTER 4 Dynamic games in empirical industrial organization

the Ancients 2 (DOTA2) in 2018 and 2019 by OpenAI Five OpenAI et al. (2019).21
This example is notable for several reasons. First, the human team was composed of
the very best players in the world who have very high-powered incentives to become
experts in playing the game—the prize money at the world championships in 2021
is $40 million. Second, the pace of advancement on the OpenAI side is astounding.
In 2016, the AI could only play limited versions of the game with single opponents.
Three years later it was roundly and repeatedly defeating the best human players.
The experience of OpenAI Five suggests some important lessons for the promise
of machine learning in DP problems. OpenAI makes admirable progress on all of
the criteria: it was able to develop successful policies in an environment with highdimensional state spaces, complex action sets, and multiple strategic actors. However,
there are some caveats.
First, much of the computer science work on machine learning often focuses on
providing improved, rather than exact, solution to decision problems. A machine
learning approach which provides a better quality translation of a text, or a more
competitive player in DOTA2, is clearly useful. However, in economics, we usually assess algorithm for solving games by how closely they compute equilibrium
strategies. Thus, a better machine learning algorithm for solving a game might nevertheless be quite far away from the equilibrium policy. Indeed, in 2019, the OpenAI
algorithm was still learning how to play DOTA2. While there is some work providing worse case bounds for these dynamic decision problems, the evidence here is
much less clear. Indeed, reading some of the computer science literature reminds us
of what econometrics would look like if estimators were judged exclusively based on
Monte-Carlo’s without reference to any theorem proving consistency or asymptotic
distributions. It can be difficult to assess how well these methods can be extended to
somewhat different dynamic games.
Second, if the goal of machine learning is providing tools that ease computational
burden in both time and effort, OpenAI is not a particularly compelling test case.
To achieve its current level of sophistication, it has played millions of games, both
against itself and against human opponents. This has taken years of computer time,
and significant amounts of hardware.
Third, there is clearly some adaption that the research team had to make to bring
ideas of deep learning to playing DOTA2; this is not an off-the-shelf endeavor. For instance, the AI was initially restricted to play reduced-complexity versions of DOTA2.
This suggests that even the cutting-edge machine learning techniques still require
some hand-tuning in defining and restricting the scope of the underlying context that
it is trying to learn. As a practical example in economics, when using reinforcement
learning approaches for dynamic games it is critical to make sure that the algorithm
explores a large enough part of the state space to ensure it is not confined to a locally
optimally policy.

21 See, for example, a popular press discussion in https://www.vox.com/2019/4/13/18309418/open-aidota-triumph-og, accessed August 26, 2021.

3 Identification and estimation

Some of these outstanding issues are driven by a fundamental result proved by
Chow and Tsitsiklis (1989) thirty years ago: in general, no algorithm can solve the
DP problem, for some level of error, without suffering from the curse of dimensionality when the state space is continuous. Iskhakov et al. (2020) have an engaging
discussion about the prospects for machine learning techniques that help reduce the
state space:
Even though machine learning can potentially address the curse of dimensionality
by employing model selection when estimating high dimensional choice probabilities, data still limits what we can learn about underlying model structure. But even
in the ideal case where machine learning can recover a precise, sparse representation of P (d|x) that allow us to estimate the structural parameters, we cannot rely
on this approach for counterfactual simulations. If choice probabilities P (d|x)
fundamentally change shape in the counterfactual and require a different set of
variables and basis functions, it is still necessary to solve high dimensional DP
problems whose sparsity structure is unknown.

This emphasizes a fundamental difference between some machine learning contexts
like computer vision, where dimensionality reduction and neural networks have combined to produce high-performance classification systems for identifying objects in
photographs, and dynamic games: in the latter, the value functions and optimal policies are not objects to be identified from a static snapshot, but rather are endogenous,
fluid objects that change in response to decisions made in other parts of the state
space. Any solution method using machine learning techniques for complexity reduction in the state space has to be adaptive, as states that one might group together
as homogeneous at the beginning of the solution process may turn out to be ultimately
very different from each other at the final solution. In this sense, the consistent classification of states for the purposes of reducing state space complexity generates yet
another fixed point that needs to be solved in parallel to those governing the value
functions.
Circling back to the original question of whether machine learning techniques
will be beneficial for dynamic games estimation and counterfactual computation, our
assessment is a cautious one. Fundamental challenges remain: many machine learning techniques that are marketed as solving the world’s problems are nothing more
than effective ways to approximate functions. The econometrics literature has already
delivered a library of nonparametric techniques that are capable of consistently recovering nearly-arbitrary functions, some of which are much easier to use than the very
nonlinear neural networks currently in vogue. No algorithm can ever escape the curse
of dimensionality when the state space is continuous, as many are. Finally, dimensionality reduction techniques, like those from computer vision, are promising but
still need to confront the problem that the underlying object they are approximating
changes while computing the solution. On the other hand, the practical performance
of specific implementations like OpenAI Five raise the possibility that future advancements will bring us much closer to the promise of an easy-to-use, accurate, and
quick off-the-shelf methodology for estimating and computing dynamic games.

271

272

CHAPTER 4 Dynamic games in empirical industrial organization

4 Empirical applications
There are, at present, a large number of applications of dynamics games in IO. This
literature is eclectic, motivated by specific applications. To organize our venture in
this field, we start by tracing a chronology of the major methodological innovations
in the field and how they were applied, then move out to different topics of substantive interest to IO economists, such as innovation, antitrust, asymmetric information,
regulation, uncertainty, natural resources, and dynamic matching.

4.1 Earlier empirical work on dynamic games
The history of empirical applications of dynamic games in industrial organization
can be split into two distinct periods. Early on, a handful of empirical applications
directly applied the Pakes and McGuire (1994) algorithm, mainly Yale graduate students such as Benkard (2004) and Gowrisankaran and Town (1997). These papers
addressed substantive research questions and highlighted the need for an econometric approach to estimation that sidestepped the computational burden of repeatedly
solving the theoretical model. The main innovation was the subsequent development
of the estimation methods described above based on CCPs that has directly led to
the current era of empirical applications. We organize our discussion of this literature roughly chronologically, beginning with a discussion of Benkard (2004) and
Gowrisankaran and Town (1997) before turning to four papers that bridged the two
eras: Jofre-Bonet and Pesendorfer (2003), Ryan (2012), Collard-Wexler (2013), and
Dunne et al. (2013). Much like the econometric methodology upon which they are
based, these papers co-evolved contemporaneously; they are important not only as
they are among the first examples of applications using these methods, but because
they also shed light on challenges to implementation.

4.1.1 Competition in the hospital market
Gowrisankaran and Town (1997) is one of the very first applications using the MPNE
framework in an empirical context. Based on the theoretical quality ladder model of
Pakes and McGuire (1994), the authors examine competition in the US hospital market. This market is an economically important part of the US economy, both in terms
of direct expenditures (approximately 5 percent of GDP) and its role in ensuring the
health of the population. It is also an industry with heavy government involvement
(via service requirements and regulated payments for certain types of consumers),
rapid technological advancement, and, in later years, consolidation. The authors build
a dynamic model of competition in this industry that aims to capture many of these
salient institutional details.
On the supply side, there are two types of firms: for-profit and non-profit hospitals, each with different objective functions, taxation treatment, and investment costs.
Non-profit hospitals differ from for-profit hospitals by maximizing a weighted sum of
profits and consumer surplus in their objective function, whereas for-profits care only
about the former. For-profit hospitals also have to pay additional taxes that non-profits

4 Empirical applications

often do not. Non-profits cannot engage in the same range of financial transactions as
for-profits, which may influence their investment costs.
Hospitals are differentiated by location and quality and may improve their quality
through investment as the Pakes and McGuire (1994) quality ladder model. Hospitals may enter, exit, and set prices for the private market, but are required to accept
Medicare patients at a government-imposed regulated price. On the demand side,
consumers differ by income and insurance coverage and decide which hospital to
attend; the authors use a tailored logit model to estimate demand.
They use this model to evaluate three different counterfactuals: a change to the
Medicare reimbursement rate mechanism, the introduction of universal health-care
coverage, and an adjustment to the taxes of non-profit hospitals. Understanding how
these policies affect the provision of hospitals in the United States is important. Given
how slow the entry and exit process for hospitals is likely to be, it is hard to find good
sources of identification for a strictly empirical approach to these questions.
Their empirical strategy is a modified version of the nested fixed point from Rust
(1987) or Pakes (1986) using a simulated method of moments (McFadden, 1989;
Pakes and Pollard, 1989) approach. In an inner loop, for each guess of the parameter vector they solve the MPNE of their model. They then simulate data from the
ergodic distribution of this equilibrium, and construct simulated moments of demand
and supply that aggregate data over hospitals and time. In an outer loop, a nonlinear
optimizer searches over the parameter space to minimize a distance metric between
simulated moments and their empirical counterparts.
There are several drawbacks to using this aggregated moment approach. The
first is that it is statistically inefficient in two ways: aggregating information loses
granularity in the underlying data-generating process that would be captured by a
full-information estimation method, and the choice of which moments of the data to
use ignores some of the empirical restrictions of the model. Second, this approach
presumes that all markets are mature enough that they have reached their ergodic
distributions. If an industry is still growing to maturity, this approach cannot be used
since one effectively is matching the long-run distribution of states to a non-ergodic
transition path. Third, the nested fixed-point approach is also computationally burdensome, as it requires repeatedly solving the entire dynamic game, for each market,
for each guess of the parameter vector. Finally, the dynamic game has multiple equilibria but their implementation of the NFXP algorithm does not account for this issue.
The authors make several simplifying assumptions to reduce the dimension of the
state space. First, they aggregate a rich set of observable hospital characteristics into
a single quality index, and assume that this index has a discrete and finite support
with a relatively small number of points. Second, the stochastic process for quality is
restricted to move up or down by at most one unit. Third, the authors only consider
a small number of firms as potentially being active. All these restrictions are made
because of computational convenience, but they may have important impact on the
estimation of the model and its predictions on the evolution of market structure. For
instance, it is well known that the number of potential entrants in a market can have

273

274

CHAPTER 4 Dynamic games in empirical industrial organization

important effects on incumbent firms’ incentives to guard against entry, which in turn
changes the evolution of market structure.
Despite all these limitations, this paper was highly innovative and was among
the very first papers to take the MPNE framework to an empirical setting. As such,
the authors had to confront an entire host of issues that had never been dealt with
previously in the literature. It is also important to note that many of these problems
are still present (and potentially acute) at the frontier today.

4.1.2 Dynamic output competition with learning by doing
Benkard (2004) considers the production decisions of wide-body aircraft manufacturers: Boeing, Airbus, and for the time period considered, Lockheed as well. The
dynamics here are driven by learning by doing, where aircraft production costs fall
with accumulated experience. This mechanism produces intertemporal strategic considerations when pricing an aircraft, as lower prices not only increase sales, but also
speed the learning process, while also potentially robbing rival firms of additional
experience.
A manufacturer i produces different varieties of aircraft which are indexed by
. The production technology includes an equation that represents the causal relationship between a manufacturer’s labor requirement for producing one aircraft
of type , Lit , and the manufacturer’s experience as measured by the number of
planes of that type produced in the past, Eit . This log-linear equation: log Lit =
θ log Eit + γ log Sit + uit , where Sit represents observable characteristics of the
aircraft type such as size and speed, and uit is an unobservable productivity shock.22
Experience evolves based on cumulative production, discounted by a forgetting rate,
Ei,t+1 = δEit + qit , where qit represents the number of planes produced at time
t. The demand side of the model is a nested logit demand system, with unobserved
product level quality ξit that evolves exogenously over time. The model includes
potential entrants who may choose to enter the market after paying a sunk entry cost.
Benkard calibrates the entry cost based on accounting data on development costs of
aircraft released by Lockheed.
In this dynamic oligopoly model, every period t firms decide how much to produce of each product. The vector of state variables xt includes the firm-product
specific variables xit = (Eit , ξit ) for every firm and product, and a time-varying
market size Mt . This model is solved using a version of the Pakes and McGuire
(1994) algorithm that exploits symmetry in firms’ strategy functions. As in the work
of Gowrisankaran and Town (1997), the state space needs to be reduced for computation, and this reduction is achieved in the base case by ruling out multi-product firms,
that is, assuming that Boeing’s 777 and 747 are produced by two separate firms.
The model is used for counterfactuals, but also to see if a dynamic oligopoly
model can rationalize, quantitatively, some intriguing characteristics of the industry.
Aircraft are frequently sold below marginal cost, especially at the early years of a

22 This labor requirement equation is estimated in the companion paper Benkard (2000).

4 Empirical applications

product line. This below-cost pricing may be interpreted as predation (which often
triggers anti-dumping sanctions), but it could be partly explained by learning by doing
motives which also affect the pricing strategy of a monopolist not concerned about
potential entrants. In an oligopoly industry, learning by doing can also exacerbate
predatory motives, since lower prices at the early years of a product can induce the
exit of rivals. Benkard’s estimates show that prices can be up to 50 percent below cost
when an aircraft is first introduced, and these prices are even lower when facing competitors that have more experience producing aircraft. This matches observed data on
price/cost margins earned by Lockheed. The model is also used to predict concentration in the industry. Learning by doing functions as an additional entry barrier above
development costs, and this leads to more concentrated market structure.
An attentive reader will notice that the dynamic decisions made by firms are not
used for estimation. Instead, Benkard uses static techniques to estimate the firm’s
cost function and demand. So one can think of this line of research trying to uncover
what are the dynamic implications of a model, and whether these are quantitatively
accurate. While this type of quantitative exercise is common in macroeconomics, in
IO, this is the most successful exercise of this type.

4.1.3 Dynamics in auctions
An early paper that presages the following explosion in CCP-based applications is
Jofre-Bonet and Pesendorfer (2003). This transitional paper sits at the crossroads of
the methodological innovations discussed above and the empirical applications that
followed. The authors leverage the insights from Elyakime et al. (1994) and Guerre
et al. (2000) (hereinafter GPV) to estimate bidder valuations in a series of repeated
first price procurement auctions for highway paving services.
There are two potential sources of dynamics: capacity constraints and learning
by doing. Firms only have a limited amount of paving capacity in the short run,
so winning a large paving contract today may preclude the firm from being able to
compete for future contracts. On the other hand, winning a contract today gives the
firm additional experience and expertise which may lower costs for future projects.
The optimal bidding strategy for a firm has to account for these two economic forces
in addition to the standard trade-off between the probability of winning and rents.
The dynamic incentives change the standard first-order condition considered in GPV
through the inclusion of an extra term that accounts for changes in future costs that
may accrue as the result of winning the auction today.
The estimation approach proceeds in two steps, in the same spirit as the CCP
methods described in Section 3.3.2. The first step consists of nonparametric estimation of the reduced form bidding functions relating a firm’s bid with the observable
state variables. In the second step, these reduced forms are plugged into the first order condition characterizing a firm’s best response, and then structural parameters are
estimated.
In this model, time is discrete with an infinite horizon. Firms share a common
discount factor β. There are two types of firms: regular firms and fringe firms. Fringe
firms are differentiated from regular firms in that they only exist for one period, while

275

276

CHAPTER 4 Dynamic games in empirical industrial organization

regular firms are infinitely-lived. The authors classify firms into these two categories
on the basis of how frequently they bid in the data; the largest ten firms are considered to be regular firms and everyone else is a fringe firm. In every period, a sequence
of events occur. First, the government presents a single paving contract with idiosyncratic characteristics that the firms may bid on. Second, bidders obtain a draw of
private costs, cit , for performing the job that comes from a commonly-known distribution that depends on the vector of state variables. Crucially, costs are assumed to
be conditionally independent of the contract characteristics. Third, the auction runs
and the seller awards the contract to the lowest bidder, subject to a reservation price.
The vector of common knowledge observable state variables xt is (s0t , sit : i ∈ I)
where sit is a list of projects, each with an associated size and time left to completion, that firm i has won in the past, and s0t contains the characteristics of the current
contract being auctioned off. This state vector is updated in two ways: the backlog
increases (endogenously) when a firm wins a contract, declines (exogenously) each
period as the firm works on finishing off existing paving contracts. The authors assume that contracts are completed at a fixed rate each period.
Let bit be firm i’s bid at auction t, and let αi (xt , cit ) be firm i’s bidding strategy
function. The firm’s expected profit at period t is equal to its rent if winning, bit − cit ,
times the probability of winning, Wiα (bit , xt ) ≡ E(1{bit < αj (xt , cj t ) for any j =
i} | bit , xt ). Given other firms’ bidding strategies, the value function for bidder i is
the solution to the following Bellman’s equation:
⎡
⎤
(bit − cit ) Wiα (bit , xt )
⎢
⎥


⎥.
Viα (xt , cit ) = max ⎢
+β Wiα (bit , xt ) E Viα (xt+1 , ci,t+1 ) | xt , cit , i wins
⎣
⎦
bit




α
α
+β 1 − Wi (bit , xt ) E Vi (xt+1 , ci,t+1 ) | xt , cit , i loses
(32)
In the right hand side, the first term is the familiar static payoff from a first-price auction. The second and third terms are the continuation values if winning and if losing
the auction, respectively. Each firm forms expectations about the value of the world in
the next period for each of the possible winners of the contract today, including itself.
Note that, once we account for the probability of winning, the continuation values
do not depend on the current bid bit . This property plays an important role in the
structure of the first order conditions of the model. This formulation is very general,
as firms in principle are carrying around a huge state space. As we explain below, the
authors introduce important simplifying assumptions in this general framework.
A profit-maximizing firm sets marginal cost equal to marginal revenue. In GPV,
one can solve for cost as a function of the observed bid and a markup term. Here, the
cost equation has an extra term that comes from the continuation values. Let hαi (xt )
be the hazard function of firm i’s bids: hαi (xt ) = giα (xt )[1 − Gαi (xt )]−1 , where g and
G are the density and cumulative functions in the distribution of firm i’s bids. The
authors show the following expression for the first order condition of optimality in

4 Empirical applications

firm i’s best response:
1
h
j=
 i j (bit |xt )
 hj (bit |xt ) +
,

+β
EVitα (i wins) − EVitα (i loses)
=i h (bit |xt )

cit = bit − 

(33)

j =i

where EVitα (i wins) is firm i’s continuation value if it wins the current auction (that
is, E(Viα (xt+1 , ci,t+1 ) | xt , cit , i loses)), and similarly, EVitα (i loses) is the continuation value if it loses. The third term in the right hand side represents the dynamic
marginal value of winning the auction.
Eq. (33) is the key condition for the estimation of the structural parameters. The
econometric object of interest is the cost in the left hand side of this equation. In
the right hand side, the hazard functions can be estimated using data on bids and
state variables. As in many other applications, the authors consider that the discount
factor is known. The authors show that the continuation values EVitα (i wins) and
EVitα (i loses) can be represented as a recursive equation involving the bid distribution function.
Even when the specification of the cost function (that relates a firm’s cost cit
with the firm’s backlog in vector xt ) is parametric, the first step in the sequential
method to estimate the parameters in this cost function is nonparametric. That is, for
consistency of the estimator, the estimation of the hazard functions hi (bit |xt ) should
be nonparametric because these functions are endogenous equilibrium objects such
that a parametric specification of these functions is, in general, incompatible with the
equilibrium outcome. However, in this model, the dimension of the space of xt is very
large, such that nonparametric estimates of hazard rates hi (bit |xt ) can be extremely
imprecise given the curse of dimensionality in nonparametric estimation. Therefore,
the authors end up estimating hazard functions under strong exclusion, aggregation,
and parametric restrictions on how the vector xt enters in these functions. This is a
common issue in this literature when using two-step estimation methods.
Jofre-Bonet and Pesendorfer (2003) estimate the bid distribution function as a
parameterized Weibull distribution for regular bidders and as a beta distribution function for fringe bidders. These choices have substantive restrictions, as the likelihood
is only well-specified for a range of parameter values. To capture the dependence of
these distributions on state variables, they impose the symmetry condition that all
bidders behave identically conditional on equal states. This is a strong assumption
that would be violated if the identity of the firm matters beyond what is captured in
the state variables, e.g., if there is persistent difference in firm types. These restrictions are imposed through a parameterization of several of the arguments of the bid
distribution.
This approach illustrates a common trade-off that practitioners face when using
two-step estimation methods in dynamic structural models. Even for unidimensional
distributions, the nature of dynamic games may require knowledge of functions evaluated at states that are visited rarely. Having enough observations at every point in

277

278

CHAPTER 4 Dynamic games in empirical industrial organization

the state space is a very high burden outside of the simplest dynamic models, and as a
result practitioners have resorted to using parametric approaches. However, this also
comes with a cost.
In addition, while the GPV techniques used in the paper make estimation possible,
there remains an issue of how to compute a solution to the dynamic bidding model in
the paper, which is, currently, a topic of ongoing research. Needless to say, this limits
the scope of counterfactuals from their model.
Jeziorski and Krasnokutskaya (2016) extend Jofre-Bonet and Pesendorfer model
to allow for subcontracting in response to capacity constraints. Groeger (2014) studies dynamics generated by sunk entry costs that involve multiple sequential auctions.
Hopenhayn and Saeedi (2016) develop a dynamic model of bidding in second price
online auctions agents can revise their bids, and bidding opportunities and values follow a joint Markov process. They estimate the model using data from eBay auctions.
Dee (2020) proposes and estimates a dynamic model for pay-per-bid auctions—
a type of auction where bidders incur a cost each time they place a bid.

4.1.4 Environmental regulations in concentrated industries
Ryan (2012) studies the cost of environmental regulation in concentrated industries,
where the effects of long-run changes to market structure can dwarf the direct costs
associated with regulatory compliance. Specifically, he measures the welfare costs associated with the 1990 amendments to the Clean Air Act in the US Portland cement
industry, that is the upstream industry from the concrete market.23 The amendments
introduced a variety of new regulations that applied to the cement industry, including
new environmental assessment standards for greenfield cement plants and additional
classes of regulated pollutants. In principle, these regulations could have changed the
cost structure of the industry in several ways and, by extension, led to a different evolution of market structure. Ryan constructs a model of the cement industry, which has
several features which make it well-suited for analysis in the BBL framework, estimates a change in the cost structure of firms before and after regulation using a panel
on every cement plant in the United States from 1980 to 1998, and compares realized
outcomes against a simulated counterfactual where the regulation did not exist. His
primary finding is that entry costs increased, leading to fewer firms in equilibrium and
a loss of between $810 million and $3.2 billion in surplus. A static analysis would
miss the change in entry costs and find the wrong sign of costs to incumbent firms,
who actually benefit from reduced competition under the amendments.
The cement industry has several institutional features that make it an attractive
setting for two-step estimation. The first is that cement is a largely homogeneous
commodity due to its use as a construction material. Cement is also hygroscopic (i.e.,
it absorbs water from the air), making storage expensive, and has a relatively low
value to volume ratio. The combination of these two factors leads it to be shipped
overland only relatively short distances, which means that most cement markets are

23 See Section 4.1.5 below for our description of Collard-Wexler (2013) study of the US concrete industry.

4 Empirical applications

quasi-independent geographically-differentiated regional markets. This is useful for
modeling and estimation purposes, as it both reduces the number of firms that need
to be considered in each market and generates a cross-section of observations. The
spot market for cement is also highly seasonal, due to construction demand peaking
in the warm weather months. Given that storage is expensive, most firms do not hold
significant amounts of stock from year to year, which also simplifies the state space.
The technology of cement production also lends itself to parsimonious modeling. Technological progress in the industry is very slow; the basic process of making
cement has changed very little since the late 1800s. To produce cement, firms mine
limestone (often co-located with the plant), grind it up into small pieces, and then
heat it through a large rotating kiln that is fired to very high temperatures at one end.
It is then ground up and gypsum is added to create cement. Cement plants typically
produce nonstop at a constant rate for most of the year before shutting down in the
winter to perform maintenance on the kiln. This is important to note for several reasons: first, marginal costs should be reasonably flat until that maintenance period is
reached; at that point, the opportunity cost of production increases as the firm eats
into the maintenance period. Second, firms are primarily differentiated through their
location and their productive capacity. Third, emissions are a key component of cement production, both through the pyrochemical process of converting limestone into
clinker and through the burning of fossil fuels to produce that heat. Fourth, fixed costs
are a first-order feature of the industry. The typical cement market has only a handful of firms active, and the average size of cement plants is large and has increased
steadily over the twentieth century. A typical cost for a greenfield plant is half a billion dollars, and plant lifetimes approach one hundred years. Finally, most plants in
most years are capacity constrained and produce right up to their boilerplate ratings.
This suggests that long-run changes to market structure may be the dominant margin
for assessing the costs of regulation, as firms may have relatively little margin for
adjustment in the output market.
The theoretical model has three primary components: a state space, induced transitions over those states in response to firm actions, and per-period payoff functions
which depend on firm actions, market demand, and the state vector. As with the prior
work on dynamic games, there is a relatively simple state space consisting of the productive capacities of each firm in a regional market. Potential entrants are encoded
with a zero capacity. In contrast to prior literature, the state space is continuous, as
capacity is not naturally discrete. The industry has been in a long period of sustained
consolidation, as a smaller number of larger firms become the dominant firm type.
This is also useful for bounding the number of potential firms in the industry, as it is
very unlikely that any market would see more than one entrant, especially after the
passage of the 1990 amendments.
In each period, firms compete in Cournot competition subject to their capacity
constraints. Capacity constraints are modeled through a “hockey-stick” specification
for cost that generates constant marginal costs before increasing as a function of the
firm’s capacity:
c(qit ; δ) = δ0 + δ1 qit + δ2 1(qit > ν sit ) (qit − ν sit )2 ,

(34)

279

280

CHAPTER 4 Dynamic games in empirical industrial organization

where qit is firm i’s output at year t, sit is its production capacity, δs are cost parameters, and ν is a parameter that determines the output/capacity ratio at which the
additional cost kicks in. The lack of meaningful dynamics in production, due to high
storage costs and seasonal demand, is particularly helpful in this setting to pin down
the range of admissible dynamic parameters.
The vector of common knowledge state variables is xt = (zt , sit : i ∈ I), where
zt is a demand shock. Every period, firms make dynamic decisions: incumbent firms
decide investment (or divestment) in capacity and whether to stay or exit from the
market, and potential entrants decide whether to enter in the market. Let ait represent firm i’s dynamic decision. Firms’ capacities change endogenously as a result
of decision ait . All transitions are assumed to take one period to happen. There
is an adjustment cost function that captures investment, divestment, entry, and exit
costs associated with decision ait . Since all these actions are discrete, i.i.d. private
information shocks in adjustment costs are introduced to ensure the existence of a
pure strategy equilibrium (Doraszelski and Satterthwaite, 2010). All firms, including
potential entrants, receive a draw from a distribution of fixed adjustment costs and
decide whether to engage in costly capacity investment/divestment. Additionally, incumbent firms receive a draw of exit costs from a common distribution and decide
whether to exit or continue. Potential entrants receive a draw of entry costs from a
common distribution and decide whether to enter the industry (and at what capacity
level), or remain outside the industry. As discussed in Section 3.2 above, fixed costs
of production are not jointly separately identified from entry and exit costs, and are
assumed to be zero.
In this dynamic game, as described in Section 2.2.3, we can use a CCP function
Pi ≡ {Pi (ait |xt : (ait , xt ) ∈ A × X )} to represent a firm’s strategy. The key equilibrium requirement is that a firm’s strategy Pi should be optimal given the strategies of
its competitors. Optimality requires that the expected present value from following
that strategy is at least as good as from using any other feasible strategy, Pi :
0
0
/∞
/∞


t
t
EPi ,P−i
β π(ait , xt , θ π ) > EPi ,P−i
β π(ait , xt , θ π )
(35)
t=0

t=0

where strategy-dependent expectations are taken over future actions, states, and
shocks, and π(ait , xt , θ π ) is the profit function, where θ π is the vector of structural
parameters in profits.
The empirical approach closely follows the BBL method described in Section 3.3.3 above: a first step where the econometrician estimates reduced form
equilibrium policy functions (i.e., CCPs) directly from the data, followed by a second step where those reduced forms are projected onto an underlying theoretical
model. The inequality in Eq. (35) is the heart of that second step projection. Ryan
assumes that all of the markets play the same equilibrium, allowing him to pool markets together when estimating policy functions.24 BBL requires high-quality, flexible
24 See Otsu et al. (2016) for a statistical test and evaluation of the pooling assumption.

4 Empirical applications

reduced-form estimates of the policy functions for each element of the state space: the
probability of entry and exit, the probability of investment/divestment, and the level
of investment/divestment if it occurs. Without solving for an equilibrium, these reduced form functions are nonparametric objects for the econometrician. However,
there is a huge curse of dimensionality in the nonparametric estimation of these
reduced form functions. Using panel data from a few hundred markets over two
decades, achieving the maximum possible precision in the estimation of structural
parameters in the second step requires imposing substantial smoothing / aggregation
restrictions in the nonparametric estimation in the first step (Ackerberg et al., 2014;
Chernozhukov et al., 2016). To deal with this problem, Ryan estimates parsimonious
parametric policy functions in the first step of the method. He uses probits to estimate
the probability of entry and exit, where the arguments of the probit are a constant, the
sum of capacity of competitors, a dummy variable for post-1990, and the firm’s own
capacity for the exit policy function. The investment/divestment probabilities and levels are estimated using an adaptation of the (S, s) rule from Attanasio (2000), where
two latent bands around the current state define when firms adjust and to what level.
The critical aspect of this specification is that it allows for lumpy adjustment, where
firms do nothing for long periods of time and then abruptly make a large change to
their capacity. The arguments of these band functions are b-spline basis functions of
the firm’s own capacity and the sum of competitors capacity.
After projecting the reduced forms down onto the underlying dynamic structural
model, Ryan finds that the distribution of fixed entry costs both increased in mean
and decreased in variance after the 1990 policy change. Both factors lead to potential
entrants facing much higher draws of entry costs. In contrast, the distributions of
exit costs, investment costs, and divestment costs did not have statistically significant
differences before and after 1990.
Ryan performs a counterfactual experiment where the cost structure of firms is
held constant at pre-1990 levels, and compares that outcome to that with the actual
post-1990 parameters. For computational reasons, Ryan restricts the experiment to
two different initial conditions in a four-firm market, which was chosen to be close to
the representative cement market in the US. Starting without any firms, the regulation
severely restricts entry, lowering profits and consumer surplus. The distribution of
active firms is compressed downward (by about one firm on average), although this is
partially offset by firms choosing larger capacities when they do enter. Prices go up
very modestly, but it is really the lack of entry (and associated capacity) that drives
the total surplus declines, leading both firms and consumers to be worse off.
In a second experiment, Ryan starts the market with two mature firms, one large
and one small, that are endowed with a combined capacity similar to the average US
market. In this setting, the incumbent firms actually do better under the regulations,
as higher costs effectively prevent entry while not harming existing firms directly.
While pre-1990 entry costs has two firms active only 4 percent of the time, after the
amendments that proportion increases to 11 percent.
These two counterfactuals are intended to put very rough bounds on the costs of
the amendments. While there are no markets that have zero firms, the estimated cost

281

282

CHAPTER 4 Dynamic games in empirical industrial organization

in this setting should be a conservative upper bound, conditional on the market size.
On the other hand, many markets do look more like the second setting, with mature
firms and low turnover. The weakness of both experiments is clearly that they do not
actually model the cement market in the US directly. This was completely driven by
computational restrictions, as it proved impossible to compute equilibria for markets
with five or more firms.

4.1.5 Demand shocks and market structure
An important limitation of the static models of entry of Bresnahan and Reiss (1990),
Bresnahan and Reiss (1991), and Berry (1992) is the inability to look at how uncertainty affects market structure in oligopolies. Collard-Wexler (2013) directly address
the question of how volatility of demand affects market structure in the market for
ready-mix concrete, the downstream industry of Portland cement studied by Ryan
(2012) and described in the previous section. In Section 4.7 we discuss other papers
that study how uncertainty influences the organization of production, that also relates
to long-standing debates in macroeconomics on the role of uncertainty in investment.
Collard-Wexler studies the market for ready-mix concrete, which is a combination
of water, gravel, sand, and cement, and is used for construction projects such as basements, sidewalks, and roads. This industry is even more geographically differentiated
than the market for cement studied by Ryan (2012). Because ready-mix concrete is
heavy and starts to set once cement and water have been mixed in, transportation is
quite limited, with the average load of ready mix concrete being delivered no more
than a half and hour away by truck. This means that one can think of the industry
as a collection of hundreds of geographically segmented local markets. It is this geographical segmentation combined with the production of a reasonably homogeneous
good that makes ready-mix concrete a good setting for looking at the empirical consequences of differences in competition. It has been studied first by Syverson (2004),
but also in Foster et al. (2008) and Backus (2020). In addition, because ready-mix
concrete is part of the manufacturing sector, in contrast to other locally-segmented
markets considered in Bresnahan and Reiss (1991), like dentists or tire dealers, it is
included in Census of Manufacturers with data on all plants in the industry going back
to the early 1960’s. So there is data on the choices of thousands of plants over decades
in terms of entry and exit decisions, as well as investment choices. This combination
of variation in market structure and many plant level decisions allows the paper to
rely less on parametric assumptions to estimate conditional choice probabilities.
A distinguishing feature of the ready-mix concrete industry is that demand is
very volatile, with year to year demand changes averaging 30 percent. This demand
volatility is usually caused by variation in government spending on local construction
projects. To evaluate the effect of removing this demand volatility, Collard-Wexler
estimates a structural model of entry and exit and discrete investment. In this model,
the state of the market, represented by vector xt , includes the size distribution of
firms, (sit : i ∈ I) (where sit = 0 means that the firm is not active in the market), and
an exogenous state variable zt that measures the state of the demand for construction
in a local market. Every period (year), firms choose to be active or not in the market,

4 Empirical applications

as well as three discrete levels of plant size. That is, a firm’s decision at period t is its
size at period t + 1, i.e., ait = si,t+1 . There is an assumption of one year time-to-build.
The profit function, πi (ait , xt , θ π ) is equal to ri (xt , θ r ) − τ (ait , sit , θ τ ), where
ri (.) is a variable payoff function (revenue minus variable cost), and τ (.) is an adjustment cost function that captures the costs of market entry and exit, and the cost of
growing and shrinking firm size. The dataset in this paper does not include information on firms’ output. Therefore, in contrast to the modeling and estimation approach
in Ryan (2012), the payoff function ri (.) is not based on an explicit specification
of demand, variable costs, and the form of (static) competition. Instead, following a
common approach in static models of market entry based on Bresnahan and Reiss
(1991), the payoff function ri (.) is a semi-reduced-form linear-in-parameters function of the firm’s own size (sit ), competitors’ sizes (s−it ), and the state of demand
(zt ). Finally, there are private information shocks, ε( ait ), to the value of taking an
action, which are assumed to be i.i.d. extreme value type 1.
In this paper, all the parameters in the profit function πi , both θ τ and θ r , are estimated from the equilibrium conditions in the dynamic game, based on firms’ entry,
exit, and investment decisions. This approach is not feasible without a large amount
of data on entry and exit decisions of firms in markets with differing demand and
market structure. This explains why this modeling approach is relatively unusual in
the broader literature. Moreover, the adjustment cost function τ (.) has many parameters to estimate, as it measures the cost of moving between any two discrete size
categories.
More than two dozen parameters are estimated using a two-step CCP method
similar to the ones in Aguirregabiria and Mira (2007) and Bajari et al. (2007). As
we have discussed in Section 3.3.5, a major issue with standard CCP methods is the
presence of persistent unobserved market heterogeneity. A common effect of ignoring
this type of unobservables when present is that the response of entry to the number
of firms is biased. Indeed it can be positive. As a diagnostic of this issue, CollardWexler finds far more negative coefficients of competition on entry when market
fixed effects are included, suggesting that there is indeed the presence of persistent
unobserved profit shocks in these markets.
The “hack” used in Collard-Wexler (2013) is to group markets into a couple of
categories based on their market fixed effect. This group becomes an additional observed state that can simply be added to the rest of the state space. This grouping
does well at replicating the results from market fixed effects regressions, without
having to estimate different market fixed effects in the structural model. Of course,
this approach is problematic since endogenous variables are being used to create this
grouping, and the estimated fixed effects suffer the incidental parameters problem
(Heckman, 1981). Thus, a more holistic approach to classification, such as the one in
Arcidiacono and Miller (2011), seems more appropriate. This approach has been used
by Igami and Yang (2016) for the estimation of a dynamic game of market entry/exit
in the Canadian fast food industry.
Collard-Wexler uses the estimated structural parameters to simulate out the effect
of shutting off demand shocks associated to local government projects. To evaluate

283

284

CHAPTER 4 Dynamic games in empirical industrial organization

this effect, he needs to solve for firms’ equilibrium strategies under the counterfactual scenario where demand shocks are eliminated. Given that the state space in this
model has around 50 million points, standard methods to compute a MPNE, such
as Pakes and McGuire (1994), are not feasible. Instead, the stochastic algorithm of
Pakes and McGuire (2001), a machine learning algorithm, is adapted to solve this
dynamic programming problem.
Because of high sunk costs of entry, there is no effect of demand volatility on
plant shutdown and new plant entry. It is worthwhile for plants to wait out periods of
low demand, even if they lose money for several years. However, demand fluctuations
do change the size distribution in the industry, as firms would build larger plants in
the absence of demand volatility. This effect opens up interesting avenues by which
macroeconomic policy that reduces swings in demand may permanently alter market
structure, which is not attainable with static models or entry. Later in this section,
we will discuss the work of Kalouptsidi (2014) which further investigates the role of
adjustment frictions, such as time to build, in a volatile demand environment.

4.1.6 Subsidizing entry
Dunne et al. (2013) examine the determinants of market structure in two service industries using the empirical framework of Pakes et al. (2007). This paper is of interest
both for substantive reason, they assess an important entry subsidy for helping locate health care providers into underserved geographic areas, and because it directly
connects back to two of the most influential early papers on structural entry models:
Bresnahan and Reiss (1990) and Bresnahan and Reiss (1991). Those papers advanced
a two-stage model of entry, and used the relationship between the total number of active firms and market size (population) to indirectly infer the nature of competition.
For example, suppose that we observe only one firm active in all markets below a
population threshold of 20,000 people, and only two firms for populations above that
threshold. If that is the case, we can infer that, in markets with more than 20,000 people, competition must be near Bertrand-levels of intensity, as no additional amount of
demand, as proxied for by population, can induce additional entry. That could only
be true if the firms are minimally differentiated and pricing near marginal cost. On
the other hand, if we observe a steady increase in the number of active firms as population increases, we can infer that competition is less intense. At the extreme, a linear
relationship between population and active firms would be consistent with collusion,
where prices do not fall with entry and firms only have enough demand to cover their
fixed costs. A data innovation that Bresnahan and Reiss use, focusing on small, isolated markets to obtain a cross section of independent markets, is carried over to this
paper.
Dunne et al. (2013) extend the static two-stage framework to a dynamic game.
This is necessary to understand the effects of different types of subsidies (e.g., subsidies on entry costs versus subsidies on fixed operating costs) and their differential
impact on potential entrants and incumbents in the short-run and long-run. In their
extended model, there are two types of firms: potential entrants and incumbents. Potential entrants take a draw from a distribution of entry costs before deciding to enter.

4 Empirical applications

Incumbent firms earn product market profits and receive a draw from the fixed costs
of operation. If the fixed costs are sufficiently high, that firm exits. The vector of
state variables xt consists of the number of incumbent firms, nt , and a vector of exogenous profit-shifters, zt , that evolves as a finite-order Markov process. Following
the tradition in Bresnahan and Reiss’s entry models, the flow profit of an incumbent
firm, π(nt , zt ), is modeled as a reduced form: it is a linear-in-parameters function of
variables nt and zt .25 In addition to this flow profit, there are fixed costs, θ F C + εtF C ,
paid by any incumbent firm, and entry costs, θ EC + εtEC , paid by potential entrants
that choose to enter in the market. The authors assume that εtF C is i.i.d. Exponential,
and εtEC is i.i.d. chi-square.
The authors study two different health care industries: dentists and chiropractors.
They argue that balance sheet data from the US Census Bureau provides good measures of flow profits πmt in the geographic markets included in their sample. Given
they observe profits, they estimate the parameters in the profit function π(nt , zt ) by
estimating the following linear regression model:
πmt = θ0 +

5


θn 1{nmt = n} + θ6 nmt + θ7 n2mt + h(zmt , θ z ) + ωm + umt

(36)

n=0

where vector zmt includes socioeconomic variables at the local market level: population, average real wage paid to employees in the industry, real per-capita income,
county-level real medical benefits, and infant mortality rate. The term h(zmt , θ z )
represents a quadratic function of these five exogenous state variables. A drawback
to this approach is that accounting profits observed in balanced sheet data can be
substantially different than economic profits, especially in this setting as the two professions considered (dentists and chiropractors) are highly mobile. Indeed, one of the
policy concerns with using entry subsidies is that the practitioners leave the needy
areas after their contracted term of service is over.
Given estimates of θ parameters and market fixed effects ωm in the regression
equation (36), the authors follow the empirical strategy in Pakes et al. (2007) to
estimate fixed cost and entry cost parameters from the dynamic game. That paper
leverages a discrete state space to generate a matrix representation of the value function. To fit their data into that approach, and to reduce the dimension of the state
space, Dunne et al. assume that the only exogenous state variable in the dynamic
game is the estimated index h(zmt , 
θ z ) that is discretized it into ten categories. To
control for persistent unobserved market-level heterogeneity, they also allow for a
lower-dimensional vector of fixed effects formed by binning the estimated fixed effects from the regression equation (36) (i.e., 
ωm s). These simplifications are sufficient
to allow the authors to form estimates of the continuation values at every point in the
state space.
25 For the purpose of this paper which is interested in the effects of entry subsidies, a drawback of a

reduced form specification of the profit function is that it is not possible to measure consumer surplus.
This limits the content in the counterfactual evaluations.

285

286

CHAPTER 4 Dynamic games in empirical industrial organization

They find that profits decline quickly for dentists, but the same regression for
chiropractors is not statistically significant. The implied net present values for these
professions are reasonable, however. They estimate monopolist dentists in high-value
markets have an average net present value of 1.3 million 1983 dollars, while chiropractors have less than half of that. For dental markets labeled as high need (and
therefore subsidized), they estimate that entry costs are 11 percent lower. This leads
to about one-half more firm per market on average, at the cost of about $170,000 per
additional entrant. A subsidy targeting the fixed costs of firms to keep them active
has a much higher cost per retained firm, about half a million dollars, due primarily
to infra-marginal firms that were not going to exit also receiving a subsidy. Targeting
the subsidy to potential entrants is therefore far more cost effective.

4.2 Innovation and market structure
Going back to Shumpeter (1942), there has been interest in studying the causal relationships between innovation and competition, and, more specifically, the hypothesis
that less competition can have a positive impact on innovation. This interest was supercharged by the work in endogenous growth theory, such as Romer (1986) and
Aghion and Howitt (1992), that placed the study of the determinants of economic
growth at the forefront of economics. A line of work in this literature has been based
on cross-industry regressions of innovation on competition, with Aghion et al. (2005)
being the most prominent example. In contrast, the recent work in industrial organization has tended to use the predictions of appropriately calibrated or estimated models
of dynamic oligopoly in the Ericson and Pakes (1995) framework. This is due in part
to the long held skepticism in IO of regressions of outcomes against market structure.26 Furthermore, for many of the industries studied in the papers that we review
in this section, such as hard drives or microprocessors, the effects of competition on
innovation are likely to dwarf, in terms of welfare evaluation, the effects of competition on prices conditional on technology, given the vast decreases in costs that these
industries have produced.

4.2.1 Microprocessor innovation: Intel vs AMD
Goettler and Gordon (2011) study competition between Intel and AMD in the PC
microprocessors industry. The authors assess the question of whether there would
have been more or less innovation without AMD. Indeed, given the rapid pace of
technological change in the semiconductor industry, the welfare effect of reduced
competition on innovation is the most important antitrust issue. Goettler and Gordon
(2011) propose and estimate a dynamic game of investment in R&D and dynamic
price competition between Intel and AMD. Importantly, their model incorporates the
durability of the product as a potentially important factor for innovation. In their
model, there are two main forces driving innovation. First, because consumers value

26 See Berry et al. (2019) for a discussion of the history of thought on this issue.

4 Empirical applications

product quality (i.e., microprocessor speed) there is competition between firms to
have a product at the technological frontier. A second factor driving innovation is
endogenous technological obsolescence. Since microprocessors have little physical
depreciation, firms have the incentive to innovate to generate a technological depreciation of the microprocessors (PCs) that consumers own and encourage consumers
to upgrade. Note that duopolists are affected by these two forces to innovate, whereas
a monopolist faces only the latter, but in a stronger way.
The demand side of the model is dynamic, with forward-looking consumers. The
state variables in consumer h’s decision problem are: the quality of the PC (microprocessor) that the consumer currently owns, as measured by the logarithm of the
∗ ; the current quality of the product that each firm
microprocessor’s speed in MHz, qht
sells, qt = (qj t : j ∈ {Intel, AMD}); and the distribution of qualities of the products
owned by all the consumers, t .27 The distribution t is part of a consumer’s state
variables because it affects her expectations about future prices. The vector of state
variables in the firms’ decision problems is (qI ntel,t , qAMD,t , t ). Given these state
variables, firm j makes two dynamic decisions: price, pj t , and investment in R&D
xj t to enhance product quality. Note that, because computers are durable goods, firms
face a Coasian pricing problem, so pricing has dynamic implications as it changes
consumer holdings in the future.28
Every quarter t, a consumer decides whether to buy a new PC (microproces∗ . The current utility of
sor) or waiting and keeping her current PC with quality qht
∗ +ε
not buying is u0,ht = γ qht
0,ht . The utility of buying brand j ∈ {Intel, AMD} is
uj,ht = γ qj t − α pj t + ξj + εj,ht , where ξj is a brand fixed-effect, and the consumer
taste shocks (ε0,ht , εI ntel,ht , εAMD,ht ) are i.i.d. extreme value type 1. Consumers are
forward-looking and maximize expected and discounted intertemporal utility.29 Market shares for consumers currently owning a product with quality q ∗ is:
∗

sj t (q ) =

exp{vjcon (qt , t , q ∗ )}

con
∗
∗
exp{v0con (qt , t , q ∗ )} + exp{vIcon
ntel (qt , t , q )} + exp{vAMD (qt , t , q )}
(37)
where vjcon is the conditional choice value function in a consumer’s decision problem.
Using the distribution of consumers’ owned qualities, t , yields the market share of

27 The model restricts each firm to selling only one product because the large computational burden of

allowing multi-product firms in a model with dynamics in both demand and supply. Esteban and Shum
(2007) (for automobiles) and Gowrisankaran and Rysman (2012) (for digital cameras) estimate dynamic
demand models of differentiated product with multi-product firms and forward-looking consumers but
with supply side models that are substantially simpler than in Goettler and Gordon’s study.
28 Esteban and Shum (2007) also study the effects of durability and secondary markets on dynamic price
competition between automobile manufacturers.
29 This dynamic demand model is a simplified version of the model in Gowrisankaran and Rysman (2012)
which includes random coefficients, multi-product firms, and several product attributes.

287

288

CHAPTER 4 Dynamic games in empirical industrial organization

brand j :
sj t =



sj t (q ∗ ) t (q ∗ )

(38)

q∗

By definition, next period distribution of owned qualities, t+1 , is a known closedform function of t , st ≡ (sI ntel,t , sAMD,t ), and qt , that we can represent as t+1 =
F (t , st , qt ).
In each period, microprocessor firms make an investment decision to try to reach
a higher quality level. Change in quality, qj,t+1 − qj t , can be zero (unsuccessful investment) or a positive constant δ (successful investment). The probability of success
is denoted χj , and depends on the firm’s investment xj t , with the same functional
form as Pakes and McGuire (1994):
χj (xj t , qj t ) =

aj t (qj t ) xj t
1 + aj t (qj t ) xj t

(39)

where the term aj t (qj t ) represents firm j ’s investment efficiency that has the following form:
/
%
( 0
q̄t − qj t 1/2
aj t (qj t ) = a0,j max 1, a1
(40)
δ
where q̄t ≡ max{qI ntel,t , qAMD,t } is the frontier or highest quality in the industry at
period t. This is an increasing function of the technology gap q̄t − qj t , that captures
the idea that generating successful innovations is more difficult for the leader that is at
the technological frontier than for the follower that is catching up. This helps rationalize why AMD and Intel never drift to having too different quality levels. Parameters
a0,I ntel and a0,AMD allow for persistent differences in the investment efficiencies of
the two firms, that can rationalize why AMD reached the same microprocessor speed
as Intel despite having a substantially smaller level of R&D investment.
In addition, the non-frontier firm has marginal costs that are lower than the firm
with the highest level of quality. The frontier firm has costs that are λ0 , while costs
are reduced for the non-frontier firm by λ1 (q̄t − qj t ). That is, parameter λ1 represents
the dollar reduction in marginal cost per unit of quality difference with respect to the
leader (as quality q is the logarithm of microprocessor speed).
Note that the space of the state variables qt and t is unbounded, as they can
increase forever at increments δ. To deal with this issue, the authors impose the restriction that different structural functions are homogeneous of degree with respect to
quality. This restriction makes it possible to recast the state space as one relative to
the frontier q̄t . This modified state space is bounded.30
For the estimation of the model, the authors estimate first the marginal cost parameters λ0 and λ1 using proprietary production costs data from In-Stat/MDR, a market
30 This trick is used extensively to discuss balanced growth paths in the macroeconomics literature.

4 Empirical applications

research firm specializing in the microprocessor industry. The rest of the structural
parameters—both the demand parameters α, γ , ξI ntel , and ξAMD , and the supply
side parameters a0,I ntel , a0,AMD , and a1 —are estimated using the structure and predictions of the dynamic oligopoly model. Goettler and Gordon (2011) use a simulated
method of moments estimator, similar to the approach used by Gowrisankaran and
Town (1997). However, instead of assessing the gap between the data and the ergodic
distribution predicted by the model, they look at the prediction from the model starting in the observed state in 1993 all the way out until 2004. They consider moments
related to the firms’ innovation rates, R&D intensities, differential quality, frequency
of quarters where Intel is the leader, gap to the quality frontier, average prices, and
OLS coefficients in the regression of prices on qualities and in the regression of market shares on qualities. Parameter δ is fixed at 0.20 (i.e., 20%), and the discount factor
is fixed at 0.90 at the annual level.
The ratio between the estimates of γ and α shows that consumers are willing to
pay $21 for enjoying a 20% increase in quality during one quarter. According to the
ratio between ξI ntel − ξAMD and α, consumers are willing to pay $194 for Intel over
AMD. The model needs this strong brand effect to explain the fact that AMD’s share
never rises above 22 percent in the period during which AMD had a faster product.
The innovation efficiencies a0,j are estimated to be 0.0010 for Intel and 0.0019 for
AMD, as needed for AMD to occasionally be the technology leader while investing
much less than Intel.
The authors use the estimated model to implement two main sets of counterfactuals. The first set deals with the effects of competition on innovation. For instance, they
solve and simulate the model under the counterfactual scenario of Intel monopoly and
compare the results to the actual data. According to this experiment, the innovation
rate (i.e., the growth rate in frontier quality q̄t ) increases from 59.9% to 62.4%; investment in R&D more than doubles, increasing by 1.2 billion per quarter; price increases
by $102 (70%); consumer surplus declines by $121 million (4.2%); industry profits
increase by $159 million; and social surplus increases by $38 million (less than 1%).
Therefore, they find competition from AMD had a negative impact on the speed of
innovation, but overall it has had a positive effect on consumer welfare because the
competition effect on prices have than offset the lower quality. They also consider the
counterfactual scenario of a symmetric duopoly where the two firms have the same
demand brand fixed effects and innovation intensity parameters. The effects are basically the opposite to the first experiment: investment in R&D, innovation rates, and
average quality decline, but prices also decline and this effect more than offsets the
quality decline such that consumer welfare increases by $34 million (1.2%), industry
profits decline by $8 million, and social surplus increases by $26 million (less than
1%).
The finding that innovation by a monopoly exceeds that of a duopoly reflects two
features of the model: a monopoly must innovate to induce consumers to upgrade;
the monopoly is able to extract much of the potential surplus from these upgrades
because of its pricing power. However, if there were a steady flow of new consumers

289

290

CHAPTER 4 Dynamic games in empirical industrial organization

into the market, such that most demand was not replacements of older computers, the
monopoly would reduce innovation below that of the duopoly.
In a second set of counterfactuals, Goettler and Gordon study the claim that Intel used anti-competitive foreclosure practices against AMD.31 To study the effect
of such practices on innovation, prices, and welfare, the authors perform a series of
counterfactual simulations in which they vary the portion of the market to which
duo
Intel has exclusive access. Let sImon
ntel,t and sI ntel,t be Intel’s market shares under
monopoly and under free competition with AMD, respectively. The authors incorporate foreclosure using a simple model where the degree of foreclosure is measured
duo
by a parameter ζ ∈ [0, 1] such that sI ntel,t = ζ sImon
ntel,t + (1 − ζ ) sI ntel,t . The authors
solve and simulate the dynamic oligopoly model for a grid of values for parameter
ζ . Not surprisingly, margins monotonically rise steeply with ζ . However, innovation
exhibits an inverted U shape with a peak at ζ = 0.5. Consumer surplus is actually
higher when AMD is barred from a portion of the market, peaking at 40% foreclosure. This finding highlights the importance of accounting for innovation in antitrust
policy. The decrease in consumer surplus from higher prices can be more than offset
by the compounding effects of higher innovation rates.

4.2.2 Hard drive innovation: new products and cannibalization
Igami (2017) also studies the relationship between competition and innovation. He
focuses on the propensity to innovate of new entrants relative to incumbents in the
hard drive industry. Similarly to microprocessors, there has been dramatic fall in the
price of hard drive storage. However, in contrast to microprocessors where Intel has
had a dominant position for almost 50 years, the leading hard drive producers have
changed several times over the last forty years. These shifts correspond to periods
where the product format changed from 5.25 to 3.5 inch drives and from 3.5 to 2.5
inch. In addition, at some points in time, there are several dozen firms producing
hard drives, but there has been gradual exit from this industry down to four firms.
The active entry and exit of firms leads to a natural discussion on how the incentives
to innovate differ between new entrants and incumbents, given that innovation tends
to displace existing products.
The key empirical evidence that motivates this paper is that the propensity to
adopt a new product (e.g., producing the new 3.5 inch format instead of the old 5.25
inch) is substantially higher for new entrants than for incumbent firms. Igami focuses
on the transition from 5.25 to 3.5 inch format, and studies three main factors that
may contribute to the difference in the propensity to innovate of incumbents and
new entrants: cannibalization, preemption, and differences in innovation costs. For
an incumbent firm, the increase in sales and revenue from the introduction of a new
product comes partly at the expense of cannibalizing its old products. This is not
the case for a new entrant. Therefore, cannibalization may contribute to explain the

31 In 2009, Intel paid AMD $1.25 billion to settle claims of anti-competitive practices to foreclose AMD

from many consumers.

4 Empirical applications

higher propensity to product innovation by new entrants. The magnitude of this effect
depends, among other things, on the degree of demand substitution between new and
old products. Preemptive motives can encourage incumbent firms to early adoption
of new products to deter entry and competition from potential entrants. This factor
may partly offset the contribution of cannibalization. Last but not least, incumbents
and new entrants can have different costs of adopting new products. This difference
can go in either direction. Economies of scope between old and new products can
imply lower adoption costs for incumbent firms. On the other hand, incumbent firms
may exhibit organizational inertia that makes it costly to abandon old practices and
adjust the operation to the idiosyncrasies of the new product.
In Igami’s model, there are four (endogenous) types of firms in the market, and the
state of the market at period t consists of the number of firms of each type: potential
pe
entrants nt , incumbents producing only the old product nold
t , incumbents producing
only the new product nnew
,
and
incumbents
producing
both,
nboth
. Notice that this
t
t
does not leave room for differences in market share between firms. The vector of
state variables xt is completed by demand shocks for the new and old products. ξtnew
and ξtold . Every year t, potential entrants decide to enter with the old or new product, incumbents decide to exit or stay in the market, and old incumbents also decide
whether to adopt the new product.32 There is one year time to build for these entries,
exit, and adoption decisions to be effective.
Demand has the structure of a static logit model between old and new products
and an outside alternative. Following the standard structure in the Ericson and Pakes
(1995) model, incumbent firms compete in prices à la Bertrand. To apply a full solution method for the estimation of the structural parameters, and to avoid the issue
of the multiple equilibria in the counterfactual experiments, Igami imposes three restrictions that imply uniqueness of a MPBNE in his model: (i) the industry has a
finite horizon T that is certain and common knowledge; (ii) within each of the four
types, firms are homogeneous up to i.i.d. private information shocks in entry, exit,
and adoption costs; and (iii) every year t, firms take dynamic decisions according to
a pre-established order that depends on firm type. In the benchmark version of the
model, the order of moves is the following: first, old incumbents choose to exit, stay
and innovate, or stay and not innovate; second, incumbents producing both products
choose to exit or stay; third, new incumbents choose also between exit or stay; and
finally, potential entrants decide whether to enter or not. Igami presents estimates of
the model under other orders of moves. The estimates of the dynamic structural parameters (entry, exit, and adoption costs) are quite robust to the different orders of
moves considered.33
32 In principle, new incumbents might also choose to start producing the old product, and old incumbents

might decide to stop producing the old product. However, these choices are never observed in this industry
during the sample period.
33 See, for instance, Table 6 in Berry (1992), for an example of the impact of these assumptions on
ordering of moves on parameter estimates. In the context of repeated—yearly—interactions, it is plausible
that ordering of move assumptions are less material. Table 4 in Igami (2017) does a nice job of looking at
the impact of alternative assumptions on the ordering of moves in a dynamic game context.

291

292

CHAPTER 4 Dynamic games in empirical industrial organization

Igami estimates the dynamic parameters using Rust’s nested fixed point algorithm, as described in Section 3.3.1 above. The state space that Igami (2017) considers is large given that there may be dozens of firms within each endogenous type.
There are over 38,000 states in his model. This makes the computation of the maximum likelihood estimator using the NFXP algorithm computationally intensive. To
keep this cost tractable, Igami considers a parsimonious specification of the model
with only three dynamic parameters: φ, the fixed cost of operation; κ inc , the sunk
cost of adopting the new product for the old incumbents; and κ ent , the sunk cost of
entry with the new product for a new entrant.34 Igami estimates demand parameters
and marginal costs using standard static tools.
The parameter estimates show that the sunk cost of innovation is smaller for incumbents than for new entrants, κ inc < κ ent . That is, economies of scope seem more
important than organizational inertia. The magnitude of the estimated sunk cost of
innovation is between 0.6 and 1.6 billion dollars, which is comparable to the annual
R&D budget of specialized hard drive manufacturers like Seagate.
Igami (2017) uses the estimated model to implement counterfactual experiments
to evaluate the contribution of cannibalization and preemptive motives to the different
innovation rates of incumbents and potential entrants. To isolate the effects of incumbents’ incentive to avoid cannibalization, Igami divests each incumbent into a legacy
firm and new product firm. That is, for every old incumbent firm, there is an independent firm that decides whether adopt or not the new product. This counterfactual
incumbent type is more likely to enter the newer format as they do not internalize
the cannibalization of the old product. The equilibrium under this scenario shows
that the gap between entrants and incumbents in their innovation choices shrinks by
57 percent. To identify the effect of preemption, one needs to obtain an incumbent’s
behavior under the hypothetical scenario that the firm’s own entry decision into the
new format did not change what potential entrants would do. More specifically, Igami
assumes that incumbents’ strategies are the solution of a dynamic programming problem that assumes that potential entrants do not respond to the number of incumbent
firms in the newer format—they assume there are none of these in the market. This
counterfactual shows that the long term number of incumbents that enter the newer
format falls by 38 percent.35
These counterfactuals show that both cannibalization and preemption play an important role in the decision of incumbent firm to adopt the new format. However, in
the hard drive industry, the incentive to avoid cannibalization has dominated preemptive motives.
34 The computation time of solving for the equilibrium of the dynamic game does not depend on the

number of parameters. However, the number of iterations in the search of the parameter estimates does
increase with the number of parameters.
35 The counterfactual exercise of shutting down preemption motives is a difficult one. This issue has been
confronted by a number of papers, Chicu (2013) and Besanko et al. (2014) being perhaps the most notable.
Like any deviation away from Nash Equilibrium, fixing a coherent system of beliefs is always tricky when
shutting down one such mechanism. In this case, potential entrants may find it ex-ante unprofitable to enter
in equilibrium given their incorrect beliefs.

4 Empirical applications

4.2.3 Car innovation and quality ladders
Hashmi and Biesebroeck (2016) study the effect of market power on innovation in
the automobile industry. They propose and estimate and model of innovation and
quality competition that combines features of the static model of demand and price
competition in Berry et al. (1995) (BLP hereinafter) and the dynamic quality ladder
game in Pakes and McGuire (1994).
The automobile industry has many manufacturers and types of cars. Estimating a
dynamic game with the enormous state space that results from this number of firms
and products is impractical. The authors make substantial simplifying assumptions.
The model starts with a stripped down version of BLP, in which consumers choose
a manufacturer or brand (instead of a car model as in BLP). The utility of consumer
h if she chooses brand j at period t is uh,j t = θp pj t + ξj t + eh,j t , where pj t and
ξj t are the brand’s price and quality, respectively, and eh,j t is the usual extreme value
type 1 shock.36
Firm quality ξj t has a discrete and finite support and it evolves endogenously
as the result of the firm’s investment in R&D.37 The stochastic process for quality
depends on two forces: depreciation and successful innovation. Depreciation makes
quality decline in ξ units with an exogenous probability λd . Successful innovation
can make quality increase in ξ units with and endogenous probability λujt which
depends on the firm’s investment in R&D, xj t , and current quality according to the
following equation:
λujt = exp{− exp{θ1u ln(xj t + 1) + θ2u ξj t + θ3u ξj2t }}.

(41)

Therefore, we have that P r(ξj,t+1 − ξj t = ξ ) = (1 − λd )λujt , and P r(ξj,t+1 − ξj t =
−ξ ) = λd (1 − λujt ).
The estimation of the demand parameter θp is based on a standard IV method.
Quality ξj t is obtained as a residual from the demand equation. Then, these qualities
are discretized and the parameters λd , θ1u , θ2u , and θ4u in the stochastic process of
quality are estimated by maximum likelihood. Note that the estimation of all these
parameters does not use the predictions from the dynamic game.
Given the vector of state variables xt = (ξj t : j ∈ I) and a private information
i.i.d. shock in the cost of R&D investment, εj t , firms choose their investments in
R&D to maximize their expected present values. The cost of R&D investment is
given by the following cubic function:
$
!
c(xj t , εj t , θ c ) = θc,1 + θc,2 xj t + θc,3 xj2t + θc,4 εj t xj t
(42)

36 The authors measure brand price as the weighted average of the prices of all the car models the manu-

facturer sells.
37 In the BLP model, product quality is relative to the value of an outside alternative. This relative aspect

makes more plausible the assumption of finite support, at least for the automobiles product category that
does not present a trend in demand during the sample period.

293

294

CHAPTER 4 Dynamic games in empirical industrial organization

where θ c = (θc,1 , θc,2 , θc,3 , θc,4 ) are parameters to be estimated The parameters in
the cost of R&D are estimated from the predictions of the dynamic games using the
estimation method in Bajari et al. (2007). For the reduced form estimation of firms’
strategy functions in the first step of BBL, the authors consider a specification that has
the flavor of oblivious equilibrium or moment-based equilibrium, as the explanatory
variables are the firm’s own quality and aggregate moments in the cross-sectional distribution of all the firms’ qualities, i.e., mean, standard deviation, kurtosis, skewness,
and inter-quartile difference.
The final step of this exercise is to look at how changing the number of firms
alters optimal investment decisions within the estimated model. For this experiment,
and for computational reasons, the authors consider a strong simplification of their
dynamic game, with at most five automobile manufacturers, and where quality is
restricted to take 15 values. They solve for a MPBNE using the Pakes and McGuire
(1994) algorithm. The authors find that adding another firm would lower the rate of
innovation in this industry, and this effect is magnified with higher quality entrants.
An important consideration in this modeling exercise is the degree to which cars
are vertically versus horizontally differentiated. If cars were only vertically differentiated, then one might expect much more intense competition on the quality dimension.
The simplified demand system that is being used in this exercise, while practical for
shrinking down the state to a single firm dimension ξj , also shapes the conclusions
of this exercise in a way that is difficult to assess.
Xu and Chen (2020) study spillovers between firms’ R&D investments in the
Korean electric motor industry. The authors use the concept of oblivious equilibrium from Weintraub et al. (2008) to simplify computation, which also allows for the
use of standard estimation techniques like GMM. Indeed, in this type of work it is
sometimes hard to distinguish competitive models from oligopolistic ones. Kryukov
(2010) analyzes the development of new drugs in the pharmaceutical industry. Aw et
al. (2011) consider the synergies between firms’ investment in R&D and the decision
to export.
On the quality ladder side, Borkovsky et al. (2012) present numerical experiments
for this class of models. Lin (2015) develops and estimates a dynamic game of entry and exit and quality competition between nursing homes. Indeed, a large part of
health care operates in an environment of regulated prices, either for most countries
in world outside the US, or for a large fraction of health care expenditures in the
United States. In this setting, competition acts more directly on quality rather than on
prices.

4.2.4 Data on innovation
A notable aspect of this literature is how differently it treats empirical work from
the canonical empirical models presented in the previous sections. When looking
into innovation into new products and technologies, it is very rare to have a panel
of independent markets, or enough repeat innovations that are similar to be able to
estimate much from the dynamic choices of agents.

4 Empirical applications

There are some other papers that look at innovation through the lens of a dynamic model, but are able to utilize more cross-sectional data by looking at localized
adoption of new technologies. Schmidt-Dengler (2006) looks at the timing of MRI
adoption, while Caoui (2019) is concerned with adoption of digital projection in
movie theaters in France.
Overall, these studies contribute to filling in the theory literature on the relationship between competition and innovation. Given the flexibility of the Pakes and
McGuire (1994) framework, these calibrations to specific industries both in terms of
modeling details and parameter estimates help give us an idea of what predictions we
should expect, essentially through the accumulation of computational case studies.
However, like much of the IO theory literature before it, many of the outcomes in this
literature do depend on the details of the industry under study. This make extrapolation to new industries and innovations quite tricky, but also emphasizes the role of
capturing industry level detail properly.

4.3 Antitrust policy towards mergers
Merger policy occupies a central role in industrial organization, as antitrust is the
most important area in which IO economists shape the debate on policy. Whinston
(2007) provides an extensive discussion of antitrust policy on horizontal mergers.
However, most of their survey takes a static viewpoint on the impact of mergers,
essentially implying that all effects of a merger are realized immediately. In addition,
in the Department of Justice and Federal Trade Commission’s Horizontal Merger
guidelines U.S. Department of Justice and Federal Trade Commission (2010), only
Section 9 discusses the role of post-merger entry or exit.

4.3.1 Endogenous mergers
One of the first applications of the Ericson and Pakes (1995) framework of dynamic
oligopoly was the work of Gowrisankaran (1999) which proposes a model of endogenous mergers and market structure. More recently, Mermelstein et al. (2020)
have attempted to address the problem of endogenous mergers from the perspective
of a competition authority which is assessing different merger rules with and without
commitment power. These papers address the thorny problem of how to deal with
a sequence of mergers, considering that the free-rider problem from a merger shifts
as the industry becomes more concentrated.38 A full evaluation of a merger should
take into account this type of effect. Modeling endogenous mergers is complicated as
it involves a bargaining process between firms that should take into account, in one
way or the other, the value added from different possible mergers. Thus, if firms are
not identical, they need to evaluate all alternative merging parties, and indeed, the
sequence of future mergers. Several of the figures outlining the protocol for merger

38 A free-rider problem in merger occurs when non-merging parties are liable to be the largest beneficia-

ries of the merger. See Farrell and Shapiro (1990).

295

296

CHAPTER 4 Dynamic games in empirical industrial organization

choice in Gowrisankaran (1999), such as Fig. 1 and Fig. 2, can only be described as
baroque, which underscores the complexity of modeling merger choice.
Another attempt at embedding the negotiation process inside a dynamic game,
in this case the hospital-insurance company problem first considered by Ho (2009)
is Lee and Fong (2013). In this game, each period there is some probability that
two parties get the chance to include a hospital in their network, and one party can
make a take-it or leave it offer. The outside option in this bargaining game is the
continuation value if agreement is not reached, much like the game described by
Shaked and Sutton (1984) following Rubinstein (1982). This paper also requires an
enormous level of skill with computation to implement.
In practice, to compute these theory models, the authors need to make numerous
choices as to the parameters that should be used, and these choices tend to be somewhat disconnected with the empirical reality in any given industry. For instance, both
Gowrisankaran (1999) and Mermelstein et al. (2020) look at Cournot competition in a
homogeneous product industry. A step in the direction of bringing data to bear on this
issue is the Igami and Uetake (2020) model of mergers in hard drive manufacturing.
One of the more difficult issues to resolve in this literature is the choice and timing
of merging parties. Igami and Uetake (2020) make merging opportunities a random
arrival process. It is difficult to see how empirical work can improve on this—which
is unfortunate since data does not seem to inform merger choices of firms much.39
Igami and Uetake (2020) propose and estimate a simpler dynamic game of endogenous mergers using the same industry and similar assumptions about market
structure and profit function as in Igami (2017). In each period, a firm can make a
merger offer to another firm, and it pays a sunk cost of making this offer. If an offer
is made, the two parties, i and j , negotiate an acquisition price pij , through different
bargaining protocols such as take-it-or-leave-it or some form of Nash Bargaining.
Firms have productivity levels ωit , but this immediately poses the question of
what happens to productivity when firms i and j merge. Does the merged firm have
productivity ωit , or ωj t , a convex combination of the two productivities, the maximum of the two, or perhaps an even higher productivity due economies of scope?
Following Farrell and Shapiro (1990), Igami and Uetake (2020) assume that productivity after a merger becomes:
2
1
(43)
ωi,t+1 = max ωit , ωj t + ij,t+1
where ij,t+1 is a merger synergy term that is drawn from a Poisson distribution
with parameter λ. Note that this parameter λ and the cost of making a merger are
key to guiding how quickly firms will want to merge. Furthermore, the change in
productivity of firms following a merger can be used to identify the value of the λ
parameter.
39 There is huge variation in the volume of merging activity over time that is not explained very well, e.g.,

so-called merger waves. See Jovanovic and Rousseau (2002), among others, for a discussion of the issues
in this literature.

4 Empirical applications

A merger authority needs to assess which mergers to let through, and the simplest
possible merger policy is one that simply sets a minimum number of competitors for
any industry. In the mobile phone market, there is a serious discussion of how markets
perform with three or four competitors. Likewise, there is a discussion in the airline
industry on whether the authority’s merger decisions lead to too few competitors in
the industry (Olley and Town, 2018). In both cases, one can think of the competition
authority as picking the minimum number of competitors. Note that Mermelstein
et al. (2020) consider more sophisticated issues in this policy discussion, such as
whether the authority can credibly commit to its merger rules, or instead adapts sequentially its merger policy.
Igami and Uetake (2020) find that, given their estimates for the hard drive industry, a threshold of N = 3 firms is close to being socially optimal.

4.3.2 Evolving market structure and mergers
An important component of antitrust scrutiny of a merger is the possibility of postmerger entry. Indeed, in the simplest steady-state model of market structure with
identical firms, market structure is completely unaffected by merger activity. If the
free entry condition dictated that four firms could be supported in a market before
the merger, then there will be four firms in the market regardless of whether the
merger goes through or not. This calculus can be altered by realistic dynamics in the
entry and exit process, which lead to slow adjustments of market structure. Indeed, an
important takeaway from the literature on firm dynamics is how slowly changes occur
in many industries. For instance, Collard-Wexler and De Loecker (2015) looks at the
entry process of mini-mills into the production of steel. While this new technology
does displace the older integrated producers in the steel bar product segment, and
increasingly in the steel sheet segment, the entire process takes more than forty years,
so the process of reallocation is quite slow.
Collard-Wexler (2014) looks at the effects of a merger that would knock out a
competitor in the ready-mix concrete market, using data on ready-mix concrete markets in isolated markets in the tradition of Bresnahan and Reiss (1991), and more
specifically of the model of industry dynamics in Bresnahan and Reiss (1994). Let
nmt be the number of incumbent firms in a market m at time t. This number evolves
according to firms’ entry and exit that determines a transition probability function
P (nmt |nm,t−1 , bmt ), where bmt is a vector of exogenous market characteristics that
also evolves over time. In his application, the model of Abbring and Campbell (2010)
is used to justify a single equilibrium in the entry and exit game, which imposes restrictions on the ordering of moves of firms, as well as the structure of the process
for demand. This means that the entry and exit policy rules follow demand thresholds, where the gap between exit and entry thresholds is indicative of the importance
of sunk costs. Collard-Wexler (2014) finds that a merger that initially transforms a
duopoly market into a monopoly market would induce 9 to 10 years of monopoly relative to the benchmark of no merger. Indeed, the analysis of a merger in this market
is closer to a situation where there is not post-merger entry all at.

297

298

CHAPTER 4 Dynamic games in empirical industrial organization

Benkard et al. (2010) propose a dynamic analysis of mergers in the airline industry. The airline industry works particularly well from an empirical perspective since
the researcher can look at a cross-section of airline routes—indeed this was the motivation behind how Berry (1992) designed his analysis.40 Moreover, there has been
ongoing displacement of legacy airlines in the industry by newer entrants such as
Southwest and JetBlue. Thus, dynamic entry considerations, for instance by the new
carriers, can be important in assessing the impact of a merger in the airline industry.
Benkard et al. (2010) estimate conditional choice probabilities for the likelihood
of entering or exiting a route given by Pi (ait |xt ), where xt includes a demand covariates (such as population at each endpoint), characteristics of an existing airline’s
network, and competitors routes. The authors also estimate the (Markovian) stochastic process of the exogenous variables in vector xt , given by Q(zt+1 |zt ). A merger
will alter which firms offer service on different routes: they change the state of the
market from xno-merger to xmerger . Given the CCPs for firm’s entry and exit choices
and the process for exogenous states, the authors can forward simulate how the market evolves if a merger occurs or not and compare the expected outcomes on market
structure. Key to this exercise is that a market structure simply changes the set of
firms that participate in a route market but does not create new potential entrants in
that route.41
One of the main conclusions from this exercise is that the trends of the market
matter. Indeed, much of the dynamic effects of mergers come not from changes in
the number of competitors in the markets that experienced a merger, but instead from
existing firms exiting markets in the absence of a merger.

4.3.3 Revealed merger efficiencies
A final tack on looking at the impact of mergers is to attempt to uncover the cost
efficiencies that they generate. Outside the world of antitrust litigation and their expert witnesses, these costs are thought to be very difficult to ascertain. Indeed, IO
economists tend to be averse to using accounting cost data to assess synergies, and
besides which, how would one know what these costs would have been in the counterfactual where the merger did not occur?
One approach to this problem is to use revealed preference to back out what perceived merger efficiency must be to rationalize the merger choices of firms. Jeziorski
(2014) applies this model to estimate cost efficiencies after the 1996 deregulation
of US radio industry. Likewise, a working paper version of Stahl (2016) looks at
revealed preference and mergers for the market for TV.42
40 Clearly this approach ignores cross-market synergies in the airline routes. This is a topic of active

research.
41 In these applications, an airline is considered a potential entrant in a route (say Chicago-New York

route) if it operates flights in one of the two cities that define the route. After a merger, airlines’ entry-exit
decisions in different routes can generate changes in the set of potential entrants in other routes. Benkard
et al. (2010) ignore these endogenous changes and impose the restriction that the set of potential entrants
in every route remains constant as in the first period when the merger occurs.
42 The working paper version of this paper was dynamic, but this was dropped in the published version.

4 Empirical applications

4.4 Dynamic pricing
The standard version of the Ericson and Pakes (1995) model assumes that price
competition is static. This is a convenient assumption, both for computation and estimation, as it allows estimating parameters in demand and marginal costs using static
methods. However, there are often good reasons to believe that firms’ pricing decisions are dynamic and forward looking. Dynamics in demand, either because of
durability or storability of products, introduces important forward looking considerations in pricing. We have already seen this in Section 4.2.1 in the model of Goettler
and Gordon (2011), where PCs/microprocessors are durable products and current
prices affect consumers’ replacement decisions, and therefore, future demand and
profits. As mentioned in that section, Esteban and Shum (2007) and Gowrisankaran
and Rysman (2012) are also good examples of applications with dynamic price competition because of product durability. Hendel and Nevo (2013) estimate a dynamic
game of firms’ intertemporal price discrimination under product storability and consumer stockpiling using supermarket data on two-liter bottles of Coke and Pepsi.
Other factors that introduce dynamics in pricing decisions are price adjustment costs,
firm inventories and capacity, network effects, and learning. In this section, we review
empirical applications of dynamic pricing games that incorporate these factors.

4.4.1 Competition with price adjustment costs
Dynamic models of price competition with price adjustment costs—or more generally, sticky prices—have a long tradition in IO (Rotemberg, 1982; Gertner, 1985;
Rotemberg and Saloner, 1987). They are also among the first empirical applications
of dynamic structural models in IO. Slade (1998) proposes a model where the demand for a product in a store depends on a stock of goodwill that accumulates over
time when the store charges low prices, and erodes when the price is high. The model
incorporates also menu costs of changing prices. Aguirregabiria (1999) studies the
relationship between inventories and prices in supermarkets. He proposes a model
where retailers have lump-sum costs of placing orders, such as menu costs of changing prices, face substantial demand uncertainty, and experience stockouts. In addition,
this paper was pathbreaking in that it was one of the first applications of the HotzMiller approach to firm decisions. Kano (2013) shows that strategic complementarity
in price competition, together with menu costs, implies that firms may decide not to
respond to firm-idiosyncratic shocks because they know that their competitors will
keep their prices constant. Mysliwski et al. (2020) study price competition between
manufacturers in the UK butter and margarine industry. They propose and estimate
a dynamic game that incorporates both consumer brand switching costs and firms’
price adjustment costs. Ellison et al. (2018) study price competition between online
sellers of computer components. They propose and estimate a dynamic game where
managers have costs of acquiring information about other firms’ prices, and of changing their own price.

299

300

CHAPTER 4 Dynamic games in empirical industrial organization

4.4.2 Limit pricing
There has been a long concern in IO about the possibility that incumbents can deter
entry. One such approach is limit pricing: setting a low enough price so that a potential entrant would find it unprofitable to enter the market. However, it is not clear
that this type of strategy is subgame perfect, since an incumbent would not want to
choose this low price in the subgame where the entrant comes in.
A theoretically grounded motivation for limit pricing was provided by Milgrom
and Roberts (1982). In their model, potential entrants have incomplete information
about the cost of incumbents: they observe a signal for these costs but do not know
them perfectly. Incumbents’ prices contain information about their costs, and therefore potential entrants use also this information when they make their entry decisions.
If a potential entrant believes that incumbents’ costs are low enough, its best response
is not to enter. This leads to the possibility that incumbents choose prices that are
lower than those they would choose in the absence of entry deterrence motives. While
the Milgrom and Roberts (1982) is certainly a reasonable way of rationalizing limit
pricing behavior, it is not clear if this mechanism is relevant in actual markets. In particular, this mechanism requires a reasonably large amount of imperfect information
about costs, and that repeated interaction between firms should not uncover too much
of this information over time. Empirical applications have provided evidence on the
relevance of limit pricing in different industries.
An interesting piece of empirical evidence on limit pricing comes from the more
reduced form work of Goolsbee and Syverson (2008) on limit pricing in the airline
industry. They use a novel source of exogenous variation in the probability of entry,
based on Southwest entering the airports of two endpoints of a route, say starting to
fly to Jacksonville and Tampa, but not currently serving the Tampa to Jacksonville
route. They then observe incumbent airlines lowering their pricing in response, not to
the actual entry of Southwest Airlines, but to the raised potential entry of Southwest.
This is credible evidence that limit pricing motives are quantitatively relevant in the
airline industry.
To fully bridge the model of Milgrom and Roberts (1982) with the evidence from
Goolsbee and Syverson (2008), one needs to build a structural model of limit pricing,
and this is the purpose of Sweeting et al. (2020)’s paper. They study airline pricing in
109 routes with a dominant incumbent airline that faces potential entry from Southwest during 1993–2010. Air travel is a natural application as many (smaller) routes
have a dominant incumbent with (changing) private information about its operating
cost. For instance, incumbent hub-and-spoke carriers have private information about
the profitability of connecting traffic.
Sweeting et al. (2020) build a novel and analytically tractable model of dynamic
limit pricing. There is an incumbent (with index I ) and a potential entrant (with index E). The incumbent has a constant marginal cost cI,t that evolves according to a
first order autoregressive process. Firms’ products are differentiated, and consumer
demand has a nested logit structure as in Berry (1994). Demand is common knowledge to the incumbent and the potential entrant. Every period t, the incumbent sets
its price pI,t . The potential entrant (i.e., Southwest) does not know the incumbent’s

4 Empirical applications

cost cI,t , but it uses the history of incumbent’s prices and Bayesian updating to construct beliefs about this cost. Every period, the potential entrant decides whether to
enter the market. In the absence of asymmetric information and entry deterrence motives, the model is a standard static model of Bertrand competition in a differentiated
product industry. Note that, because the incumbent’s marginal cost changes randomly
over time, asymmetric information does not disappear over time for a potential entrant that never enters: it never perfectly learns the incumbent’s cost from watching
its pricing.43 Sweeting et al. (2020) show that this dynamic game has a unique fullyseparating MPBNE for any finite number of periods. This equilibrium can be easily
computed, enabling empirical implementation.
The key parameters that need to be estimated relate to the distribution of entry
costs and the variation in the incumbent’s marginal cost cI,t . In particular, the more
variation in marginal costs for the incumbent, the greater the asymmetric information,
and thus, the larger the incentives for limit pricing. Likewise, if marginal costs vary
considerably over time, this information asymmetry does not shrink much over time,
and Sweeting et al. (2020) find considerable variation in costs over time as inferred
from pricing choices.
The estimation of this model shows that limit pricing substantially lowered prices,
increasing consumer surplus by $600 million and increasing total welfare by $500
million on the 109 small routes studied. Subsidizing entry can have substantial welfare benefits, e.g., a subsidization program costing $8000 would have increased
consumer welfare by $9.7 million while lowering incumbent profit by $4.7 million.
The authors also present several extensions of the basic model, including “two-way
learning” (in which the incumbent also learns over time about the entrant’s cost), and
endogenous evolution of marginal cost that depends on endogenous capacity and demand for connecting traffic. Perhaps most importantly, this paper shows that the limit
pricing model of Milgrom and Roberts (1982) may have real bite in certain markets.

4.4.3 Dynamic pricing with network effects
The dynamic consequences of indirect network effects (Katz and Shapiro, 1985) have
been studied empirically, most extensively for video game consoles. Consumers purchase video game consoles not only because of the intrinsic value of this equipment
but also because they grant access to libraries of video game titles. Moreover, software developers release products based on their expectation of console sales. This
creates strong indirect network effects, where software developers value popular consoles, and consoles are more popular among consumers if more software has been
developed for them. This sets up a dynamic game with positive spillovers in the
actions of consumers and developers. For instance, a manufacturer that introduces
a new video console has an incentive to start fixing a very low price—it could be
even below marginal cost—to build a substantial group of clients that generate positive spillovers and future demand for its product—echoing the dynamic incentives in
43 This is in contrast to Jovanovic (1982)’s model of entry and asymmetric information, where a firm’s

type is constant over time.

301

302

CHAPTER 4 Dynamic games in empirical industrial organization

Benkard (2004). The process of increasing a firm’s market share because of indirect
network effects is denoted “tipping.”
Dubé et al. (2010) study dynamic price competition and tipping in the market
of video game consoles. There are N video console manufacturers indexed by j .
Every period t, consumers who have not yet purchased a console choose between not
purchasing (waiting at least one more period) and purchasing one of the N consoles
in the market. The value of purchasing a console j is ωj t + δj − α pj t + ξj t + εh,j t ,
where pj t is price, ξj t is a demand shock, δj ’s are brand fixed effects, εh,j t is a logit
consumer specific shock, and ωj t is the expected and discounted value of using this

product in the future, which is equal to Et ( Ts=0 γ nj,t+s ) where γ is a parameter
and nj,t+s is the number of video game titles available for console j at period t + s.
Consumers are forward looking, and have rational expectations about future prices
and titles of each console.
In a model with indirect network effects, the number of titles nj t depends on the
cumulative number of consumers who have purchased this console, represented by
variable yj t . The authors do not model explicitly (structurally) the behavior of software developers, and instead consider a simple reduced form function that relates nj t
and yj t .44 The state of the market at period t is xt = (yj t , ξj t : j ∈ X ), which is also
the vector of state variables in consumers’ and firms’ dynamic decision problems.
Every period t, firms choose the price of their video consoles to maximize expected
and discounted intertemporal profits. They have an incentive to lower the price of
their consoles, at least initially, in order to tip market shares to their benefit.
The set of model parameters consists of the demand parameters, manufacturers’ marginal costs, and the discount factors of consumers (βc ) and firms (βf ).
The authors estimate demand parameters using the demand part of the model, the
simulation-based estimation method in Hotz et al. (1994), and fixing consumers’ discount factor at βc = 0.90. The estimate of the network effect parameter, γ , is positive
and statistically significant. The economic significance of these network effects is
evaluated using counterfactual experiments. The authors do not estimate marginal
costs but instead use estimates from industry reports. Based on these parameters, the
authors solve for an equilibrium of the dynamic game under different levels of network effects as measured by parameter γ , including the estimated value and γ = 0
(no network effects). Firms’ discount factor is fixed at βf = 0.99 under the argument
that firms are more forward looking than consumers. These numerical experiments
show that tipping is not a necessary outcome even if indirect network effects are
present, but it appears when γ becomes large enough. They also show that tipping
can lead to a substantial increase in market concentration of 24 percentage points or
more.

44 As usual, this shortcut implies that some counterfactual experiments that modify structural parameters

in consumers’ demand or in the costs of console manufacturers can have an impact on the behavior of
software developers, but this reduced form equation cannot account for this effect. This is the well-known
Lucas critique of reduced form models.

4 Empirical applications

Lee (2013) also estimates a structural model of network effects in the video console industry. In contrast to Dubé et al. (2010), Lee’s model also endogenizes the
behavior of software developers. This paper focuses on the role of exclusive titles and
vertical integration in the evolution of the market for consoles, but does not model
the console’s pricing decision. Exclusivity can either harm consumers by restricting
the availability of software, or aid them, by making it easier for a new platform to
enter the market.
Lee’s model is similar to the one by Dubé et al. (2010), but with the following
differences: (i) the value to consumers of a particular portfolio of titles is explicitly
included in consumer utility; (ii) the portfolio of titles for a platform/console is an
endogenous variable that is the result of software developers’ decisions; (iii) software
developers choose in which platforms/consoles to release their titles, including the
option of releasing titles on multiple platforms, and (iv) platform pricing is exogenous
in this model. Both consumers and software developers are forward looking.
Most of Lee’s model is estimated using tools of dynamic demand. However, the
software developers’ porting costs are estimated using a moment inequality approach.
The profits of the observed console porting choices must be greater than alternatives,
say either the choice of only release on the PlayStation versus porting the title for
release on both the PlayStation and the Xbox. The assumption that Lee makes is that
the expected future paths of console purchases and software releases are only affected
by the changes in the lifetime hardware utility from these release decisions. Thus, a
software title such as “Halo” understands that if it chose to port to the PlayStation,
this would alter the number of consumers who would adopt, and software developers
who would develop for, the console in the future. Lee’s set estimate of porting costs
is between $150,000 to $200,000, which implies that popular games would unlikely
develop only for a single console in the absence of integration or exclusive contracts.

4.5 Regulation
In this section, we review applications that study how regulations can have effects
on firms’ dynamic incentives, such as the environmental regulation studied by Ryan
(2012). Note that a central aspect of regulation for IO economists, antitrust, has already been discussed in Section 4.3.

4.5.1 Environmental regulation
Fowlie et al. (2016) build on Ryan (2012) to assess the efficacy and efficiency of various proposals to curb greenhouse gas emissions while guarding against emissions
leakage, which is the migration of pollution from regulated to unregulated jurisdictions. Using the US Portland cement industry as a backdrop, the authors examine
several policy designs for allocating pollution permits in a cap-and-trade system
where the economic environment is complicated by both market power and imports
from unregulated jurisdictions. As Buchanan (1969) points out, completely internalizing an externality through a Pigouvian tax is generally inefficient when firms have
market power as they are already reducing their output. Additionally, when only a

303

304

CHAPTER 4 Dynamic games in empirical industrial organization

subset of sources are regulated (incomplete regulation), the regulated firms are placed
at a competitive disadvantage such that, as compliance costs increase, supply shifts
from the regulated to the unregulated firms, potentially creating the ironic outcome of
increasing overall pollution. Policymakers have sought tools to balance these countervailing forces. In this paper, the authors consider the long-run effects of allocating
permits to domestic firms via four different mechanisms: a permit auction (equivalent
to a carbon tax); a grandfathering scheme where a fraction of permits are given for
free to incumbent firms based on, say, historic emissions levels; a dynamic updating
scheme where permits are allocated each period in proportion to output or emissions
in the last period; and finally, a border tax adjustment which penalizes imports according to their foreign carbon intensity.
The authors assume that the price of a pollution permit is equal to the social cost
of carbon and consider a range of damages. They find that all four allocation mechanisms result in social losses for social damages below $40 per ton of carbon dioxide.
This is driven by the sum of losses in the product market and emissions leakage exceeding the benefits of the carbon abatement. The largest losses occur when firms
have to bear the full cost of compliance, under the auctioning and grandfathering
mechanisms. Policies that allocate permits on the basis of emissions or production
do much better, since they (partially) address the welfare losses that are driven by
reductions in domestic output. In all cases, welfare effects are magnified by firm exit,
particularly when regulated firms do not have any compliance cost assistance. When
damages are above $40 per ton, dynamic permit allocations and the border tax adjustment scheme both result in social welfare gains.
To highlight the effects of accounting for dynamics, the authors decompose their
welfare measures into product market surplus, emissions reduction, and emissions
leakage for both the static and dynamic case. In all cases, the static estimates look
better than the dynamic estimates, as they miss the changes to market structure that
the various permit mechanisms induce. This paper highlights the important role that
dynamic games frameworks can play in assessing substantive problems in environmental regulation.

4.5.2 Land use regulation
Suzuki (2013) considers the dynamic effects of land-use regulation (“zoning”) on
business entry. Zoning restrictions may constrain building characteristics or uses
within a certain geographic area. Examples include banning certain exterior materials (e.g. no visible siding from the street), requiring buildings to conform to stylistic
templates (e.g. in historic districts all new buildings must look original to the neighborhood), limiting which types of businesses can operate (e.g. restricting commercial
and industrial operations to be distant from residential areas or where alcohol-serving
establishments can be located), and capping how tall buildings can be.
Suzuki focuses on the effect of land-use regulations on mid-scale chains in the
Texas lodging industry. This industry is a promising setting for examining these regulations: land-use regulation is a first-order cost component for hotels, competition
is local, and the author argues that agents in this industry are aware that land-use

4 Empirical applications

regulations can serve as effective barriers to entry. Data on hotel revenue comes
from quarterly taxation data collected on every hotel in Texas. Suzuki constructs
firm-market revenue functions that depends on market characteristics, chain characteristics, and the degree of competition in the market. A key input to the study is
the use of a land use regulation index to proxy for the stringency of regulation. He
follows Gyourko et al. (2008), who produces a range of residential land-use intensity
indices (commercial indices were not available for his analysis) based on a survey of
local governments. These indices include factors like the average number of months
developers wait to receive building permits, whether there are density restrictions,
and if developers have to pay for infrastructure upgrades related to their projects. The
author focuses on six chains that account for 90 percent of the mid-scale chain hotels
in Texas and defines the relevant market as a county. Similar to Bresnahan and Reiss
(1990), he restricts his analysis to counties that have data on land use regulations, are
not located in the four largest urban regions of Texas, have at least 50,000 residents,
and have had at least four openings and closings during the sample period. Of 254
counties in Texas, this filters out all but 35.
Suzuki builds a model of entry and exit in this industry for hotel chains. Players
are chain hotel operators in the mid-scale segment. State variables include the number
of hotels operated by each firm in the market and exogenous market-level characteristics such as population. Firms open and close hotels in local markets in each period
to maximize expected discounted profits, with firms paying stochastic entry and exit
costs. Since he observes (accounting) revenues directly, he also posits that firms pay
fixed costs to operate. The author uses the method in Bajari et al. (2007) to recover
the distributions of entry and exit costs. The revenue function is estimated using OLS,
and the policy functions are estimated using a multinomial logit. In addition to the
standard two-step procedure for estimating dynamic parameters, the author includes
a third step where he regresses market-level cost estimates on the land-use indices to
decompose which regulations drive costs.
Suzuki finds that the average hotel pays approximately $250,000 each quarter per
hotel. The cost of opening a new hotel is estimated to be $2.4 million, with substantial
heterogeneity by chain. These numbers are roughly in the range of what industry
sources report as building costs, although this comparison is tempered again by the
issue of accounting versus economic costs. Interestingly, the costs appear to be much
lower for one of the chains; this may be driven by the imprecision of the policy
functions for that chain, as it had relatively few entries and exits during the sample
period.
The third-stage regression of operating and entry costs on land use stringency
are unfortunately imprecise. This is mainly because all regulatory variation is crosssectional and the set of markets in the estimation is very modest. To try to discern
some deeper insights into what is happening, Suzuki runs several counterfactual experiments. Limiting the analysis to a subset of three counties and capping the number
of active hotels per chain at three to make computation feasible, he solves the dynamic model and simulates outcomes under lenient (costs one standard deviation
lower), observed, and stringent regimes (costs one standard deviation higher). He

305

306

CHAPTER 4 Dynamic games in empirical industrial organization

finds that the number of active hotels ranges by about one active hotel between the
two extreme counterfactuals.
This paper illustrates both the promise and limitations of the dynamic games literature. The research question is very interesting, as land-use regulations are plausibly
a first-order determinant of firm density, variety, and location. Indeed, a structural
model of housing supply which is realistic, and as a function of this realism needs
to be estimated from data, would be a huge innovation in the literature in urban economics. However, even with excellent data on revenues of all players in each market,
which is data that is typically hard to come by outside of regulated settings, estimating a link between regulation and market outcomes is very difficult. Part of that is
driven by the need to have clean market definition, which in this setting resulted in
discarding almost all of the data. Some of the markets were thrown out for being too
small and/or not having enough variation; we note that this is in some sense the exact
opposite of the ideal market in Bresnahan and Reiss (1990), where ideally one wants
to observe the stable, unchanging long-run market configuration. This concern is amplified by the relatively high data requirements for estimating dynamic parameters;
it is not sufficient to have high-quality data, it is also necessary to see a sufficient
amount of variation across all actions that have associated parameters. That problem
is exacerbated by the very nature of the strategic interactions that these models focus
on: the fewer firms, the more market structure matters, but the lower the probability
of observing sufficient variation to precisely recover underlying dynamic primitives.

4.5.3 Product variety
Market structure consists not only of the number of firms and products, but also which
goods they offer. Sweeting (2013) examines the response of radio stations to the introduction of additional licensing fees for playing music. This paper is an example of
how the primary welfare effects of a policy may be driven by a change in the types of
products firms offer rather than through prices. The paper considers the effects of the
Performance Rights Act of 2009 in the US, which stipulated that radio broadcasters
should pay performance rights in addition to the composition rights that they already
paid. The fees would convert to a flat rate for stations with revenues above a certain cap, while noncommercial and talk stations would be exempt. Ambiguity in the
legislation led to the possibility of performance fees as high as 25 percent of advertising, an order of magnitude above composition fees. Sweeting develops a structural
model to estimate the propensity of firms to switch from music formats to non-music
formats as a result of these fees.
Radio stations are a good place to examine the effects of fees on product variety for several reasons. First, markets are local due to limited broadcasting range
(echoing the isolated markets of Bresnahan and Reiss (1990)), radio stations fall into
generally easily-definable segments (e.g. classical, rock, country, top 40), spectrum
constraints restrict the number of active radio stations in each market, and demographics vary widely across the sample, providing strong demand-side instruments.
Heterogeneity in customer demand is important for two main reasons: match-quality
between listener and station depends on tastes, which vary with observable demo-

4 Empirical applications

graphics, and advertisers value different listeners by demographics. In a string of
papers that use either regressions or a static model of entry, (Waldfogel and Berry,
1999; Berry and Waldfogel, 2001; Berry et al., 2016) investigate issues of product
variety in the radio industry such as the effect of mergers and free entry on product
variety.
The players in Sweeting’s model are radio station operators in local markets. Each
firm has a per-period profit function that consists of advertising revenues, fixed cost
savings from operating several stations in the same format, repositioning costs that
are incurred when a station changes format, and a vector of action-specific private
information shocks. Advertising revenues are a function of listener demand, which is
modeled using a discrete choice model. Utility is dependent on demographics, which
are modeled to be slowly changing over time, introducing additional dynamics into
the model. Finally, station quality is assumed to evolve according to an exogenous
AR(1) process.
Sweeting estimates his model using a combination of methods, primarily variants
of Aguirregabiria and Mira (2007), with a set of robustness checks following Bajari
et al. (2007). He estimates demand statically, recovering a rich set of preferences interacting demographics and station characteristics. For example, Black listeners have
much higher marginal utility for urban formats than for country, while Hispanic listeners have particularly strong preferences for Spanish-language stations. Per-listener
revenues are estimated as a function of demographics using a linear regression. The
author reports four different sets of estimates, according to which estimator they came
from: two variants of the pseudo-likelihood from Aguirregabiria and Mira (2007),
one following Pakes et al. (2007), and one using the forward-simulation approach of
Bajari et al. (2007). One of the key innovations that he has to make in the pseudolikelihood method is the use of an approximation to the value function using basis
functions. The first three approaches give roughly similar estimates for most of the
coefficients in the model. Unsurprisingly, given how the estimators use statistical information from the model, the main difference is that the Pakes et al. (2007) estimator
tends to be less efficient than the other two approaches. Sweeting reports estimates
from the forward-simulation exercise using two different objective functions. The
original BBL function is the squared error of inequalities that violate the optimality
condition of Eq. (35), at all states. The second approach is inspired by Pakes et al.
(2015) (PPHI), where the optimality condition has to be true on average across states.
The PPHI estimator trades off a loss of statistical information against the possibility
of being more robust as it uses averaging, which may smooth out approximation errors in the estimated policy functions and simulation error from forward simulation.
A second tradeoff is that the PPHI approach produces set-identified estimates, as there
are only six inequalities. Comparing the estimates from the two forward-simulation
estimates, Sweeting finds that the BBL point estimate lies outside the 95 percent
confidence interval estimated by PPHI for six of the nine parameters, which may indicate that the BBL estimates are biased in his sample. He ends up using the PML
estimates in the counterfactuals, as they generally were consistent with the estimates
from PPHI.

307

308

CHAPTER 4 Dynamic games in empirical industrial organization

Sweeting simulates market evolution under two counterfactual fee schedules. He
computes equilibrium outcomes for all markets when fees are set at zero, ten, and
twenty percent of advertising revenues. He simulates out forty years and collects
various summary statistics. The first finding is that a significant percentage of musicplaying stations switch to non-music formats when fees are imposed, especially at
the 20 percent level. He estimates that 578 stations would still be playing music
after 40 years, starting from a base of 713. There is heterogeneity across formats,
with the Urban format losing the least stations. Non-music formats gain in all three
settings, partially reflecting demographic changes that increase the number of consumers that have preferences for Spanish-language stations. Much of the change takes
place within five years, but the industry is still adapting at the forty-year mark. Indeed,
the ability of dynamic models to assess the speed of adjustment to a new steady-state
is an important improvement over the previous static literature on radio and product
variety. The general takeaway is that which products are offered can be sensitive to
policy choices. While no welfare numbers are reported, the loss of choice may have
significant implications on consumer surplus and producer profits.

4.5.4 Industrial policy
Kalouptsidi (2018) examines the effects of government subsides in China on firm entry into shipbuilding, which is upstream to her previous work on the global shipping
industry in Kalouptsidi (2014) discussed in Section 4.7 below. This is an interesting
use of the dynamic games framework, since state-directed subsides are not publicized, as they may be in violation of international trade agreements, but they can
be inferred using data on firms’ actions and the principle of revealed preference.
The author uses a dynamic framework to detect the subsidies and infer their size,
and then computes a counterfactual world without the subsidies in order to calculate their incidence across domestic and foreign producers. There are dynamics on
both the consumer and supply sides of the market for ships. Shipyards have backlogs
that accumulate over time; there may be congestion (negative costs) or learning-bydoing (positive benefits) associated with these backlogs. On the demand side, ships
are long-lived capital investments, and consumers consider expectations about future
states of the world (including shipping demand and the evolution of shipping fleets)
before making purchases, much like the dynamic demand model used in Goettler
and Gordon (2011) for PC microprocessors, or Gowrisankaran and Rysman (2012)’s
study of the digital camera market.
The basic empirical strategy is to estimate cost structures in this industry before
and after 2006, when China identified shipbuilding as a “strategic industry” in need
of “special oversight and support.” Aggregate statistics show a large change after
2006, with a large amount of entry into the sector in China and a significant increase
in Chinese market share. Variation in the cost structure of Chinese firms before and
after 2006 is inferred to be the result of state-sponsored subsidies. The key identifying assumption, as in Ryan (2012), is that the subsidy policy was an unforeseen,
permanent, and immediate change.

4 Empirical applications

The estimation method is a variant of the Hotz-Miller two-step method. A technical innovation in the paper is the use of sparse approximation techniques from the
machine learning literature, namely, LASSO, to allow for a very large state space.
There is a large number of state variables in firms’ decision problem, such as the age
distribution of the current fleet and the backlog of different shipyards. Doing a basis
function approximation to the value function requires one to consider interactions between state variables, which could yield a basis with thousands of components. This
makes a dimension reduction technique very attractive, and this particular paper uses
LASSO to do so, even if the combination of value function iteration and LASSO
is not well understood to our knowledge, with the closest papers in the economics
literature being Arcidiacono et al. (2016).
The primary finding of the paper is that Chinese costs declined 13 to 20 percent,
or 1.5 to 4.5 billion dollars, after 2006. She does not find similar declines for firms in
other countries, which lends credence to the assumption that subsidies were behind
the shift. What is important here is not only the direction of the change in subsidies—
these could be read off in part from the change in the Chinese market share over
time—but the magnitudes that are implied. With estimates of the cost structure in
hand, she performs two primary counterfactuals. In the first, she removes all subsidies and simulates the resulting equilibrium. In the second, she removes investment
subsidies but keeps cost subsidies; this helps us understand the relative importance
of the two subsidies. Without any subsidies, she finds that the Chinese shipbuilding industry’s market size would be half as large. The primary beneficiary would be
Japan. Market prices would be higher as the subsidies shifted out the supply curve.
The customers of this industry, oceangoing shippers, gained about 400 million dollars
in surplus as a result of lower prices. These gains are relatively minor compared to
the estimated four billion dollar cost of the subsidies. Finally, she estimates a significant allocative inefficiency as production shifted from low-cost Japanese shipyards
to higher-cost Chinese firms. She does find evidence that there is significant learningby-doing in this industry, which is often a stated rationale for subsidies.

4.6 Retail
In this section we discuss a variety of papers in the retail trade sector. In many ways,
this part of the economy is suited for the type of cross-market identification of the
Bresnahan and Reiss (1991) approach, as retail is a non-tradeable sector, so interactions between firms really are local.

4.6.1 Economies of density and cannibalization
Walmart, the retail giant, started with a single store in Bentonville, Arkansas in 1962.
It has since grown to over 3000 stores in the United States. A startling fact is that
Walmart always opened new stores near old ones; it never jumped to a distant location and then filled in the markets in between. Holmes (2011) studies these patterns,
posing and estimating a model of store location that accounts for two important countervailing economic forces: on one hand, placing a new store near an old one can

309

310

CHAPTER 4 Dynamic games in empirical industrial organization

lead to cannibalization of sales from the old store. On the other hand, Walmart experiences economies of density due to the use of regional distribution centers where
large stocks of items are kept. Keeping stores close to distribution centers cuts down
on trucking costs and speeds up restocking times. Using data on store-level sales,
Holmes is able to estimate a significant negative cannibalization effect, while he uses
a profit-maximizing revealed preference argument to bound the benefits of economies
of density. The basic strategy is to perturb the sequence of store openings; under the
assumption that the observed policy (which stores to open where and in which order)
is optimal, alternative sequences should generate lower profits. Holmes finds that
economies of density are significantly positive: locating a store one mile closer to a
distribution center reduces annual costs by approximately $3500. Given the scale of
Walmart’s operations across the country, these economies of density play a key role
in Walmart’s successful business model.
Holmes focuses on the decision about where and when to open two types of
stores: regular stores that sell general merchandise and supercenters that also sell
groceries. He takes other choices, like how many stores to open and analogous
decisions about distribution centers, as given. There are L possible locations for
these stores, consisting of all the census blocks in the US (approximately, 11 mils ∈ {0, 1} be the indicator of the event “Walmart has a store
lion locations). Let at
s :=
of type s ∈ {regular, supercenter} in location  at year t,” and let a t ≡ {at
1, 2, ..., L; s = regular, supercenter} be the vector describing the map of Walmart’s
stores in the US at period t. The heart of the analysis is the present discounted profits
associated with a sequence, (a 1 , a 2 , ..., a t , ...) of store openings45 :
0
/
L
∞


 s

t−1
s
s
s
s
(44)
(ρt β)
at Rt − V Ct − ft − τ dt .
max
(a 1 ,a 2 ,...,a t ,...)

t=1

s

=1

s , variable costs, V C s ,
Profits of a store type s in location  consist of revenue, Rt
t
s
exogenous fixed costs, ft , and economies of density, τ djs t , where d is the distance to the nearest distribution center. The discount factor has two terms in it: the
usual intertemporal discount rate, β, and an additional term, ρt , that captures the
s are obtained from
fact that per-store revenues are growing over time. Revenues Rt
the estimation of a nested logit consumer demand model using revenue data from
Walmart stores in 2005. This demand system captures cannibalization between Wals are obtained using data labor costs, land value, and
mart’s stores. Variable costs V Ct
price-cost margins at the local level, and calibrating some parameters. The exogenous
fixed cost fts depends on population density in location , mt , according to function,
fts = ω0 + ω1 ln(mt ) + ω2 [ln(mt )]2 . This feature of the model contributes to explain Walmart’s propensity to open stores in locations with low population density.

45 In this paper, Walmart’s store openings are assumed irreversible. Closing a store is not possible. It

has not been until recently that Walmart started closing stores. Note that the irreversibility of the entry
decisions implies that we cannot use the finite dependence properties described in Section 3.3.4 to derive
relatively simple optimality conditions for the estimation of the model.

4 Empirical applications

The dynamic structural model is used to estimate fixed cost parameters (ω0 , ω1 , ω2 ),
and economies of density parameter τ .
Using the inequality average approach from Pakes et al. (2015) and Bajari et al.
(2007), Holmes considers deviations from Walmart’s observed behavior that consist
of pairwise resequencing in the opening dates of two stores. For instance, if store
number 1 actually opened in 1962 and store number 2 opened in 1964, a pairwise
resequencing would be to open store number 2 in 1962, store number 1 in 1964,
leaving everything else the same. This is a clever strategy, because Holmes assumes
that outside of those swaps, the entire future sequence remains constant. This implies
that those future streams of profits difference out, leading to a clean, simple estimator.
He considers three broad types of swaps: density-decreasing swaps where he switches
the order of an early store located close to a distribution center with a later store that
is located farther away; density-increasing swaps which move in the other direction;
and population-density swaps which hold density constant but changes the sequence
of stores that face different population densities. The target of the first two types of
deviations is the economies of density parameter τ , and the target of the third type of
deviation is the vector of ω parameters in the fixed cost.
Holmes finds a tight bound around $3500 per mile as the cost savings of locating
closer to a distribution center. A back-of-the-envelope calculation suggests that this is
about four times as much as would be implied by trucking costs alone; the remainder
could be interpreted as the benefits of increased flexibility to respond to demand
shocks.
This moment inequality approach to deal with the complexity of dynamic choice
models is pathbreaking, which makes it surprising that this approach has not really
been picked up in subsequent work. One issue is that in models with competing
agents, such as Jia (2008)’s analysis of the entry decision of Walmart in competition with Kmart (but a static analysis), one cannot simply look at a deviation without
thinking through how rivals will react to these deviations, both in the current period and in the future. This makes computing deviations substantially more difficult.
Moreover, there are relatively few instances where the researcher is interested in payoff parameters per se, without needing to work through their implications on firm
behavior.

4.6.2 Chains
Hollenbeck (2017) investigates the role that demand-side factors may play in firms
organizing their production into chains—defined in the paper as “any business that
operates multiple outlets offering similar goods or services under the same banner”—in the context of the Texas hotel industry, the same industry considered by
Suzuki (2013). On the supply side, firms may form chains to exploit economies of
scale and scope. On the demand side, consumers may view chain affiliation as a form
of quality signaling in a market for experience goods; rather than take their chances
with a single-location motel in west Texas they may decide to go to the nearest Motel
6. Of course, this benefit is not free, as firms have to pay affiliation fees to join chains,

311

312

CHAPTER 4 Dynamic games in empirical industrial organization

so in equilibrium not all firms will join a chain; furthermore, the decision to affiliate
may also be a function of market competition and other market-specific factors.
The decision to build a hotel of a certain quality and whether to associate with a
chain are both dynamic decisions: significant irreversible costs are incurred today for
the promise of higher returns in the long run while accounting for the strategic responses of rivals. The Texas hotel industry provides a nice environment for studying
this question: one can reasonably partition the (enormous) state of Texas into a significant number of discrete markets that do not directly compete with each other, à la
Bresnahan (1989); the state of Texas collects a hotel occupancy tax, which means that
high-quality revenue data is available for every establishment in the state; and AAA
publishes information about the quality (e.g. number of stars) and characteristics of
each hotel, including chain affiliation.
In a first step, Hollenbeck uses this information to build a revenue model as a
function of market characteristics, market structure, and chain affiliation. He finds
that chain affiliation is associated with a 27 percent premium in revenue per available
room. This estimate is slightly lower if estimated from a subset of hotels that switched
affiliation during the sample period. In a second step, he recovers costs associated
with running a hotel as an independent versus as having a chain affiliation. This
paper is one of the very first to use the methods of Arcidiacono and Miller (2011)
to estimate dynamic parameters in the presence of possible correlated unobservables.
Hollenbeck finds that higher-quality hotels have higher costs, but, significantly, chain
hotels do not have a cost advantage over independents. Entry costs are estimated to
be higher for chain firms, but the difference is on the order of the chain affiliation fee.
Accounting for the unobserved heterogeneity is critical to these findings, as failing
to do so significantly biases the effect of being a chain on costs. He documents a
declining chain premium over time, which is consistent with the idea that the increase
in online information about the quality of hotels is substituting for the signaling effect
of chain affiliation, further developed in Hollenbeck (2018).
This paper highlights a number of appealing features of the setting. First, it is
close to the ideal data set discussed previously: there are a small number of firms
competing in large number of distinct markets; there is high quality data on their
product offerings and revenues, partially driven by the fact that a tax authority collects
and reports the data; the technology in the industry is relatively simple and slow
moving; and finally, the set of dynamic parameters of interest are both relatively small
and, perhaps most importantly, transparently and directly connected to moments of
the data.

4.6.3 Unobserved heterogeneity and entry in retail
Igami and Yang (2016) also examine the role of unobserved heterogeneity in dynamic
models using the empirical setting of hamburger chains in Canada. Firms appear to
prefer to locate in places with many other competitors. The authors argue that this is
due to unobserved geographic heterogeneity and not positive spillover effects from
being close to competitors, such as higher consumer traffic. Entry and exit in this
setting happens at the level of openings and closings of chain stores, more than entire

4 Empirical applications

firms entering or leaving the industry. As in Holmes (2011), there is a cannibalization concern that opening an additional outlet harms sales at extant stores. On the
other hand, there may be preemption motives due to the threat of competitor entry.
Hamburger chains are a good setting to study these incentives, as there are both many
chains and stores that compete in local markets. This generates lots of variation for
empirical analysis. Additionally, chains compete primarily on the entry/exit margin
instead of prices or product variety, as those are often uniform across chains in a
given region for marketing reasons.
The empirical approach in this paper mixes together three separate approaches.
First, they use a nonparametric finite mixture method from Kasahara and Shimotsu
(2009) to recover a minimum number of market types, which vary in their profitability
in a way that is not captured by observable variables. Second, they use the estimation
technique of Arcidiacono and Miller (2011) to obtain firm entry and exit strategies
conditional on those market types. Finally, they follow Bajari et al. (2007) and use
forward simulation to recover firm profits and the cost of entry. Linking together the
number of types from the first step with the latter two estimation steps is a significant
methodological innovation. The primary takeaway from this paper is to highlight the
empirical bias of ignoring unobserved heterogeneity and proposing methodology to
handle it.

4.6.4 Effect of Walmart on rival grocers
A complementary paper to the previous two works is Arcidiacono et al. (2016), adapting continuous time methods to develop a model of retail competition, specifically
investigating the effect of Walmart’s entry on the retail grocery market. In contrast to
Holmes’ work, this paper considers the entry decisions of Walmart and seven competing supermarket chains along with a fringe of dozens of single-store competitors.
Markets are characterized by population levels and growth rates and are allowed to
have unobserved heterogeneity. This specification generates a very high-dimension
state space—up to 157 million states across 205 markets. They adapt continuous
time methods to deal with the problem of computing equilibria in a such a large
state space. In contrast to the general merchandising retail sector, they estimate that
Walmart’s primary effect was on other grocery chains rather than on independent
grocers—in fact, they estimate that independents actually benefit from Walmart’s
entry via a change in product market competition. As with Igami and Yang (2016),
unobserved heterogeneity is a key input to obtaining unbiased estimates of Walmart’s
effect on competitors; without it, independent grocery stores would have been uniformly worse off after Walmart’s entry.

4.6.5 Exit in declining industries
Takahashi (2015) looks at strategic exit when demand is declining. Strategic delay
may lead to suboptimal outcomes, as firms have incentives to free ride on the capacity reductions (or at the extreme, exit) of their competitors, as studied at a theoretical
level by Ghemawat and Nalebuff (1985) or Fudenberg and Tirole (1986). In the presence of uncertainty, there is also a real option value of waiting for more information

313

314

CHAPTER 4 Dynamic games in empirical industrial organization

before making an irreversible decision to exit. Takahashi studies the US movie theater industry in the 1950s, which was facing a combination of a long-run decline in
demand due to the increased adoption of home televisions and a large initial stock
of theaters. The author compares the profits that firms earn in the observed equilibrium against two counterfactuals. In the first, firms are non-strategic and exit when
operating profits are equal to fixed costs (“coordination benchmark”). The difference
in profits between this outcome and the observed data is interpreted as the cost of
strategic behavior. In the second counterfactual, firms exit in a coordinated fashion
to maximize total industry profits (“regulator benchmark”). The difference in profits
between this counterfactual and the coordination benchmark is interpreted as the cost
of oligopolistic competition. He finds that the delay in exit from strategic interactions
is 2.7 years on average. Less than four percent of that delay is due to strategic behavior, while 96 percent is due to oligopolistic competition. This implies a loss of a little
less than five percent of optimal profits in the median market.
Compared to much of the dynamic games literature, this paper has some unique
features. The first is that the model is a modified version of Fudenberg and Tirole
(1986), who provide a theory of exit in duopoly with incomplete information. The
model is in continuous time. Theaters are endowed with a time-invariant fixed cost
of operation but do not know the costs of their competitors (although they know
the common distribution that generates fixed costs for all players). This generates a
strategic motive to delay exit, as it is possible that some of their competitors will exit
instead and residual profits increase. This is balanced against the cost of delay, as
revenues are declining over time. In the unique equilibrium of this game, firms exit
in the order of their fixed costs from high to low. Another advantage of this approach
is that the computational cost of finding the unique equilibrium is low, so Takahashi
can utilize a full-solution estimation approach. Another difference from much of the
literature is that the focus is on exit alone; a benefit of this approach is that the set
of potential exiters in each period is observed, as opposed to the ad hoc modeling
assumptions about the pool of potential entrants that is typically required in other
settings. Indeed, in general it is easier to model counterfactuals with exit rather than
entry for this reason.

4.6.6 Repositioning
Ellickson et al. (2012) consider repositioning decisions of retail firms. As they highlight, the firm’s choice of where to position itself in the market (e.g., which segments
to compete in, how it brands itself, or which pricing strategy it uses) is a dynamic
decision with costs that may exceed the costs associated with entry, investment, or
exit in a given market. For example, if McDonald’s decided to become a chain of
upscale French bistros, it would face significant repositioning costs in moving away
from its current branding and business practices as a family-friendly fast food restaurant. Those costs are likely larger, and therefore more strategically important, than
decisions relating to entry or exit in marginal markets. Furthermore, these costs are
also likely much higher for existing incumbents than new entrants, who do not have
to overcome existing brand capital when deciding how to position themselves. El-

4 Empirical applications

lickson et al. (2012) focus on supermarket pricing strategy, specifically the choice
between relatively static “everyday low prices” (EDLP) and “promotional pricing”
where prices vary periodically due to discounts. The authors leverage the entrance
of Walmart Supercenters, which follow the EDLP strategy, as a shock event to local market structure. The authors estimate the payoffs associated with each possible
combination of own pricing format and competitors’ pricing formats, and use the
gains/losses associated with switching pricing strategies to infer repositioning costs.
They find that repositioning costs are both large and asymmetric among formats.
Moving to promotional pricing from EDLP is associated with a $2.3 million cost,
while the reverse is estimated at about six times as much. Conditioning on competitive conditions is also important, as the presence or absence of Walmart is a
significant driver of profitability across and within pricing strategies.
The specification of revenue and cost functions includes dummy variables for
whether the chain focuses on EDLP or promotional pricing. However, the model assumes that the decision of pricing format is market-specific and there are not spillover
effects across markets. This seems a rough way of capturing economies of scale or
scope in the choice of pricing format between stores of the same chain. This restriction is imposed to avoid having to solve for the equilibrium of a dynamic game over
all the markets, would be computationally very challenging. The authors also mention the need to model unobserved heterogeneity more systematically; the literature
is finally making strides in that direction almost a decade after this paper was written.

4.6.7 Advertising
Dubé et al. (2005) is an interesting twist on the standard empirical approach taken
by most papers. They assess the question of whether “pulsing” in advertising, that
is, periods of promotions followed by periods of regular pricing, can be sustained
in a Markov-perfect equilibrium. Their goal is not to estimate structural parameters
and perform a counterfactual, per se, but rather to, one, illustrate that such a strategy
is feasibly profitable, and two, illustrate some conditions on the demand-advertising
relationship that are necessary for that profitability. This echos the goal of Benkard
(2004) or Sweeting et al. (2020) in showing the pricing consequences of a dynamic
model with learning by doing and limit pricing. The study these questions, the authors
look at the market of frozen entrees, which is an advertising intensive industry and
studied in Sutton (1991), for example.
The paper proceeds in two distinct steps. First, using a long panel of consumer
purchases, they estimate demand for frozen entrees as a function of price and advertising. The length and richness of the panel allows them to estimate market-specific
fixed effects, which helps dealing with issues of unobserved heterogeneity, and also
estimate the nonlinear relationship between advertising and demand. Critically, they
find that there is a threshold of advertising that is necessary for a demand response to
advertising. Below that threshold the impact is negligible; above the threshold, they
find persistent effects.
Given some combination of convexity in advertising costs or concavity in advertising’s marginal return, the existence of the minimum threshold suggests that

315

316

CHAPTER 4 Dynamic games in empirical industrial organization

“pulsing” can be an optimal strategy: firms engage in relatively high advertising first
to build up brand equity, and once declining marginal returns set in, they then find it
optimal to not engage in any advertising at all until that brand equity depreciates to a
sufficiently low level. To test this conjecture, they take the estimated demand model
and calibrated costs using accounting data, and plug them in a dynamic game of pricing and advertising. As the authors state, their objective is not to obtain an in-sample
fit of any particular time series of prices, but rather to see if it is possible to generate
a pattern of prices. The answer to that question is positive, and they show that the
manufacturers in their equilibrium alternate between periods of intensive advertising
and periods without any. They also show that this pattern crucially depends on the
non-convexity of the advertising return function; without the threshold, firms do not
exhibit pulsing behavior.
While this paper has a minimum of estimated dynamics, all of which come
through the demand side via the advertising state variable, and supply-side parameters are minimal and calibrated to accounting cost data, it is useful to point out a
major strength of their approach: the paper is transparently clear about how nonconvexities in demand lead to differences in dynamic outcomes. While the frontier in
the dynamic games literature has moved forward, with increasing demands for better
identification, higher modeling complexity, and broader scope of counterfactuals, this
paper serves as a refreshing counterpoint that simple models with good data can also
be useful at elucidating the broader relationship between primitives and equilibrium
outcomes.

4.7 Uncertainty and firms’ investment decisions
The role of uncertainty on investment, and particularly the option value of waiting,
is a fundamental application of dynamic models in economics (Dixit and Pindyck,
1994). This is a critical channel of public policies aimed at business cycles, since
variation in uncertainty not only changes firms’ investment choices but the entire
relationship between investment and the underlying state space. Much of the recent
work in industrial organization has looked into the role of uncertainty in the firm’s
investment choices, both in oligopoly and competitive contexts.

4.7.1 Firm investment under uncertainty
Bloom (2009) provides a simple and helpful empirical framework to study the impact
of uncertainty on firms’ investment. This framework has been quite influential in
recent empirical work in IO and macroeconomics. Firms have a Cobb-Douglas sales
generating production function, qit = f (ωit , kit , it ) = exp{ωit } kitαk αit , where kit is
capital stock, it is labor, and ωit is the logarithm of the firm’s total factor productivity
(TFP) which is a composite of firm level efficiency and demand shocks. The process
for the evolution of ωit is given by a heteroscedastic random walk:


ωit = ωi,t−1 + ln 1 + σi,t−1 uit ,

(45)

4 Empirical applications

where σi,t−1 uit is a random shock, with uit i.i.d. standard normal, and σi,t−1 > 0
represents the variance of the shock to productivity. Uncertainty is measured by σt−1 ,
and it varies over time according to a Markov chain with two points of support, σL or
σH , and a transition matrix Fσ . In this framework, the term uncertainty shock means
that σt−1 has shifted from σL or σH .
In the absence of intertemporal concerns, a (mean preserving) change in uncertainty would not be particularly interesting, as it would simply affect the variance of
investment but not its mean or its intertemporal allocation. Irreversibility, sunk costs,
adjustment costs, or time-to-build introduce the intertemporal linkages that can generate rich and interesting effects of uncertainty. In Bloom’s model, these dynamic or
intertemporal concerns are introduced through adjustment costs in capital and labor
inputs. For instance, Bloom considers the following specification for capital adjustment costs46 :
AC(ωit , kit , it , ait ) = 1{ait > 0} ait − (1 − θr )1{ait < 0} ait
% (2
ait
+ θq 1{ait = 0} f (ωit , kit , it ) + θk kit
,
kit

(46)

where ait is capital investment, and θr , θq , and θk are parameters: θr represents the
cost of reselling capital; θq captures adjustment costs that do not depend on the investment amount but are proportional to the amount of sales (e.g., because stoppage
of the production process to install or disinstall capital equipment); and θk captures a
quadratic adjustment cost.
Bloom (2009) estimates the adjustment costs parameters using GMM, by matching moments on the variability of investment to identify the quadratic cost θk , the
frequency of zero investment to get parameter θq , and the asymmetry around zero in
the distribution of investment to get θr . In the estimated model, uncertainty shocks—
changes from σL to σH —alter the firm’s investment policy function. In particular,
they increase the size of the inaction region, the states where firms decide neither to
invest nor to sell capital. This has an effect on aggregate productivity because it slows
the reallocation of factors to more productive firms.
Asker et al. (2014) study how differences in the magnitude of productivity shocks
across industries and countries change investment decisions and the alignment of productivity and investment. Echoing the findings in Bloom (2009), greater uncertainty
on productivity, as measured by σc (where c indicates a country or country/industry), leads to lower alignment between productivity and capital. Through the lens of
Hsieh and Klenow (2009)’s model, this may appear as more misallocation in capital, but in fact, in this very model there is no inefficiency whatsoever. Furthermore,
differences in measured σc explain most of the variation in measured misallocation
between countries.
46 See also Cooper and Haltiwanger (2006) for a similar specification of capital adjustment costs function

and for a structural estimation of the parameters in this function using firm panel data.

317

318

CHAPTER 4 Dynamic games in empirical industrial organization

4.7.2 Uncertainty and oil drilling in Texas
Kellogg (2014) studies the impact of uncertainty on drilling decisions of oil producers in Texas. Drilling is an interesting investment decision: it is a one-time decision
(irreversible), and it is mainly a binary decision as the intensive margin of drilling
is mainly determined by geological factors. As such, it fits well the “option value
of waiting” effect of uncertainty (Dixit and Pindyck, 1994) better than the continuous investment decisions considered in Section 4.7.1 above, where uncertainty can
either raise or lower investment levels such as described by Caballero and Pindyck
(1996). Moreover, while Bloom (2009) and Asker et al. (2014) are concerned with
uncertainty in productivity shocks, the main source of uncertainty in the oil industry
revolves around the price of oil. In this context, there are well developed financial
tools to measure uncertainty, either by looking at the gap between futures prices and
option prices for oil, or by looking at daily changes in oil prices and backing out
implied volatility through a GARCH model.
Using very detailed data on drilling activity in Texas, Kellogg (2014) finds that
firms do pull back on drilling activity when uncertainty goes up. Moreover, this response is more closely linked to measures of uncertainty that come from futures and
option prices which are forward looking, rather than measures of volatility based
on past changes in the price of oil. This is an important finding, as the normative
implications of uncertainty have been well studied theoretically, but the positive effects of uncertainty—i.e., how does uncertainty empirically shape firm’s decision
making—are much less studied due to issues of how to measure uncertainty with
either realization of shocks or more direct elicitation of expectations.

4.7.3 Uncertainty in shipping
Other papers that look into the role of uncertainty in firms’ investment decisions
are Kalouptsidi (2014) in the bulk shipping industry and Jeon (2020) in the container
shipping industry. These industries involve shipping of commodities across the globe,
and thus, respond stronger to changes in global economic activity, such as the great
recession of 2008. Moreover, ships are long-lived assets that take several years to
build, so there is a natural delay in the response of the industry to demand shocks.
The main difference between these two shipping sectors is that bulk shipping is a
relatively unconcentrated industry where shipments are made on the spot, while container shipping is more concentrated and most of the routes are fixed. This leads to
very different empirical approaches.
Firms in the bulk shipping industry ship bulk commodities, such as coal and
wheat, that occupy the entire hold of a ship. Commodity prices fluctuate wildly over
the business cycle, and these are passed through into large changes in shipping rates.
The typical ship lasts 20 to 30 years, after which they are scrapped for recycled steel.
Ships take a year or more to build, and shipyards have limited capacity, so they maintain order books. This means that the time from ordering a ship until delivery depends
critically on the backlog at the shipyard, which is itself endogenous.

4 Empirical applications

In Kalouptsidi’s model, a firm is a shipowner (indexed by i) that owns only one
ship, and a ship is characterized by its age.47 Let kit ∈ {0, 1, ..., K} be the age at
period t of the ship owned by firm i. The state of the industry at period t has three
components, xt = (nt , bt , dt ): nt = (n0t , n1t , ..., nKt ) is the vector with the age distribution of all the ships active in the industry, where nkt is the number of ships with
age k at period t; bt = (b1t , n2t , ..., nT t ) is the vector with the backlog of orders at
shipyards, where bst is the number of ships to be delivered at period t + s; and dt is
the aggregate demand of shipping services, that follows an exogenous Markov process. The transition of the state variables in nt and bt is quite straightforward. For
instance, the number of ships with age k > 0 at t + 1 is equal to the number of ships
with age k − 1 at period t minus the number of ships that are scrapped. Likewise,
backlog state variables evolve according to a simple rule.
At a given period (quarter), a firm can be an incumbent or a potential entrant,
depending on whether it owns a ship or not. Every period, incumbents decide whether
to scrap their ships or continue operating, and potential entrants decide to enter (i.e.,
order a ship) or not. The scrap value of a ship is φit , and it is private information of the
firm and i.i.d. over time and firms. The Bellman equation describing an incumbent’s
decision problem is:
V (kit , xt ) = π(kit , nt , dt ) + β Eφ (max {φit , E [V (kit + 1, xt+1 ) | xt ]} )

(47)

A potential entrant chooses to order a new ship if the expected value after entry is
greater than entry costs, which are represented by function κ(nt ). Note that entry
costs depend on the state of the market. Importantly, there is time to build, that also
evolves endogenously as a function of the backlog vector. More specifically, the order
of a ship at period t is delivered after T (bt ) periods. Accordingly, a potential entrant
decides to enter at period t if the following condition holds:
,
+
β T (bt ) E V (0, xt+T (bt ) ) | xt > κ(nt ),

(48)

where the expression in the left hand side is the expected discounted value of an
incumbent with a new ship T (bt ) periods in the future.
Kalouptsidi solves this dynamic game by looking at a quasi competitive version of
the model, more specifically, by invoking the arguments from Weintraub et al. (2008)
on oblivious equilibrium. For the estimation of the model, the author exploits in an
ingenious way information from the resale market for ships. For bulk shipping, there
is good information on transaction prices in the resale market of ships. Let pk,t be the
transaction price of a ship of age k at period t. Under the assumption of perfect competition, no transaction costs, and no asymmetric information in the resale market,
we have that pk,t = V (k, xt ), such that transaction prices provide direct information
on the realized values of function V (.). Kalouptsidi uses data on transaction prices to

47 The model does not allow for firms with multiple ships.

319

320

CHAPTER 4 Dynamic games in empirical industrial organization

estimate the whole value function based on the regression equation:
pk,t = V (k, xt ) + εk,t

(49)

for every k = 1, 2, ..., K and sample period t. This is a nonparametric regression
model where the dimension of the vector of explanatory variables xt is extremely
large (94 variables), and the sample includes only a few hundred observations (the
number of ages times the number of quarters in the sample). Therefore, this nonparametric regression is subject to a huge curse of dimensionality problem. To deal
with this issue, Kalouptsidi combines aggregation restrictions on the vector of state
variables xt and machine learning techniques such as clustering and LASSO.48
(k, xt ) and data on entry and exit decisions, KalouptGiven the estimated values V
sidi estimates the distribution of scrappage values φit , the entry cost function κ(nt ),
and the time to build function κ(bt ). Then, she looks into the impact of time to build
and its endogeneity through backlog. In comparison to a fixed time to build, backlogs make time to build longer during booms, and shorter during downturns. Indeed,
simulating the evolution of the industry, endogenous backlogs lower the volatility
of investment by 45 percent compared to constant time to build. In addition, time
to build slows the entry response to demand shocks, and leads to a fleet that is 15
percent bigger. In much of the literature on entry in industrial organization, a partial
equilibrium stance is taken on entry costs: they are a fixed parameter or distribution.
Kalouptsidi (2014) is a nice example of what changes once a more general equilibrium view on the supply of entrants is considered.
Jeon (2020) studies the role of demand uncertainty in the cyclical investment
fluctuations in the container shipping industry. This industry is more concentrated
than bulk shipping and some companies own many vessels, but it is also subject to
large swings in demand. A distinguishing feature of Jeon’s paper within this literature
is that firms’ uncertainty is not limited to future unpredictable demand shocks but
they have also uncertainty about demand parameters. Firms learn over time about
these parameters using a form of adaptive learning.
The state variables related to firm i are the aggregate capacity of all its ships,
kit , and the backlog of the firm’s orders of new ships, bit , which is also measured
in capacity units. The state of the industry is given by xt = (kit , bit , ztA , ztB : i ∈ I),
where ztA and ztB represent the state of demand in the route from Europe to Asia (the
most traveled shipping route) and elsewhere, respectively. Jeon assumes that each of
these demand variables follows an exogenous AR(1) process. For route s ∈ {A, B}:
s
zts = ρ0s + ρ1s zt−1
+ σ s ωts ,

(50)

48 Though Kalouptsidi’s approach to estimate the value function using transaction prices is quite inge-

nious, it is tricky to apply to many other industries. Most resale markets of capital are characterized by
substantial transaction costs and asymmetric information. This is well known for cars and trucks, with
perhaps the exception of aircraft (Gavazza, 2011a,b). See for instance the evidence on aerospace plants
in Ramey and Shapiro (2001). These frictions in capital resale markets imply that there is not a straightforward relationship between transaction prices and firms’ values. Nevertheless, Kalouptsidi’s approach
could be applied using other measures of firm valuation, such as stock market values.

4 Empirical applications

where ωts is i.i.d. standard normal. As usual, firms have uncertainty about future realizations of ω shocks. Jeon considers that firms also have uncertainty about the parameters (ρ0s , ρ1s , σ s : s ∈ {A, B}) that govern the stochastic process of these variables.
Following the macroeconomics literature on agents’ learning (Evans and Honkapohja, 2012), Jeon assumes that firms in this industry use a form of adaptive learning to
update their beliefs about these parameters. A parameter λ, which controls the weight
that new data receives in the updating rule of beliefs, plays a key role in this learning mechanism. A higher value of λ increases the responsiveness of firm beliefs to a
downturn in demand.
Jeon (2020) assumes that the industry outcomes come from a moment based
Markov equilibrium (MME), as defined by Ifrach and Weintraub (2017), that we
have discussed in Section 2.4.3. This equilibrium concept aids in reducing the size
of the state space by focusing on moments of the distribution of firms. In this model,
without rational expectations, the parameters in firms’ beliefs and learning process
should be estimated together with the rest of the structural parameters in investment
costs and scrap values from the predictions of the dynamic game. That is, observed
firm behavior reveals not only firms’ “preferences” bu also their beliefs. Jeon estimates all these parameters using a full-solution method of simulated moments. The
estimates show that the weight in the learning process of a 10-year-old observation
relative to a current observation is 45%.
The author presents counterfactual experiments to evaluate the impact of uncertainty about demand parameters on the level, volatility, and pattern of firms’ investment. Removing uncertainty about demand parameters reduces aggregate investment
by 17% and its volatility by 22%, and reallocates investment across the demand cycle:
it reduces the positive response of investment during boom years. Interestingly, there
is also a very substantial impact on welfare, increasing producer surplus by 85%, but
having only a small negative impact on consumer surplus. This paper shows the potentially important impact on industry outcomes of sources of firm uncertainty which
have not been included in the most standard dynamic models of competition in IO.

4.8 Network competition in the airline industry
An airline’s network is the set of city-pairs that the airline connects via non-stop
flights. The choice of network structure is one of the most important strategic decisions of an airline. Indeed, one of the assumptions that is the most difficult to accept
in Berry (1992), is that entry decisions are solely about origin and destinations, rather
than the entire route network. Two network structures that have received particular
attention in studies of the airline industry are hub-and-spoke networks and pointto-point networks. In a hub-and-spoke network, an airline concentrates most of its
operations in one airport, called the hub. All other cities in the network (i.e., the
spokes) are connected to the hub by non-stop flights such that travelers between two
spoke cities must take a connecting flight to the hub. In contrast, in a point-to-point
network, all cities are connected with each other through nonstop flights. Like the
work of Holmes (2011) on Walmart’s distribution network, it is quite challenging to

321

322

CHAPTER 4 Dynamic games in empirical industrial organization

consider the entire network formation process, since the underlying set of networks
is the power set of all origin destination pairs. Moreover, while the field of network
economics is quite large, there is a paucity of work on strategic network formation.
Soon after deregulation of the US airline industry in 1978, most airline companies
adopted hub-and-spoke networks to organize their routes. Different hypotheses have
been suggested to explain airlines’ adoption of hub-and-spoke networks. According
to demand-side explanations, some travelers value the services associated with the
scale of operation of an airline in the hub airport, e.g., more convenient check-in
and landing facilities and higher flight frequencies. Cost-side explanations argue that
an airline can exploit economies of scale and scope by concentrating most of its
operation in a hub airport. Larger planes are cheaper to fly on a per-seat basis, and
airlines can exploit these economies of scale by seating in a single plane, flying it to
the hub city with passengers with different final destinations. The are also economies
of scope as part of the fixed costs of operating a route, such as maintenance and
labor costs that may be common across different routes in the same airport. Another
hypothesis that has been suggested to explain hub-and-spoke networks is that it can
be an effective strategy to deter the entry of competitors (Hendricks et al., 1997).
In a hub-and-spoke network, the profit function of an airline is supermodular with
respect to its entry decisions for different city-pairs. This complementarity implies
that a hub-and-spoke airline may be willing to operate non-stop flights for a city-pair
even when profits are negative because operating between that city-pair can generate
positive profits connected with other routes. Potential entrants are aware of this, and
therefore, it may deter entry.49
Despite the attractive features of the Hendricks et al. (1997) entry deterrence argument, there were no previous studies that empirically explore this entry deterrence
motive in airlines’ use of hub-and-spoke networks. Part of the reason for this lack of
empirical evidence is the absence of structural models of dynamic network competition that incorporate this hypothesis and that were flexible and realistic enough to be
estimated with actual data. This limitation in the literature motivated Aguirregabiria
and Ho (2012) to develop an estimable dynamic game of airline network competition
that incorporates the demand, cost, and strategic factors described above.
In their model, every quarter airline companies decide the city-pairs where they
operate non-stop flights and the fares for each route-product they serve. The structure
of the model follows Ericson and Pakes (1995): direct strategic interactions between
firms occur only through the effect of prices on demand; price competition is static;
and firms’ entry decisions in city-pairs is dynamic or forward looking and it affects
other firms’ profits only indirectly through its effect on equilibrium prices. While
static entry models such as Berry (1992) and Ciliberto and Tamer (2009) provide
49 This argument for entry deterrence does not suffer from several limitations that hinder other more

standard arguments of predatory conduct. In particular, it does not require a sacrifice on the part of the
incumbent (i.e., a reduction in current profits) that will be compensated for in the future only if competitors
do not enter the market. Furthermore, it is not subject to well-known criticisms of some arguments and
models of spatial entry deterrence (see Judd (1985)).

4 Empirical applications

measures of the effects of hubs on fixed operating costs, endogenizing the existence
of hubs and, more generally, the structure of airlines’ networks is important for multiple reasons. Treating hub size as a variable that is endogenously determined in the
equilibrium of the model is important for some predictions and counterfactual experiments using these structural models, such as the medium and long run effects of a
merger.
The model is estimated using data from the Airline Origin and Destination Survey
(DB1B) of the US Bureau of Transportation Statistics with information on quantities,
prices, and route entry and exit decisions for every airline company in the routes
between the 55 largest US cities, for a total of 1485 city-pairs.
Given the huge dimension of the state space in this network game, the authors
need to develop several methodological contributions for the estimation and for the
solution of an equilibrium in this model. They propose a method to reduce the dimension of the state space in dynamic games that extends to dynamic games the
inclusive values approach in Hendel and Nevo (2006), Nevo and Rossi (2008), or
Gowrisankaran and Rysman (2012). The main contribution of their approach to
model inclusive values is that they endogenize the transition probabilities of the inclusive values such that one can use the estimated model to make counterfactual
experiments that take into account how these transition probabilities depend on the
strategies of all the players, and therefore how they change in the counterfactual scenario. They also propose and implement a relatively simple homotopy method to deal
with multiple equilibria when making counterfactual experiments with the estimated
model.
Their empirical results show that an airline’s number of connections in an airport
has a statistically significant effect on consumer demand, unit costs, fixed operating
costs, and costs of starting a new route (“entry costs”). However, the economically
most substantial impact is on entry costs. Counterfactual experiments show that eliminating this effect on entry costs would reduce very substantially airlines’ propensity
to use hub-and-spoke networks. For some of the larger carriers, Hendricks et al.
(1997)’s strategic entry deterrence motive is the second most important factor to explain this network choice.

4.9 Dynamic matching
A newer strand of literature has looked into market equilibrium in industries characterized by search and matching frictions, primarily focused on the market for taxi
service in New York City. This literature is somewhat apart from the literature on
dynamic games that uses an Ericson and Pakes (1995) framework. Instead, these papers focus on dynamic competitive equilibrium in the tradition of Hopenhayn (1992).
However, intuitively, there should be a close correspondence between competitive
models and an oligopoly model with many firms, as has been exploited by Weintraub
et al. (2008) and Ifrach and Weintraub (2017). Moreover, these dynamic competitive models allow for substantially simpler computation, as well as more theoretical
clarity given that these models yield second welfare theorems.

323

324

CHAPTER 4 Dynamic games in empirical industrial organization

Buchholz (2018) looks into the equilibrium of the New York City taxicab market. In this environment, cabs are driving through the city looking for riders. The
friction that prevents matching between the two sides of the market is space: empty
cabs and riders are in different places. In addition, even if cabs and riders are in the
same neighborhood, they may not see each other. Some of this could simply be about
cabs and riders being on different street corners, which is a key friction modeled by
Frechette et al. (2019) in a similar study of the New York City taxi market. As well,
drivers and riders might simply have difficulty finding each other in front of Penn
Station.
A second inefficiency in this market is that fares are functions—pre-established
by the regulator—of distance, time, and a flag fall fee. This precludes, in particular, dealing with differences in demand at the origin and destination of a ride. For
instance, many people are looking for a ride from Queens to Manhattan on Friday
mornings, but not in the other direction. Thus, the social planner may want to charge
riders based on a richer set of characteristics, such as locational pricing, in this case
a higher Queens price than Manhattan price for the same trip.
Locations in the city are indexed by  ∈ {1, 2, ..., L}. Buchholz (2018) models the
state of location  at time t in terms of two variables: (i) the probability that a rider
shows up in this location, denoted λt , and where she wants to travel to, dt , which is
allowed to vary by time of day in a predictable manner; and (ii) the number of vacant
cabs in that location at time t denoted as vt . The latter state is the main endogenous
object in this market, and also needs to incorporate cabs in transit. For example, a
rider may be going to the airport, and this means that a vacant cab will show up in
45 minutes at LaGuardia airport. Given the number of riders and vacant cabs in a
location, a matching function m(λt , vt ) determines the number of riders finding a
vacant cab.
Every period, vacant taxi drivers choose in which location to search for riders.
They can decide to search in their current location or drive to another location in
the city where there may be more riders. To make this decision, they compare the
value of searching for riders at every location in the city. Solving this model via the
approach of Ericson and Pakes (1995) is clearly intractable given the thousands of
cabs in New York searching over dozens of neighborhoods at different times of day.
Instead, Buchholz (2018) uses a dynamic competitive model following Hopenhayn
(1992), which assumes that cabs are atomistic; i.e., small enough so that they do not
believe that their actions alter the equilibrium of the market. Moreover, as there are no
aggregate shocks in this model, one can solve it as if all agents have perfect foresight
over the evolution of the market over the day. This makes computation far easier as
it implies that one just needs to solve for the number of vacant cabs in each location
at every time of the day. It also avoids the issues around multiple equilibria in this
environment.
Buchholz (2018) uses his model to look at the effect of using more sophisticated
location based pricing, so charging prices based on origin and destination and time of
day, rather than simply metering by distance and time. He finds that the total number
of trips could be increased by 28 percent, and that welfare would increase around 8

4 Empirical applications

percent. This suggests that more complex pricing mechanisms could be useful in the
New York City taxi market.50
Brancaccio et al. (2020) develop a dynamic spatial equilibrium model of the interaction between world trade and oceanic transportation services. In this model,
forward-looking ship owners and exporters participate in a decentralized matching
process where exporters decide where to export and ship owners choose which ports
to move their vessels to. The authors estimate this model using detailed data on vessel movements, shipping prices, and trade flows. An interesting fact in this industry is
that prices differ substantially by the direction of travel. For instance, it is far cheaper
to ship cargo from China to Australia than the other way around, at least for bulk
shipments like coal. This means that the characterization of equilibrium in this type
of market needs to incorporate the directional flows of traffic across the globe. A contribution of this paper is to endogenize the shipping costs paid by exporters, as they
depend on the shipper’s decisions of what routes to take. The model provides a nice
tool to study the effects on shipping costs and the patterns of exports on interesting
worldwide economic events, such as the opening of the Arctic to shipping, or changes
in fuel prices.
While there is a sharp discontinuity between the theory models of dynamic
oligopoly of Ericson and Pakes (1995) versus the dynamic competitive ones of
Hopenhayn (1992), for much of the empirical work on industry dynamics, this boundary has started to blur. The computational approaches used for dynamic competitive equilibrium with aggregate shocks are quite similar to the MME and oblivious
equilibrium concepts of Ifrach and Weintraub (2017) and Weintraub et al. (2008).
Likewise, beyond correctly specifying the state space used by agents, there is not
much practical difference between CCP approaches applied to competitive versus
oligopolistic markets. Thus, we expect some convergence between the empirical literature on dynamic versus oligopolistic competition.

4.10 Natural resources
Huang and Smith (2014) investigate dynamics in a common pool setting, where an
exhaustible common resource is used by many independent agents. Exploitation of
a common pool resource can give rise to several externalities that interfere with efficient behavior: consumption by one agent reduces the stock of the resource for all
other agents, which can induce over-extraction of the good. Dynamically, this externality can also distort the optimal time pattern of resource use, shifting extraction
either too early or too late relative to the social planner. Finally, there is a static
externality that is caused by overcrowding during extraction. This congestion externality increases the costs of extraction, which may lead to lower surplus, allocative
50 In a related paper, Frechette et al. (2019) compare decentralized matching protocols—cabs picking up

hails on the street—versus a centralized dispatch protocol—Uber assigning cabs. This paper relies heavily
on the topography of New York City’s street grid to model how cabs travel to assess the efficiency gains
from a better dispatch algorithm.

325

326

CHAPTER 4 Dynamic games in empirical industrial organization

inefficiencies (e.g., the wrong firms extracting the resource), and, potentially, may
countervail the stock externality.
Huang and Smith examine these economic forces in the context of the North Carolina shrimp industry. There are several characteristics to this industry that make it
amenable to this analysis. First, the dynamics of the resource are well-understood.
The shrimp life cycle fits neatly into a year, and, importantly for modeling considerations, the species is able to reproduce at a sufficiently high rate, such that it is
reasonable to assume that the stock renews completely each calendar year. This implies that one can model the essential dynamics in a repeated finite-horizon model
that runs from January to December. This is relatively unusual in this literature, as
most settings are concerns with long-lived firms that are modeled as having an infinite
horizon. This also means that the model can be solved through backward induction,
which also guarantees a unique solution conditional on each stage game having a single equilibrium. The biological basis for stock dynamics also informs the functional
forms used in the structural analysis. There are also a number of nice weather-based
exogenous supply shifters; increased wind speeds and wave heights make the harvesting process more difficult and therefore are excellent instruments for shifting supply.
North Carolina is also a very small part of the globally-integrated shrimp industry,
which means prices for input and outputs can be taken as given. The data is also
unusually detailed, as the state collects information on every commercial shrimping
boat trip.
The model consists of N individual shrimp boats indexed by i; a state space which
includes the present stock of shrimp, input and output prices, and current weather
conditions; and transitions from state to state that depend on the present state vector
and actions of shrimp boats in the present period. Shrimp boats decide whether to go
fishing once a day. The one-period payoff function is:

α pt E(hit ) − zit β + εit (1) if ait = 1
(51)
πit (ai ) =
if ait = 1.
εit (0)
Expected revenue is the product of shrimp price, pt , and expected harvest, E(hit ).
The term zit β captures the cost of a trip, and zit is a vector of exogenous variables
such as the length of the vessel, wind speed, wave height, fuel price, an indicator
for weekend days, and the fish stock. Variables εit (0) and εit (1) are action-specific
idiosyncratic shocks which are unobservable to the researcher and are i.i.f. type I
extreme value, such that they generate the familiar logit choice probability when
integrated. The authors assume that harvest depends on whether conditions(wt ), the
stock of shrimp (st ), the total number of vessels on the water that day (nt ≡ N
i=1 ait ),
the vessel’s time invariant productivity (ηi ), and a productivity shock (uit ), according
to the following exponential function:
hit = st × exp{γ nt + wt + ηi + uit }.

(52)

The term γ nt captures the agglomeration (if γ > 0) or congestion (if γ < 0) externality, depending on the sign of γ .

4 Empirical applications

Huang and Smith allow for more complex transitions between states than other
dynamic settings. This is facilitated in part by exogeneity assumptions and the availability of high-frequency data. Price, wind speed, and wave heights are all modeled
as a vector AR(1) process, with wind speed and wave heights correlated. The price of
fuel is modeled as a function of the week of the year, and shrimp stocks are modeled
as a latent stochastic difference equation that comes from a biological model. Since
the actions today influence the state vector through the stock of shrimp, agents choose
the best action today given the strategies of their competitors and the choice-specific
continuation values.
The authors estimate the harvest production function as part of a first step, outside
the dynamic model. The estimate of the externality parameter γ implies that one unit
increase in the total number of vessels implies a 0.127% reduction in each vessel’s
harvest. Given that the average number of vessels per day is around 60, this parameter value implies a substantial congestion externality. The approach for the estimation
of the dynamic model is a mix of Aguirregabiria and Mira (2007) and Bajari et al.
(2007). The authors first estimate the transitions of exogenous state variables using
time series methods. They then estimate the conditional choice probability using a
logit that is saturated with state variables, their powers, and their interactions. In the
stage game, profits depend on the number of other vessels on the water. Conditional
on the equilibrium played in the data, one can integrate out the expected actions of all
other boats on that day using the conditional choice probability function estimated in
the prior step. The only remaining step is to compute continuation values to put into
the likelihood function. With policy functions in hand, one can use the forward simulation method from Bajari et al. (2007) to approximate the continuation value. Once
the continuation value is known, one can then maximize a pseudo log-likelihood.
The primary counterfactual evaluates the efficiency gains from using a centralized
vessel allocation policy, where a social planner, who internalizes the externalities in
this setting, decides how many (and which) shrimp boats will go fishing in a day. To
perform this counterfactual, the authors discretize the ending shrimp stock and work
backwards from the terminal date, solving the value function by filling in the optimal
social policy at each point in the state space as they go. Once the value function is
filled out for all points in the discretized state space, the social planner can pick the
path that delivers the highest surplus. The authors find that the observed equilibrium
shifts too much of the harvest early in the year, due to the extraction externality,
and this then translates to too little of the harvest happening later in the year, as
stocks would have been higher. There are too many vessels early on, and there is
also an allocative inefficiency as some of the boats are lower productivity vessels
that should not have been dispatched. Finally, they also examine the dynamics of
the industry when congestion is eliminated; they find that congestion actually has
a positive effect in equilibrium as it helps offset the extraction externality. This is a
particularly compelling counterfactual for the use of the dynamic model, as otherwise
one would incorrectly conclude in a static world that the congestion externality was
welfare reducing through its imposition of higher fishing costs.

327

328

CHAPTER 4 Dynamic games in empirical industrial organization

This paper has the flavor of both single-agent dynamics and the multi-agent tools
described above. There are many agents in this model, and their behavior only influences a particular agent through an aggregate quantity, which is the total number of
vessels on the water in a given day. In that sense, this paper presages some of the work
by Buchholz (2018) and others. It is also an example of a paper where the policy question of interest is directly estimable from the data—the authors do not compute any
counterfactual equilibria with strategic agents (the social planner is a single-agent
problem).51 Rather, the authors are able to simulate in-sample counterfactuals that
remain within the support of the observed state variables. Any counterfactuals that
change the agent’s profit incentives outside those bounds would necessitate solving
the entire equilibrium; the finite-horizon assumption makes this computationally feasible (if expensive), but one would have to address issues of multiple equilibria in the
stage game. The paper also highlights the incorporation of very rich data, with nontrivial dynamics, through their use as exogenous state variables. This contrasts with
our previous discussion that focused on the need to typically simplify the endogenous
dynamics as much as possible in order to facilitate estimation and simulation.

5 Concluding remarks
Over the last three decades, the work on dynamic oligopoly has moved from being a
primarily methods-oriented line of research towards fulfilling its promise as a central
tool in the empirical IO literature, paralleling transitions from theory to empirical implementation in demand estimation or vertical relationships. The models, methods,
and applications we have outlined in this chapter are critical to understanding questions at the heart of industrial organization. In many ways, vast progress has been
made. To an observer in the mid-1990s, the idea of a research agenda that delivered
realistic empirical dynamic oligopoly models that could account for heterogeneous
firms, complex state spaces evolving in response to both exogenous forces and endogenous strategic decisions, non-trivial dynamics on both supply and demand, and
complicated payoff structures may have seemed completely out of reach. Indeed, in
2006, Tim Bresnahan colorfully compared the chances of this endeavor to winning a
land war in Asia. Thirty years later, these are seen as difficult, but solvable, problems.
The literature has also started to deliver on its promise of quantifying the importance
of dynamics in a wide range of settings. Yet, there are many remaining challenges in
this literature, which we put into five different categories.
First, computation is still enormously difficult. Indeed, the state spaces and computational problems considered by the earliest papers in this literature, such as Pakes
and McGuire (1994) or Gowrisankaran (1999), are embarrassingly close to some of
the problems considered in the most recent papers in this survey. One might have

51 Ryan and Tucker (2012) is another example of where the counterfactuals are contained within the

support of the observed data.

5 Concluding remarks

thought that increasing computational power would eliminate this as an issue, at least
one that economists have to deal with, but this has not happened. One reason is that
increases in computing power are simply exhausted by making the models slightly
more complex. This leads both large delays in getting work published, as well as
severe restrictions on the size of the state and parameters spaces, which affect the
plausibility of this type of analysis.
When the CCP-inspired estimation papers, such as Bajari et al. (2007), Aguirregabiria and Mira (2007), or Pesendorfer and Schmidt-Dengler (2008) were coming
out, there was a hope that we had cracked the problem of computation, at least as
far as the issue of estimation was concerned. Indeed, there are many applications that
estimate parameters in models that have never been computed. However, there is a
certain hollowness to estimation of parameters without being able to draw out their
implications through a computed model, such as by running counterfactuals. Most of
the parameters estimated in dynamic models do not have stand-alone policy implications, and even those that do are better understood by putting their implications into
an equilibrium context.
Second, two decades of empirical work on dynamic oligopoly has revealed that
both the right data can be particularly hard to find and that there can be an enormous
disconnect between what the ideal empirical model asks for and what the data can
actually deliver. At a bare minimum, one needs detailed data on all the participants
in an industry, while a longitudinal panel spanning years or decades is even better.
To use CCP-based methods, one needs enough observations, by enough independent
agents, to estimate reduced-form policy functions describing agent behavior at all
possible states. Ideally, one has observations on a large number of firms; it is even
better if they are spread across independent markets. This makes using the CCP-based
approach difficult for modeling globally-integrated markets, such as those for semiconductors or hard drives. This is in contrast to the large datasets that are commonly
used for CCP-based papers in labor economics.52 Moreover, the relevant characteristics of firms need to be summarized into a parsimonious number of states, which
can often require some heroic modeling assumptions. As a result of these data and
specification challenges, many of the successful papers in this literature examine industries where institutional details of the industry generate data that is similar to that
considered in the original structural studies of entry in Bresnahan and Reiss (1990)
and Bresnahan and Reiss (1991) and where the essentially dynamics are interesting
and necessary without being too complex.
Third, many recent applications of dynamic games applying two-step methods to
estimate models with very large state spaces use very restrictive parametric specifications of reduced form CCP functions in the first step of the method. Recent work
in the econometrics literature using machine learning techniques to improve smallsample performance in high-dimensional settings may be useful in this context. For

52 See, for instance, Traiberman (2019); Ransom (2021); Llull (2018); Hincapié (2020) for recent papers

in labor using CCP’s, and in particular the large datasets employed in these analyses.

329

330

CHAPTER 4 Dynamic games in empirical industrial organization

example, Nekipelov et al. (2021) show their method can be applied to the first-stage
estimation in Bajari et al. (2007), flexibly estimating policy functions while also accounting for the fact that different equilibria may be played across different markets.
Further efforts to apply machine learning-based model selection techniques to identify the “best” specification of the reduced form CCP functions can be helpful in this
context. It is also important to consider that, if the goal is the precise estimation of
structural parameters in the second step, the “best” estimation of CCPs in the first
step is not the specification that provides the lowest standard errors of reduced form
parameters, and not even the one that provides the lowest mean square error in the
first step. Often in two-step semiparametric procedures the first step nonparametric
estimator is under-smoothed to deal with bias in the second stage parametric estimator (e.g. Abadie and Imbens (2011)). This is an exciting area for future research.
Fourth, the agenda of computational-based theory outlined in Pakes and McGuire
(1994) has not lead to a particularly well-organized body of work as to the theoretical
predictions of these models. Indeed, the researcher first computing the solution to a
dynamic game may have very little intuition of why the results end up the way they
do: John Asker has qualified this type of work of unpacking computational results on
dynamic games as “forensic”.
Fifth, multiplicity of equilibria remains an important challenge in empirical applications of dynamic games, especially in the implementation of counterfactual
experiments. Two-step methods partially circumvent this problem by conditioning on
the equilibrium played in the data, but one must either assume the same equilibrium is
played in all markets or lose precision by estimating policy functions independently.
In any case, this solution only applies to the estimation and not the computation of
counterfactuals. Besanko et al. (2010) introduce a homotopy path-following method
for tracing out some (but not necessarily all) of the equilibria in a dynamic game. In
a related paper, Besanko et al. (2014), use this homotopy method to trace out equilibria in a model of predation. They show that policy interventions not only change
the behavior of firms within an equilibrium, but may also change the set of equilibria.
Interpreting the difference between the two is crucial, but, at least for now, the tools
necessary to show this remain limited. For instance, these papers—in the context
of much more stylized models than those in empirical applications—reveal a correspondence between structural parameters and equilibrium outcomes that is chaotic,
discontinuous, and non-intuitive. Infinitesimal changes to parameters induce jumps
from single to many equilibria, with different comparative statics implications. Peering into such a Rorschach inkblot, one gets the impression that there are no robust
predictions for some important counterfactual experiments.
All of that said, we conclude on a note of optimism. As this chapter has outlined,
there is now a large body of empirical work looking at dynamic games that will inform future policy debates in economics. There has been an expansion in the types of
industries that are considered, moving us away from the Bresnahan-Reiss program of
looking at geographically isolated markets with a small number of relatively similar
competitors. Instead, recent work looks at industries with firms with complex characteristics, global integrated markets, and markets with large numbers of firms in them.

References

Furthermore, there is now a mature set of tools to both compute solutions to dynamic
oligopoly problems with large state spaces and many firms, and an even more developed set of estimation techniques for these settings that can incorporate differences in
beliefs or cross-market heterogeneity. Just as one could not have completely foreseen
all of the methodological advances in the field thirty years ago when the Markovperfect Nash equilibrium foundations were being constructed by Maskin, Pakes, and
Tirole, we are hopeful that the next wave of research in this area will successfully
address the outstanding problems in the dynamic games literature. In particular, the
field of machine learning is quickly evolving to handle problems with very large state
spaces which could further extend the purview of these methods to realistic analysis
of ever more complex and interesting markets.

References
Abadie, Alberto, Imbens, Guido W., 2011. Bias-corrected matching estimators for average
treatment effects. Journal of Business & Economic Statistics 29 (1), 1–11.
Abbring, Jaap, Daljord, Øystein, 2020. Identifying the discount factor in dynamic discrete
choice models. Quantitative Economics 11 (2), 471–501.
Abbring, Jaap H., Campbell, Jeffrey R., 2010. Last-in first-out oligopoly dynamics. Econometrica 78 (5), 1491–1527.
Ackerberg, Daniel, Chen, Xiaohong, Hahn, Jinyong, Liao, Zhipeng, 2014. Asymptotic efficiency of semiparametric two-step GMM. The Review of Economic Studies 81 (3),
919–943.
Aghion, P., Howitt, P., 1992. A model of growth through creative destruction. Econometrica 60
(2).
Aghion, Philippe, Bloom, Nick, Blundell, Richard, Griffith, Rachel, Howitt, Peter, 2005. Competition and innovation: an inverted-U relationship. The Quarterly Journal of Economics 120
(2), 701–728.
Aguirregabiria, Victor, 1999. The dynamics of markups and inventories in retailing firms. The
Review of Economic Studies 66 (2), 275–308.
Aguirregabiria, Victor, 2005. Nonparametric identification of behavioral responses to counterfactual policy interventions in dynamic discrete decision processes. Economics Letters 87
(3), 393–398.
Aguirregabiria, Victor, 2010. Another look at the identification of dynamic discrete decision
processes: an application to retirement behavior. Journal of Business & Economic Statistics 28 (2), 201–218.
Aguirregabiria, Victor, Alonso-Borrego, Cesar, 2014. Labor contracts and flexibility: evidence
from a labor market reform in Spain. Economic Inquiry 52 (2), 930–957.
Aguirregabiria, Victor, Ho, Chun-Yu, 2012. A dynamic oligopoly game of the US airline industry: estimation and policy experiments. Journal of Econometrics 168 (1), 156–173.
Aguirregabiria, Victor, Magesan, Arvind, 2013. Euler equations for the estimation of dynamic discrete choice structural models. In: Structural Econometric Models. In: Advances
in Econometrics, vol. 31. Emerald Group Publishing Limited, pp. 3–44.
Aguirregabiria, Victor, Magesan, Arvind, 2020. Identification and estimation of dynamic
games when players’ beliefs are not in equilibrium. The Review of Economic Studies 87
(2), 582–625.

331

332

CHAPTER 4 Dynamic games in empirical industrial organization

Aguirregabiria, Victor, Marcoux, Mathieu, 2021. Imposing equilibrium restrictions in the estimation of dynamic discrete games. Quantitative Economics 12 (4), 1223–1271.
Aguirregabiria, Victor, Mira, Pedro, 2002. Swapping the nested fixed point algorithm: a class
of estimators for discrete Markov decision models. Econometrica 70 (4), 1519–1543.
Aguirregabiria, Victor, Mira, Pedro, 2007. Sequential estimation of dynamic discrete games.
Econometrica 75 (1), 1–53.
Aguirregabiria, Victor, Mira, Pedro, 2019. Identification of games of incomplete information with multiple equilibria and unobserved heterogeneity. Quantitative Economics 10 (4),
1659–1701.
Aguirregabiria, Victor, Nevo, Aviv, 2013. Recent developments in empirical IO: dynamic
demand and dynamic games. In: Advances in Economics and Econometrics, Vol. 3,
pp. 53–122.
Aguirregabiria, Victor, Suzuki, Junichi, 2014. Identification and counterfactuals in dynamic
models of market entry and exit. Quantitative Marketing and Economics 12 (3), 267–304.
An, Yonghong, Hu, Yingyao, Xiao, Ruli, 2021. Dynamic decisions under subjective expectations: a structural analysis. Journal of Econometrics 222 (1), 645–675.
Andrews, Donald W.K., Shi, Xiaoxia, 2013. Inference based on conditional moment inequalities. Econometrica 81 (2), 609–666.
Andrews, Donald W.K., Soares, Gustavo, 2010. Inference for parameters defined by moment
inequalities using generalized moment selection. Econometrica 78 (1), 119–157.
Arcidiacono, Peter, Bayer, Patrick, Blevins, Jason R., Ellickson, Paul B., 2016. Estimation of
dynamic discrete choice models in continuous time with an application to retail competition.
The Review of Economic Studies 83 (3), 889–931.
Arcidiacono, Peter, Ellickson, Paul, 2011. Practical methods for estimation of dynamic discrete
choice models. Annual Review of Economics 3 (1), 363–394.
Arcidiacono, Peter, Miller, Robert A., 2011. Conditional choice probability estimation of
dynamic discrete choice models with unobserved heterogeneity. Econometrica 79 (6),
1823–1867.
Arulkumaran, Kai, Deisenroth, Marc Peter, Brundage, Miles, Bharath, Anil Anthony, 2017.
Deep reinforcement learning: a brief survey. IEEE Signal Processing Magazine 34 (6),
26–38.
Asker, John, Collard-Wexler, Allan, De Loecker, Jan, 2014. Dynamic inputs and resource (mis)
allocation. Journal of Political Economy 122 (5), 1013–1063.
Asker, John, Fershtman, Chaim, Jeon, Jihye, Pakes, Ariel, 2020. A computational framework
for analyzing dynamic auctions: the market impact of information sharing. The Rand Journal of Economics 51 (3).
Attanasio, Orazio P., 2000. Consumer durables and inertial behaviour: estimation and aggregation of (S, s) rules for automobile purchases. The Review of Economic Studies 67 (4),
667–696.
Aw, Bee Yan, Roberts, Mark J., Xu, Daniel Yi, 2011. R&D investment, exporting, and productivity dynamics. The American Economic Review 101 (4), 1312–1344.
Backus, Matthew, 2020. Why is productivity correlated with competition? Econometrica 88
(6), 2415–2444.
Bai, Yuehao, Santos, Andres, Shaikh, Azeem M., 2021. A two-step method for testing many
moment inequalities. Journal of Business & Economic Statistics, 1–11.
Bain, Joe S., 1951. Relation of profit rate to industry concentration: American manufacturing,
1936–1940. The Quarterly Journal of Economics 65 (3), 293–324.
Bain, Joseph, 1956. Barriers to New Competition. Harvard University Press, Cambridge, MA.

References

Bajari, Patrick, Benkard, C. Lanier, Levin, Jonathan, 2007. Estimating dynamic models of
imperfect competition. Econometrica 75 (5), 1331–1370.
Barwick, Panle Jia, Pathak, Parag A., 2015. The costs of free entry: an empirical study of real
estate agents in Greater Boston. The Rand Journal of Economics 46 (1), 103–145.
Bayer, Patrick, McMillan, Robert, Murphy, Alvin, Timmins, Christopher, 2016. A dynamic
model of demand for houses and neighborhoods. Econometrica 84 (3), 893–942.
Belloni, Alexandre, Bugni, Federico A., Chernozhukov, Victor, 2019. Subvector inference
in partially identified models with many moment inequalities. Technical report. cemmap
Working Paper.
Benkard, C. Lanier, 2000. Learning and forgetting: the dynamics of aircraft production. The
American Economic Review 90 (4).
Benkard, C. Lanier, 2004. A dynamic analysis of the market for wide-bodied commercial aircraft. The Review of Economic Studies 71 (3), 581–611.
Benkard, C. Lanier, Bodoh-Creed, Aaron, Lazarev, John, 2010. Simulating the Dynamic Effects of Horizontal Mergers: U.S. Airlines. Manuscript. Yale University.
Benkard, C. Lanier, Jeziorski, Przemyslaw, Weintraub, Gabriel Y., 2015. Oblivious equilibrium
for concentrated industries. The Rand Journal of Economics 46 (4), 671–708.
Berndt, Ernst R., Hall, Bronwyn H., Hall, Robert E., Hausman, Jerry A., 1974. Estimation and
inference in nonlinear structural models. In: Berg, Sanford V. (Ed.), Annals of Economic
and Social Measurement, Vol. 3. NBER, p. 653.
Berry, Steven, Compiani, Giovanni, 2021. Empirical models of industry dynamics with endogenous market structure. Annual Review of Economics 13, 309–334.
Berry, Steven, Eizenberg, Alon, Waldfogel, Joel, 2016. Optimal product variety in radio markets. The Rand Journal of Economics 47 (3), 463–497.
Berry, Steven, Gaynor, Martin, Morton, Fiona Scott, 2019. Do increasing markups matter?
Lessons from empirical industrial organization. The Journal of Economic Perspectives 33
(3), 44–68.
Berry, Steven, Levinsohn, James, Pakes, Ariel, 1995. Automobile prices in market equilibrium.
Econometrica 63 (4), 841–890.
Berry, Steven T., 1992. Estimation of a model of entry in the airline industry. Econometrica 60
(4), 889–917.
Berry, Steven T., 1994. Estimating discrete-choice models of product differentiation. The Rand
Journal of Economics 25 (2), 242–262.
Berry, Steven T., Compiani, Giovanni, 2020. An instrumental variable approach to dynamic
models. Technical report. National Bureau of Economic Research.
Berry, Steven T., Waldfogel, Joel, 2001. Do mergers increase product variety? Evidence from
radio broadcasting. The Quarterly Journal of Economics 116 (3), 1009–1025.
Bertsekas, Dimitri P., Tsitsiklis, John N., 1996. Neuro-Dynamic Programming. Athena Scientific.
Besanko, David, Doraszelski, Ulrich, Kryukov, Yaroslav, 2014. The economics of predation:
what drives pricing when there is learning-by-doing? The American Economic Review 104
(3), 868–897.
Besanko, David, Doraszelski, Ulrich, Kryukov, Yaroslav, Satterthwaite, Mark, 2010. Learningby-doing, organizational forgetting, and industry dynamics. Econometrica 78 (2), 453–508.
Bishop, Kelly C., 2008. A dynamic model of location choice and hedonic valuation. Unpublished. Washington University in St. Louis 5.
Blevins, Jason, 2014. Nonparametric identification of dynamic decision processes with discrete
and continuous choices. Quantitative Economics 5 (3), 531–554.

333

334

CHAPTER 4 Dynamic games in empirical industrial organization

Bloom, Nicholas, 2009. The impact of uncertainty shocks. Econometrica 77 (3), 623–685.
Borkovsky, Ron N., Doraszelski, Ulrich, Kryukov, Yaroslav, 2012. A dynamic quality ladder
model with entry and exit: exploring the equilibrium correspondence using the homotopy
method. Quantitative Marketing and Economics 10 (2), 197–229.
Brancaccio, Giulia, Kalouptsidi, Myrto, Papageorgiou, Theodore, 2020. Geography, search
frictions and endogenous trade costs. Econometrica 88 (2), 657–691.
Bresnahan, Timothy F., 1989. Empirical studies of industries with market power. In:
Schmalensee, Richard, Willig, Robert D. (Eds.), Handbook of Industrial Organization, Vol.
2. Elsevier, pp. 1011–1057.
Bresnahan, Timothy F., Reiss, Peter C., 1990. Entry in monopoly market. The Review of Economic Studies 57 (4), 531–553.
Bresnahan, Timothy F., Reiss, Peter C., 1991. Entry and competition in concentrated markets.
Journal of Political Economy 99 (5), 977–1009.
Bresnahan, Timothy F., Reiss, Peter C., 1994. Measuring the importance of sunk costs. Annales
d’Économie et de Statistique 31, 183–217.
Brown, Alexander L., Camerer, Colin F., Lovallo, Dan, 2013. Estimating structural models of
equilibrium and cognitive hierarchy thinking in the field: the case of withheld movie critic
reviews. Management Science 59 (3), 733–747.
Buchanan, James M., 1969. External diseconomies, corrective taxes, and market structure. The
American Economic Review 59 (1), 174–177.
Buchholz, Nicholas, 2018. Spatial equilibrium, search frictions and dynamic efficiency in the
taxi industry. Manuscript. Princeton University.
Buchholz, Nicholas, Shum, Matthew, Xu, Haiqing, 2021. Semiparametric estimation of dynamic discrete choice models. Journal of Econometrics 223 (2), 312–327.
Bugni, Federico A., 2010. Bootstrap inference in partially identified models defined by moment inequalities: coverage of the identified set. Econometrica 78 (2), 735–753.
Caballero, Ricardo, Pindyck, Robert, 1996. Investment, uncertainty and industry evolution.
International Economic Review 37 (3), 641–662.
Camerer, Colin F., Ho, Teck-Hua, Chong, Juin-Kuan, 2004. A cognitive hierarchy model of
games. The Quarterly Journal of Economics 119 (3), 861–898.
Canay, Ivan A., 2010. EL inference for partially identified models: large deviations optimality
and bootstrap validity. Journal of Econometrics 156 (2), 408–425.
Caoui, El Hadi, 2019. Estimating the Costs of Standardization: Evidence from the Movie Industry. Manuscript. University of Toronto.
Chen, Le-Yu., 2017. Identification of discrete choice dynamic programming models with nonparametric distribution of unobservables. Econometric Theory 33 (3), 551–577.
Chernozhukov, Victor, Chetverikov, Denis, Demirer, Mert, Duflo, Esther, Hansen, Christian,
Newey, Whitney, Robins, James, 2018. Double/debiased machine learning for treatment and
structural parameters.
Chernozhukov, Victor, Chetverikov, Denis, Kato, Kengo, 2019. Inference on causal and structural parameters using many moment inequalities. The Review of Economic Studies 86 (5),
1867–1900.
Chernozhukov, Victor, Escanciano, Juan Carlos, Ichimura, Hidehiko, Newey, Whitney K.,
Robins, James M., 2016. Locally robust semiparametric estimation. arXiv preprint. arXiv:
1608.00033.
Chernozhukov, Victor, Hong, Han, Tamer, Elie, 2007. Estimation and confidence regions for
parameter sets in econometric models 1. Econometrica 75 (5), 1243–1284.
Chernozhukov, Victor, Lee, Sokbae, Rosen, Adam M., 2013. Intersection bounds: estimation
and inference. Econometrica 81 (2), 667–737.

References

Chesher, Andrew, Rosen, Adam M., 2017. Generalized instrumental variable models. Econometrica 85 (3), 959–989.
Chevalier, Judith, Goolsbee, Austan, 2009. Are durable goods consumers forward-looking?
Evidence from college textbooks. The Quarterly Journal of Economics 124 (4), 1853–1884.
Chicu, Mark, 2013. Dynamic investment and deterrence in the US cement industry.
Manuscript. Harvard University.
Ching, Andrew, Osborne, Matthew, 2020. Identification and estimation of forward-looking
behavior: the case of consumer stockpiling. Marketing Science 39 (4), 707–726.
Chow, Chef-Seng, Tsitsiklis, John N., 1989. The complexity of dynamic programming. Journal
of Complexity 5 (4), 466–488.
Ciliberto, Federico, Tamer, Elie, 2009. Market structure and multiple equilibria in airline markets. Econometrica 77 (6), 1791–1828.
Collard-Wexler, Allan, 2013. Demand fluctuations in the ready-mix concrete industry. Econometrica 81 (3), 1003–1037.
Collard-Wexler, Allan, 2014. Mergers and sunk costs: an application to the ready-mix concrete
industry. American Economic Journal: Microeconomics 6 (4), 407–447.
Collard-Wexler, Allan, De Loecker, Jan, 2015. Reallocation and technology: evidence from
the US steel industry. The American Economic Review 105 (1), 131–171.
Cooper, Russell, Haltiwanger, John, 2006. On the nature of capital adjustment costs. The Review of Economic Studies 73 (3), 611–633.
Copeland, Adam, Monnet, Cyril, 2009. The welfare effects of incentive schemes. The Review
of Economic Studies 76 (1), 93–113.
Das, Sanghamitra, 1992. A micro-econometric model of capital utilization and retirement: the
case of the US cement industry. The Review of Economic Studies 59 (2), 277–297.
De Groote, Olivier, Verboven, Frank, 2019. Subsidies and time discounting in new technology
adoption: evidence from solar photovoltaic systems. The American Economic Review 109
(6), 2137–2172.
De Paula, Aureo, Tang, Xun, 2012. Inference of signs of interaction effects in simultaneous
games with incomplete information. Econometrica 80 (1), 143–172.
De Pinto, Alessandro, Nelson, Gerald C., 2009. Land use change with spatially explicit data:
a dynamic approach. Environmental & Resource Economics 43 (2), 209–229.
Dee, Jan Victor, 2020. A Dynamic Structural Model for Pay-Per-Bid Auctions: Explaining the
Excess Revenue Puzzle in Online Auctions. Technical report. Concordia University.
Dixit, Avinash K., Pindyck, Robert, 1994. Investment Under Uncertainty. Princeton University
Press.
Doraszelski, Ulrich, 2003. An R&D race with knowledge accumulation. The Rand Journal of
Economics 34 (1), 20–42.
Doraszelski, Ulrich, Escobar, Juan F., 2010. A theory of regular Markov perfect equilibria in
dynamic stochastic games: genericity, stability, and purification. Theoretical Economics 5
(3), 369–402.
Doraszelski, Ulrich, Judd, Kenneth L., 2012. Avoiding the curse of dimensionality in dynamic
stochastic games. Quantitative Economics 3 (1), 53–93.
Doraszelski, Ulrich, Lewis, Gregory, Pakes, Ariel, 2018. Just starting out: learning and equilibrium in a new market. The American Economic Review 108 (3), 565–615.
Doraszelski, Ulrich, Pakes, Ariel, 2007. A framework for applied dynamic analysis in IO. In:
Handbook of Industrial Organization, Vol. 3, pp. 1887–1966. Chapter 20.
Doraszelski, Ulrich, Satterthwaite, Mark, 2010. Computable Markov-perfect industry dynamics. The Rand Journal of Economics 41 (2), 215–243.

335

336

CHAPTER 4 Dynamic games in empirical industrial organization

Dubé, Jean-Pierre, Hitsch, Günter J., Manchanda, Puneet, 2005. An empirical model of advertising dynamics. Quantitative Marketing and Economics 3 (2), 107–144.
Dubé, Jean-Pierre H., Hitsch, Günter J., Chintagunta, Pradeep K., 2010. Tipping and concentration in markets with indirect network effects. Marketing Science 29 (2), 216–249.
Dunne, Timothy, Klimek, Shawn D., Roberts, Mark J., Xu, Daniel Yi, 2013. Entry, exit, and
the determinants of market structure. The Rand Journal of Economics 4 (3), 462–487.
Egesdal, Michael, Lai, Zhenyu, Su, Che-Lin, 2015. Estimating dynamic discrete-choice games
of incomplete information. Quantitative Economics 6 (3), 567–597.
Ellickson, Paul B., Misra, Sanjog, 2008. Supermarket pricing strategies. Marketing Science 27
(5), 811–828.
Ellickson, Paul B., Misra, Sanjog, Nair, Harikesh S., 2012. Repositioning dynamics and pricing
strategy. Journal of Marketing Research 49 (6), 750–772.
Ellison, Sara Fisher, Snyder, Christopher, Zhang, Hongkai, 2018. Costs of managerial attention and activity as a source of sticky prices: Structural estimates from an online market.
Technical report. National Bureau of Economic Research.
Elyakime, Bernard, Laffont, Jean Jacques, Loisel, Patrice, Vuong, Quang, 1994. First-price
sealed-bid auctions with secret reservation prices. Annales d’Économie et de Statistique 34,
115–141.
Ericson, Richard, Pakes, Ariel, 1995. Markov-perfect industry dynamics: a framework for empirical work. The Review of Economic Studies 62 (1), 53–82.
Esteban, Susanna, Shum, Matthew, 2007. Durable-goods oligopoly with secondary markets:
the case of automobiles. The Rand Journal of Economics 38 (2), 332–354.
Evans, George W., Honkapohja, Seppo, 2012. Learning and Expectations in Macroeconomics.
Princeton University Press.
Fang, Hanming, Wang, Yang, 2015. Estimating dynamic discrete choice models with hyperbolic discounting, with an application to mammography decisions. International Economic
Review 56 (2), 565–596.
Farias, Vivek, Saure, Denis, Weintraub, Gabriel Y., 2012. An approximate dynamic programming approach to solving dynamic oligopoly models. The Rand Journal of Economics 43
(2), 253–282.
Farrell, Joseph, Shapiro, Carl, 1990. Horizontal mergers: an equilibrium analysis. The American Economic Review 80 (1), 107–126.
Fershtman, Chaim, Pakes, Ariel, 2012. Dynamic games with asymmetric information: a framework for empirical work. The Quarterly Journal of Economics 127 (4), 1611–1661.
Foster, Lucia, Haltiwanger, John, Syverson, Chad, 2008. Reallocation, firm turnover, and efficiency: selection on productivity or profitability? The American Economic Review 98 (1),
394–425.
Fowlie, Meredith, Reguant, Mar, Ryan, Stephen P., 2016. Market-based emissions regulation
and industry dynamics. Journal of Political Economy 124 (1), 249–302.
Frechette, Guillaume R., Lizzeri, Alessandro, Salz, Tobias, 2019. Frictions in a competitive, regulated market: evidence from taxis. The American Economic Review 109 (8),
2954–2992.
Fudenberg, Drew, Tirole, Jean, 1986. A theory of exit in duopoly. Econometrica 54 (4),
943–960.
Gavazza, Alessandro, 2011a. Leasing and secondary markets: theory and evidence from commercial aircraft. Journal of Political Economy 119 (2), 325–377.
Gavazza, Alessandro, 2011b. The role of trading frictions in real asset markets. The American
Economic Review 101 (4), 1106–1143.

References

Gayle, Philip G., Xie, Xin, 2018. Entry deterrence and strategic alliances. Economic Inquiry 56
(3), 1898–1924.
Gertner, Robert, 1985. Dynamic duopoly with price inertia. PhD Thesis. MIT.
Ghemawat, Pankaj, Nalebuff, Barry, 1985. Exit. The Rand Journal of Economics 16 (2),
184–194.
Goettler, Ronald L., Gordon, Brett R., 2011. Does AMD spur Intel to innovate more? Journal
of Political Economy 119 (6), 1141–1200.
Goldfarb, Avi, Xiao, Mo, 2011. Who thinks about the competition? Managerial ability and
strategic entry in US local telephone markets. The American Economic Review 101 (7),
3130–3161.
Goolsbee, Austan, Syverson, Chad, 2008. How do incumbents respond to the threat of entry?
Evidence from the major airlines. The Quarterly Journal of Economics 123 (4), 1611–1633.
Gowrisankaran, Gautam, 1995. A Dynamic Analysis of Mergers. PhD diss., Dissertation. Yale
University.
Gowrisankaran, Gautam, 1999. A dynamic model of endogenous horizontal mergers. The
Rand Journal of Economics 30 (1), 56–83.
Gowrisankaran, Gautam, Rysman, Marc, 2012. Dynamics of consumer demand for new
durable goods. Journal of Political Economy 120 (6), 1173–1219.
Gowrisankaran, Gautam, Town, Robert J., 1997. Dynamic equilibrium in the hospital industry.
Journal of Economics & Management Strategy 6 (1), 45–74.
Green, Edward J., Porter, Robert H., 1984. Noncooperative collusion under imperfect price
information. Econometrica 52 (1), 87–100.
Groeger, Joachim R., 2014. A study of participation in dynamic auctions. International Economic Review 55 (4), 1129–1154.
Guerre, Emmanuel, Perrigne, Isabelle, Vuong, Quang, 2000. Optimal nonparametric estimation of first-price auctions. Econometrica 68 (3), 525–574.
Gyourko, Joseph, Saiz, Albert, Summers, Anita, 2008. A new measure of the local regulatory
environment for housing markets: the Wharton residential land use regulatory index. Urban
Studies 45 (3), 693–729.
Han, Lu, Hong, Seung-Hyun, 2011. Testing cost inefficiency under free entry in the real estate
brokerage industry. Journal of Business & Economic Statistics 29 (4), 564–578.
Hansen, Lars Peter, Singleton, Kenneth J., 1982. Generalized instrumental variables estimation
of nonlinear rational expectations models. Econometrica 50 (5), 1269–1286.
Harsanyi, John, 1973. Games with randomly disturbed payoffs: a new rationale for mixedstrategy equilibrium points. International Journal of Game Theory 2 (1), 1–23.
Hashmi, Aamir, Biesebroeck, Johannes Van, 2016. The relationship between market structure
and innovation in industry equilibrium: a case study of the global automobile industry. Review of Economics and Statistics 98 (1), 192–208.
Heckman, James, 1981. The incidental parameters problem and the problem of initial conditions in estimating a discrete time - discrete data stochastic process. In: Manski, C.,
McFadden, D. (Eds.), Structural Analysis of Discrete Data with Econometric Applications.
MIT Press.
Hendel, Igal, Nevo, Aviv, 2006. Measuring the implications of sales and consumer inventory
behavior. Econometrica 74 (6), 1637–1673.
Hendel, Igal, Nevo, Aviv, 2013. Intertemporal price discrimination in storable goods markets.
The American Economic Review 103 (7), 2722–2751.
Hendricks, Ken, Piccione, Michele, Tan, Guofu, 1997. Entry and exit in hub-spoke networks.
The Rand Journal of Economics 28 (2), 291–303.

337

338

CHAPTER 4 Dynamic games in empirical industrial organization

Hincapié, Andrés, 2020. Entrepreneurship over the life cycle: where are the young entrepreneurs? International Economic Review 61 (2), 617–681.
Ho, Katherine, 2009. Insurer-provider networks in the medical care market. The American
Economic Review 99 (1), 393–430.
Ho, Teck-Hua, Su, Xuanming, 2013. A dynamic level-k model in sequential games. Management Science 59 (2), 452–469.
Hollenbeck, Brett, 2017. The economic advantages of chain organization. The Rand Journal
of Economics 48 (4), 1103–1135.
Hollenbeck, Brett, 2018. Online reputation mechanisms and the decreasing value of chain
affiliation. Journal of Marketing Research 55 (5), 636–654.
Holmes, Thomas J., 2011. The diffusion of Wal-Mart and economies of density. Econometrica 79 (1), 253–302.
Hopenhayn, Hugo, Saeedi, Maryam, 2016. Bidding dynamics in auctions. Technical report.
National Bureau of Economic Research.
Hopenhayn, Hugo A., 1992. Entry, exit, and firm dynamics in long run equilibrium. Econometrica 60 (5), 1127–1150.
Horowitz, Joel, 1993. Semiparametric and nonparametric estimation of quantal response models. In: Maddala, G.S., Rao, C.R., Vinod, H.D. (Eds.), Handbook of Statistics, Vol. 11.
Elsevier, pp. 45–72.
Hortaçsu, Ali, Luco, Fernando, Puller, Steven L., Zhu, Dongni, 2019. Does strategic ability
affect efficiency? Evidence from electricity markets. The American Economic Review 109
(12), 4302–4342.
Hortaçsu, Ali, Puller, Steven L., 2008. Understanding strategic bidding in multi-unit auctions:
a case study of the Texas electricity spot market. The Rand Journal of Economics 39 (1),
86–114.
Hotz, V. Joseph, Miller, Robert A., 1993. Conditional choice probabilities and the estimation
of dynamic models. The Review of Economic Studies 60 (3), 497–529.
Hotz, V. Joseph, Miller, Robert A., Sanders, Seth, Smith, Jeffrey, 1994. A simulation estimator
for dynamic models of discrete choice. The Review of Economic Studies 61 (2), 265–289.
Hsieh, Chang-Tai, Klenow, Peter J., 2009. Misallocation and manufacturing TFP in China and
India. The Quarterly Journal of Economics 124 (4), 1403–1448.
Hu, Yingyao, Shum, Matthew, 2012. Nonparametric identification of dynamic models with
unobserved state variables. Journal of Econometrics 171 (1), 32–44.
Hu, Yingyao, Shum, Matthew, 2013. Identifying dynamic games with serially correlated unobservables. In: Structural Econometric Models. Emerald Group Publishing Limited.
Huang, Ling, Smith, Martin D., 2014. The dynamic efficiency costs of common-pool resource
exploitation. The American Economic Review 104 (12), 4071–4103.
Huang, Yan, Singh, Param Vir, Ghose, Anindya, 2015. A structural model of employee behavioral dynamics in enterprise social media. Management Science 61 (12), 2825–2844.
Huang, Yufeng, Ellickson, Paul B., Lovett, Mitchell J., 2020. Learning to set prices.
Manuscript. University of Rochester - Simon Business School.
Ifrach, Bar, Weintraub, Gabriel Y., 2017. A framework for dynamic oligopoly in concentrated
industries. The Review of Economic Studies 84 (3), 1106–1150.
Igami, Mitsuru, 2017. Estimating the innovator’s dilemma: structural analysis of creative destruction in the hard disk drive industry, 1981–1998. Journal of Political Economy 125 (3),
798–847.
Igami, Mitsuru, Uetake, Kosuke, 2020. Mergers, innovation, and entry-exit dynamics: consolidation of the hard disk drive industry, 1996-2016. The Review of Economic Studies 87
(6).

References

Igami, Mitsuru, Yang, Nathan, 2016. Unobserved heterogeneity in dynamic games: cannibalization and preemptive entry of hamburger chains in Canada. Quantitative Economics 7 (2),
483–521.
Iskhakov, Fedor, Rust, John, Schjerning, Bertel, 2020. Machine learning and structural econometrics: contrasts and synergies. Econometrics Journal 23 (3), S81–S124.
Jeon, Jihye, 2020. Learning and investment under demand uncertainty in container shipping.
The Rand Journal of Economics. Forthcoming.
Jeziorski, Przemysław, 2014. Estimation of cost efficiencies from mergers: application to US
radio. The Rand Journal of Economics 45 (4), 816–846.
Jeziorski, Przemyslaw, Krasnokutskaya, Elena, 2016. Dynamic auction environment with subcontracting. The Rand Journal of Economics 47 (4), 751–791.
Jia, Panle, 2008. What happens when Wal-Mart comes to town: an empirical analysis of the
discount retailing industry. Econometrica 76 (6), 1263–1316.
Jofre-Bonet, Mireia, Pesendorfer, Martin, 2003. Estimation of a dynamic auction game. Econometrica 71 (5), 1443–1489.
Jovanovic, Boyan, 1982. Selection and the evolution of industry. Econometrica 50 (3),
649–670.
Jovanovic, Boyan, Rousseau, Peter L., 2002. The Q-theory of mergers. The American Economic Review 92 (2), 198–204.
Judd, Kenneth L., 1985. Credible spatial preemption. The Rand Journal of Economics 16 (2),
153–166.
Kalouptsidi, Myrto, 2014. Time to build and fluctuations in bulk shipping. The American Economic Review 104 (2), 564–608.
Kalouptsidi, Myrto, 2018. Detection and impact of industrial subsidies: the case of Chinese
shipbuilding. The Review of Economic Studies 85 (2), 1111–1158.
Kalouptsidi, Myrto, Kitamura, Yuichi, Lima, Lucas, Souza-Rodrigues, Eduardo A., 2020. Partial identification and inference for dynamic models and counterfactuals. Technical report.
National Bureau of Economic Research.
Kalouptsidi, Myrto, Scott, Paul, Souza-Rodrigues, Eduardo, 2017. On the non-identification
of counterfactuals in dynamic discrete games. International Journal of Industrial Organization 50, 362–371.
Kalouptsidi, Myrto, Scott, Paul, Souza-Rodrigues, Eduardo, 2021. Identification of counterfactuals in dynamic discrete choice models. Quantitative Economics 12 (2), 351–403.
Kano, Kazuko, 2013. Menu costs and dynamic duopoly. International Journal of Industrial
Organization 31 (1), 102–118.
Kasahara, Hiroyuki, Shimotsu, Katsumi, 2009. Nonparametric identification of finite mixture
models of dynamic discrete choices. Econometrica 77 (1), 135–175.
Kasahara, Hiroyuki, Shimotsu, Katsumi, 2012. Sequential estimation of structural models with
a fixed point constraint. Econometrica 80 (5), 2303–2319.
Katz, Michael L., Shapiro, Carl, 1985. Network externalities, competition, and compatibility.
The American Economic Review 75 (3), 424–440.
Kellogg, Ryan, 2014. The effect of uncertainty on investment: evidence from Texas oil drilling.
The American Economic Review 104 (6), 1698–1734.
Komarova, Tatiana, Sanches, Fabio, Junior, Daniel Silva, Srisuma, Sorawoot, 2018. Joint
analysis of the discount factor and payoff parameters in dynamic discrete choice models.
Quantitative Economics 9 (3), 1153–1194.
Kristensen, Dennis, Nesheim, Lars, de Paula, Áureo, 2015. CCP and the estimation of nonseparable dynamic models. Technical report. Mimeo. University College, London.

339

340

CHAPTER 4 Dynamic games in empirical industrial organization

Krusell, Per, Smith Jr., Anthony A., 1998. Income and wealth heterogeneity in the macroeconomy. Journal of Political Economy 106 (5), 867–896.
Kryukov, Yaroslav, 2010. Dynamic R&D and the effectiveness of policy intervention in the
pharmaceutical industry. Manuscript. Tepper School of Business. Carnegie Mellon University.
Kydland, Finn E., Prescott, Edward C., 1982. Time to build and aggregate fluctuations. Econometrica 50 (6), 1345–1370.
Laffont, Jean-Jacques, Tirole, Jean, 1993. A Theory of Incentives in Procurement and Regulation. MIT Press.
Lee, Robin S., 2013. Vertical integration and exclusivity in platform and two-sided markets.
The American Economic Review 103 (7), 2960–3000.
Lee, Robin S., Fong, Kyna, 2013. Markov perfect network formation: an applied framework for
bilateral oligopoly and bargaining in buyer-seller networks. Manuscript. Harvard University.
Lewbel, Arthur, 1998. Semiparametric latent variable model estimation with endogenous or
mismeasured regressors. Econometrica 66 (1), 105–121.
Lewbel, Arthur, 2000. Semiparametric qualitative response model estimation with unknown
heteroscedasticity or instrumental variables. Journal of Econometrics 97 (1), 145–177.
Lim, Claire S.H., Yurukoglu, Ali, 2018. Dynamic natural monopoly regulation: time inconsistency, moral hazard, and political environments. Journal of Political Economy 126 (1),
263–312.
Lin, Haizhen, 2015. Quality choice and market structure: a dynamic analysis of nursing home
oligopolies. International Economic Review 56 (4), 1261–1290.
Lin, Zhongjian, Xu, Haiqing, 2017. Estimation of social-influence-dependent peer pressure in
a large network game. Econometrics Journal 20 (3), S86–S102.
Liu, Xiaodong, Zhou, Jiannan, 2017. A social interaction model with ordered choices. Economics Letters 161, 86–89.
Llull, Joan, 2018. Immigration, wages, and education: a labour market equilibrium structural
model. The Review of Economic Studies 85 (3), 1852–1896.
Magnac, Thierry, Thesmar, David, 2002. Identifying dynamic discrete decision processes.
Econometrica 70 (2), 801–816.
Maskin, Eric, Tirole, Jean, 1988a. A theory of dynamic oligopoly, I: overview and quantity
competition with large fixed costs. Econometrica 56 (3), 549–569.
Maskin, Eric, Tirole, Jean, 1988b. A theory of dynamic oligopoly, II: price competition, kinked
demand curves, and Edgeworth cycles. Econometrica 56 (3), 571–599.
Matzkin, Rosa L., 1992. Nonparametric and distribution-free estimation of the binary threshold
crossing and the binary choice models. Econometrica 60 (2), 239–270.
McFadden, Daniel, 1989. A method of simulated moments for estimation of discrete response
models without numerical integration. Econometrica 57 (5), 995–1026.
Mermelstein, Ben, Nocke, Volker, Satterthwaite, Mark A., Whinston, Michael D., 2020. Internal versus external growth in industries with scale economies: a computational model of
optimal merger policy. Journal of Political Economy 128 (1), 301–341.
Milgrom, Paul, Roberts, John, 1982. Limit pricing and entry under incomplete information: an
equilibrium analysis. Econometrica 50 (2), 443–459.
Murphy, Alvin, 2018. A dynamic model of housing supply. American Economic Journal: Economic Policy 10 (4), 243–267.
Mysliwski, Mateusz, Sanches, Fabio M., Junior, Daniel Silva, Srisuma, Sorawoot, 2020.
The Welfare Effects of Promotional Fees. Technical report. CeMMAP Working Paper,
CWP35/20.

References

Nagel, Rosemarie, 1995. Unraveling in guessing games: an experimental study. The American
Economic Review 85 (5), 1313–1326.
Nekipelov, Denis, Novosad, Paul, Ryan, Stephen P., 2021. Moment Forests.
Nevo, Aviv, Rossi, Federico, 2008. An approach for extending dynamic models to settings with
multi-product firms. Economics Letters 100 (1), 49–52.
Newey, Whitney K., 1994. The asymptotic variance of semiparametric estimators. Econometrica 62 (6), 1349–1382.
Norets, Andriy, Tang, Xun, 2014. Semiparametric inference in dynamic binary choice models.
The Review of Economic Studies 81 (3), 1229–1262.
Olley, G.S., Town, R., 2018. End of an era: the American airlines-US airways merger. In:
Kwoka, J., White, L. (Eds.), The Antitrust Revolution: Economics, Competition and Policy,
7th edition. Oxford University Press.
OpenAI, Berner, Christopher, Brockman, Greg, Chan, Brooke, Cheung, Vicki, D˛ebiak, Przemysław, et al., 2019. Dota 2 with large scale deep reinforcement learning. eprint. arXiv:
1912.06680.
Otsu, Taisuke, Pesendorfer, Martin, Takahashi, Yuya, 2016. Pooling data across markets in
dynamic Markov games. Quantitative Economics 7 (2), 523–559.
Pakes, Ariel, 1986. Patents as options: some estimates of the value of holding European patent
stocks. Econometrica 54 (4), 755–784.
Pakes, Ariel, McGuire, Paul, 1994. Computing Markov-perfect Nash equilibria: numerical implications of a dynamic differentiated product model. The Rand Journal of Economics 25
(4), 555–589.
Pakes, Ariel, McGuire, Paul, 2001. Stochastic algorithms, symmetric Markov perfect equilibrium, and the ‘curse’ of dimensionality. Econometrica 69 (5), 1261–1281.
Pakes, Ariel, Ostrovsky, Michael, Berry, Steven, 2007. Simple estimators for the parameters
of discrete dynamic games (with entry/exit examples). The Rand Journal of Economics 38
(2), 373–399.
Pakes, Ariel, Pollard, David, 1989. Simulation and the asymptotics of optimization estimators.
Econometrica 57 (5), 1027–1057.
Pakes, Ariel, Porter, Jack, Ho, Kate, Ishii, Joy, 2015. Moment inequalities and their application.
Econometrica 83 (1), 315–334.
Pesendorfer, Martin, Schmidt-Dengler, Philipp, 2008. Asymptotic least squares estimators for
dynamic games. The Review of Economic Studies 75 (3), 901–928.
Pesendorfer, Martin, Schmidt-Dengler, Philipp, 2010. Sequential estimation of dynamic discrete games: a comment. Econometrica 78 (2), 833–842.
Powell, Warren B., 2007. Approximate Dynamic Programming: Solving the Curses of Dimensionality, Vol. 703. John Wiley & Sons.
Ramey, Valerie A., Shapiro, Matthew D., 2001. Displaced capital: a study of aerospace plant
closings. Journal of Political Economy 109 (5), 958–992.
Ransom, Tyler, 2021. Labor market frictions and moving costs of the employed and unemployed. The Journal of Human Resources, 0219-10013R2.
Romano, Joseph P., Shaikh, Azeem M., Wolf, Michael, 2014. A practical two-step method for
testing moment inequalities. Econometrica 82 (5), 1979–2002.
Romer, Paul M., 1986. Increasing returns and long-run growth. Journal of Political Economy 94 (5), 1002–1037.
Rotemberg, Julio J., 1982. Sticky prices in the United States. Journal of Political Economy 90
(6), 1187–1211.
Rotemberg, Julio J., Saloner, Garth, 1987. The relative rigidity of monopoly pricing. The
American Economic Review, 917–926.

341

342

CHAPTER 4 Dynamic games in empirical industrial organization

Rubinstein, Ariel, 1982. Perfect equilibrium in a bargaining model. Econometrica 50 (1),
97–109.
Rust, John, 1987. Optimal replacement of GMC bus engines: an empirical model of Harold
Zurcher. Econometrica 5, 999–1033.
Rust, John, 1994a. Estimation of dynamic structural models, problems and prospects: discrete
decision processes. In: Advances in Econometrics: Sixth World Congress. In: Econometric
Society Monographs, vol. 2. Cambridge University Press, pp. 119–170.
Rust, John, 1994b. Structural estimation of Markov decision processes. Handbook of Econometrics 4, 3081–3143.
Rust, John, Rothwell, Geoffrey, 1995. Optimal response to a shift in regulatory regime: the case
of the US nuclear power industry. Journal of Applied Econometrics 10 (S1), S75–S118.
Ryan, Stephen P., 2012. The costs of environmental regulation in a concentrated industry.
Econometrica 80 (3), 1019–1061.
Ryan, Stephen P., Tucker, Catherine, 2012. Heterogeneity and the dynamics of technology
adoption. Quantitative Marketing and Economics 10 (1), 63–109.
Schmidt-Dengler, Philipp, 2006. The timing of new technology adoption: the case of MRI.
Manuscript. London School of Economics.
Scott, Paul, 2014. Dynamic discrete choice estimation of agricultural land use.
Seim, Katja, 2006. An empirical model of firm entry with endogenous product-type choices.
The Rand Journal of Economics 37 (3), 619–640.
Shaked, Avner, Sutton, John, 1984. Involuntary unemployment as a perfect equilibrium in a
bargaining model. Econometrica 52 (6), 1351–1364.
Shao, Kun, Tang, Zhentao, Zhu, Yuanheng, Li, Nannan, Zhao, Dongbin, 2019. A survey of
deep reinforcement learning in video games. arXiv preprint. arXiv:1912.10944.
Shumpeter, Joseph, 1942. Capitalism, Socialism and Democracy. Harper and Brothers, New
York.
Simon, Herbert A., 1959. Theories of decision-making in economics and behavioral science.
The American Economic Review 49 (3), 253–283.
Slade, Margaret E., 1998. Optimal pricing with costly adjustment: evidence from retail-grocery
prices. The Review of Economic Studies 65 (1), 87–107.
Srisuma, Sorawoot, Linton, Oliver, 2012. Semiparametric estimation of Markov decision processes with continuous state space. Journal of Econometrics 166 (2), 320–341.
Stahl, Dale O., Wilson, Paul W., 1995. On players’ models of other players: theory and experimental evidence. Games and Economic Behavior 10 (1), 218–254.
Stahl, Jessica Calfee, 2016. Effects of deregulation and consolidation of the broadcast television industry. The American Economic Review 106 (8), 2185–2218.
Su, Che-Lin, Judd, Kenneth L., 2012. Constrained optimization approaches to estimation of
structural models. Econometrica 80 (5), 2213–2230.
Sutton, John, 1991. Sunk Costs and Market Structure: Price Competition, Advertising, and the
Evolution of Concentration. MIT Press.
Suzuki, Junichi, 2013. Land use regulation as a barrier to entry: evidence from the Texas lodging industry. International Economic Review 54 (2), 495–523.
Sweeting, Andrew, 2009. The strategic timing incentives of commercial radio stations: an empirical analysis using multiple equilibria. The Rand Journal of Economics 40 (4), 710–742.
Sweeting, Andrew, 2013. Dynamic product positioning in differentiated product markets: the
effect of fees for musical performance rights on the commercial radio industry. Econometrica 81 (5), 1763–1803.
Sweeting, Andrew, Bhattacharya, Vivek, 2015. Selective entry and auction design. International Journal of Industrial Organization 43, 189–207.

References

Sweeting, Andrew, Roberts, James W., Gedge, Chris, 2020. A model of dynamic limit pricing
with an application to the airline industry. Journal of Political Economy 128 (3), 1148–1193.
Syverson, Chad, 2004. Market structure and productivity: a concrete example. Journal of Political Economy 112 (6), 1181–1222.
Takahashi, Yuya, 2015. Estimating a war of attrition: the case of the US movie theater industry.
The American Economic Review 105 (7), 2204–2241.
Tomlin, Ben, 2014. Exchange rate fluctuations, plant turnover and productivity. International
Journal of Industrial Organization 35, 12–28.
Traiberman, Sharon, 2019. Occupations and import competition: evidence from Denmark. The
American Economic Review 109 (12), 4260–4301.
U.S. Department of Justice, Federal Trade Commission, 2010. Horizontal Merger Guidelines.
Vreugdenhil, Nicholas, 2020. Booms, Busts, and Mismatch in Capital Markets: Evidence from
the Offshore Oil and Gas Industry. Technical report. Arizona State University.
Waldfogel, Joel, Berry, Steven T., 1999. Free entry and social inefficiency in radio broadcasting. The Rand Journal of Economics 30 (3), 397–420.
Weintraub, Gabriel Y., Benkard, C. Lanier, Van Roy, Benjamin, 2008. Markov perfect industry
dynamics with many firms. Econometrica 76 (6), 1375–1411.
Whinston, Michael D., 2007. Antitrust policy toward horizontal mergers. In: Handbook of
Industrial Organization, Vol. 3, pp. 2369–2440.
Xie, Erhao, 2021. Inference in games without Nash equilibrium: an application to restaurants’
competition in opening hours. Journal of Business & Economic Statistics. Forthcoming.
Xu, Daniel Yi, Chen, Yanyou, 2020. A structural empirical model of R&D, firm heterogeneity,
and industry evolution. Journal of Industrial Economics.

343

